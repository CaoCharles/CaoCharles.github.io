<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Charles&#39;s Blog</title>
  
  <subtitle>test</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://caocharles.github.io/"/>
  <updated>2018-12-08T17:57:45.282Z</updated>
  <id>https://caocharles.github.io/</id>
  
  <author>
    <name>查爾斯</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>職場透視營 II</title>
    <link href="https://caocharles.github.io/%E8%81%B7%E5%A0%B4%E9%80%8F%E8%A6%96%E7%87%9F-%E5%BF%83%E5%BE%97/"/>
    <id>https://caocharles.github.io/職場透視營-心得/</id>
    <published>2018-12-08T17:41:19.000Z</published>
    <updated>2018-12-08T17:57:45.282Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【職涯中心】職場透視營-II"><a href="#【職涯中心】職場透視營-II" class="headerlink" title="【職涯中心】職場透視營 II"></a>【職涯中心】職場透視營 II</h1><ul><li>時間 : 2018/12/08 9:00-18:00 </li><li>地點 : 總院270406</li></ul><h2 id="上午場"><a href="#上午場" class="headerlink" title="上午場"></a>上午場</h2><ul><li>講師 : 退休的和藹伯伯</li><li><p>經歷 : 職業生涯諮詢顧問</p></li><li><p>改變</p><ul><li>主動的(有想法)</li><li>被動的(主管要求)</li></ul></li></ul><h3 id="公司招募流程-以鴻海集團為例"><a href="#公司招募流程-以鴻海集團為例" class="headerlink" title="公司招募流程(以鴻海集團為例)"></a>公司招募流程(以鴻海集團為例)</h3><ul><li>投遞履歷</li><li>履歷審核 (篩選-人才庫)</li><li>面試徵選 (人才儲備)</li><li>任用流程</li><li>寄發通知書</li><li>報到</li></ul><h3 id="履歷與公司招募"><a href="#履歷與公司招募" class="headerlink" title="履歷與公司招募"></a>履歷與公司招募</h3><ul><li>找工作的第一步</li><li>入門/初階工作，少則數十份，多則以百計</li><li>履歷篩選<ul><li>1.人資(初步/複選/電話)&gt;&gt;徵才部門</li><li>2.徵才部門(團隊/主管)&gt;&gt;面試</li></ul></li><li>履歷到面試時間<ul><li>當天 ~ 1、2月 </li></ul></li><li>通知已收到履歷或是否面試<ul><li>視公司而定</li></ul></li><li>面試機率<ul><li>2%(美國)</li></ul></li><li>機器人篩選<ul><li>關鍵字(產業趨勢、工作內容)</li></ul></li><li>投履歷是過程，面試才是目標</li></ul><h3 id="履歷到碎紙機-或Del鍵-的捷徑"><a href="#履歷到碎紙機-或Del鍵-的捷徑" class="headerlink" title="履歷到碎紙機(或Del鍵)的捷徑"></a>履歷到碎紙機(或Del鍵)的捷徑</h3><ul><li>沒有寫到應徵項目</li><li>一次應徵很多不同類型的職缺</li><li>在技能選項完全沒有自己會的項目</li><li>沒有寫自傳或不知所云</li><li>剛畢業、沒有工作經驗就應徵主管職缺</li><li>人資看履歷平均時間 : 約40秒</li></ul><h3 id="我想被看到-客製化履歷"><a href="#我想被看到-客製化履歷" class="headerlink" title="我想被看到 - 客製化履歷"></a>我想被看到 - 客製化履歷</h3><ul><li>獨特性、與眾不同，不要人云亦云</li><li>只有4%的社會新鮮人會客製化履歷</li><li>用自我行銷的方式寫履歷</li><li>行銷四要素<ul><li>產品 : 個人經歷盤點(知己 : 認識自己)</li><li>推廣 : 目標職務研究(知彼 : 了解產業與工作內容)</li><li>價格 : 自我價值</li><li>通路 : 選取相關素材(履歷表、自傳)</li></ul></li></ul><h3 id="原來主管這麼想-需加強的就業能力"><a href="#原來主管這麼想-需加強的就業能力" class="headerlink" title="原來主管這麼想(需加強的就業能力)"></a>原來主管這麼想(需加強的就業能力)</h3><ul><li>穩定性及抗壓性</li><li>良好的個人工作態度</li><li>表達與溝通能力</li><li>強烈的學習意願與高度可塑性</li><li>團隊合作能力</li></ul><h3 id="知己知彼-客製化履歷的基礎"><a href="#知己知彼-客製化履歷的基礎" class="headerlink" title="知己知彼 - 客製化履歷的基礎"></a>知己知彼 - 客製化履歷的基礎</h3><ul><li><p>認識自己</p><ul><li>自我盤點<ul><li>專長能力 : 相關就讀科系、實習、證照</li><li>性格、人際關係 : 社團、競賽、打工、經驗</li><li>興趣 : 本科專業、長期投注的課外活動</li></ul></li><li>表格化、量化</li></ul></li><li><p>職場硬實力</p><ul><li>學歷(含輔系、學程、校內外進修)</li><li>經歷(以實習、兼職工作為主)</li><li>專長技能</li><li>語文溝通能力</li><li>相關證照</li><li>電腦、網路軟硬體</li><li>專業儀器的使用</li></ul></li><li><p>職場軟實力</p><ul><li>人際關係<ul><li>同理心</li><li>團隊合作</li><li>領導力/指導性</li></ul></li><li>時間管理</li><li>相互討論/談判協商</li><li>問題解決<ul><li>發現與問題解決能力</li><li>分析邏輯</li><li>獨立思考</li><li>學習能力</li></ul></li><li>表達溝通</li><li>情緒智力 EQ</li><li>下定決定/決策力</li><li>語言能力的掌握</li></ul></li><li><p>認識工作與產業</p><ul><li>收集資料 : 網站、書籍、雜誌、PTT</li><li>人事廣告 : 入門到五年工作經驗</li><li>實務經驗分享 : 師長、學長姐、就業博覽會</li><li>實習機會 : 學校、寒暑假、政府部門、企業</li></ul></li><li><p>求職前要先了解內容</p><ul><li>企業面</li><li>職務面 </li></ul></li><li><p>企業文化差異</p><ul><li>蝦皮 - 麥肯錫企業文化</li><li>Pchome - 本土電商文化</li></ul></li><li><p>客製化履歷的下一步</p><ul><li>1.對這份工作的了解</li><li>2.我有那些能力可以做好這份工作</li><li>3.未來的職涯規劃與這份工作的相關性</li></ul></li></ul><h3 id="履歷表的格式"><a href="#履歷表的格式" class="headerlink" title="履歷表的格式"></a>履歷表的格式</h3><h2 id="下午場"><a href="#下午場" class="headerlink" title="下午場"></a>下午場</h2><h3 id="實習大叔-The-Internship"><a href="#實習大叔-The-Internship" class="headerlink" title="實習大叔(The Internship)"></a>實習大叔(The Internship)</h3><div class="video-container"><iframe src="//www.youtube.com/embed/_i7Qu4DJyfE" frameborder="0" allowfullscreen></iframe></div><p>《實習大叔》（英語：The Internship）是一部2013年上映的由肖恩·利維執導，文斯·沃恩 和Jared Stern編劇，文斯·沃恩和肖恩製片的美國喜劇劇情片。由文斯·沃恩和歐文·威爾森主演。</p><h4 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h4><p>中年失業的兩個大叔，與一群實習生一同來到google，他們與社會新鮮人不同，有著豐富的銷售經驗，卻也不了解現在最熱門的電腦與APP，實習生需要組成團隊進行競賽，最後與其他也是個性古怪的3人和經理人一同組成隊伍，在競賽過程中，經歷了許多不同年代的價值觀碰撞，我覺得這就跟我們現在遭遇的情形相似，每個人都年輕過，都有過崇高的夢想，但隨著課業跟現實，往往變得厭世，缺乏動力的年輕人，所以我很喜歡兩個大叔的心態，就算學歷不如人，也時常用自己擅長的溝通能力將不足的地方補齊，勇往直前勇於追夢。</p><p>每次看完這部電影的感想都不盡相同，與其當個沒有想法的人，我寧願專注做自己想做的事，不要侷限在小框框裡面，有甚麼有趣的事情就嘗試做做看，就算沒做完也不見得是件壞事，遇到挫折才知道難處，面對自己不擅長的事情，與其放棄不如勇敢面對，跟不同領域的人合作，可以看到自己的盲點，就像影片裡的大叔，只有行銷方面的專業，也是透過各種構通及學習才越來越厲害，也因為善於構通，在影片裡也認識了google專職的工程師，不起眼的地方往往會疏忽，也因此得到了許多幫助，就像與諮詢的案主溝通一樣，他們雖然不擅於統計，但也有各自的專長，有時候與他們聊天反而會看到更多社會的縮影，真的還蠻有趣的，希望我以後也能繼續這樣下去，不要只侷限在機器學習的世界裡。</p><h3 id="Career-就業情報"><a href="#Career-就業情報" class="headerlink" title="Career 就業情報"></a>Career 就業情報</h3><h4 id="經驗分享"><a href="#經驗分享" class="headerlink" title="經驗分享"></a>經驗分享</h4><ul><li>本科專業<ul><li>統計 </li><li>應數 </li><li>資管 </li><li>資科 </li></ul></li></ul><h4 id="提供網站"><a href="#提供網站" class="headerlink" title="提供網站"></a>提供網站</h4><p><a href="https://statfy.mol.gov.tw/" target="_blank" rel="noopener">勞動統計查詢網</a></p><p><img src="https://i.imgur.com/0JHlIzK.png" alt=""></p><p><a href="https://icap.wda.gov.tw/" target="_blank" rel="noopener">職能發展平台</a></p><p><img src="https://i.imgur.com/lB8rUzC.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;【職涯中心】職場透視營-II&quot;&gt;&lt;a href=&quot;#【職涯中心】職場透視營-II&quot; class=&quot;headerlink&quot; title=&quot;【職涯中心】職場透視營 II&quot;&gt;&lt;/a&gt;【職涯中心】職場透視營 II&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;時間 : 2018/12/08 
      
    
    </summary>
    
      <category term="演講心得" scheme="https://caocharles.github.io/categories/%E6%BC%94%E8%AC%9B%E5%BF%83%E5%BE%97/"/>
    
    
      <category term="work experience" scheme="https://caocharles.github.io/tags/work-experience/"/>
    
  </entry>
  
  <entry>
    <title>國泰大數據競賽心得</title>
    <link href="https://caocharles.github.io/%E5%9C%8B%E6%B3%B0%E5%A4%A7%E6%95%B8%E6%93%9A%E7%AB%B6%E8%B3%BD%E5%BF%83%E5%BE%97/"/>
    <id>https://caocharles.github.io/國泰大數據競賽心得/</id>
    <published>2018-11-28T20:18:50.000Z</published>
    <updated>2018-12-08T17:47:25.447Z</updated>
    
    <content type="html"><![CDATA[<h1 id="國泰大數據競賽心得"><a href="#國泰大數據競賽心得" class="headerlink" title="國泰大數據競賽心得"></a>國泰大數據競賽心得</h1><p><a href="https://campaign.cathaylife.com.tw/cathaybigdata/" target="_blank" rel="noopener">競賽網頁</a><br><img src="https://i.imgur.com/xzKUGzt.png" alt=""></p><h2 id="緣起"><a href="#緣起" class="headerlink" title="緣起"></a>緣起</h2><p>升碩二前的暑假，正在匆忙的進行實習趕稿，那個時候大概是正在學習怎麼製作聊天機器人的語料吧!突然手機上出現訊息通知，原來是展源在密我，問我有沒有空參加比賽，我就這樣踏進了這趟旅程。</p><h2 id="初賽"><a href="#初賽" class="headerlink" title="初賽"></a>初賽</h2><p>我們第一次討論好像是在新莊的咖啡廳吧，那個時候其實大家也都沒空參加講座，我好像剛忙完實習的樣子?就在那邊討論了一個下午，我們對於初賽的資料其實不太知道怎麼下手，那天大致討論完後，好像就交給展源清理資料，填補遺失值，我們就回去了。</p><p>到了要交件的前三天，我才開始趕工分析模型，那個時候算是對Python有點小小成就的新手吧，我就在網路上觀看機器學習的課程，用了超哥教過的隨機森林模型，和暑假跟柏龍討論已久的類神經網路，不過準確率依舊在9成初徘徊，最後在網路上收尋到XGBoost，測試後發現樹模型的建模速度很快，在調校上也容易操作，就這樣在短短幾天中漸漸的將模型訓練出來，不知道為甚麼，我覺得模型總是在半夜的時候會變得準確，就這樣慢慢的習慣熬夜了。</p><p>繳件過後其實沒有想太多，畢竟我們連講座都沒有去聽，用的也不是講座裡提供的模型，想說就當作機器學習的練習吧，還好最後順利晉級，沒有拖累隊友，耶!</p><h2 id="複賽"><a href="#複賽" class="headerlink" title="複賽"></a>複賽</h2><p>複賽時國泰提供遠端的服務，讓我以後也想要自己建個遠端來玩，因為看到別人的教授提供遠端電腦給其他同學用，真的蠻羨慕的，剛好就趁複賽的時候也來碰碰遠端，還好我們隊伍分工的很清楚，展源負責清理資料，宜謙負責撰寫說明跟整理文檔，我就可以用這些資料在半夜的時候慢慢做事，思索著到底該使用哪些資料建立模型。</p><p>不過複賽的資料很雜，而我們又不太了解保險商品內容，在資料整理那部份花了大半時間來查詢資訊，常常三個人討論完放空，不過這個舉動也影響我們決賽的行為(跑去概念店找靈感)，或許冥冥之中就是這樣，好模型是要講求緣分的，我在一個邊打瞌睡邊打code的情形下，找到了還不錯的變數欄位，就這樣開始根據這個構想把主附約的模型各別訓練出來，最後加上繁雜的敘述統計，整份文件說明也靠著展源和宜謙，最終寫滿了15頁書面報告繳件給主辦單位。</p><h2 id="決賽"><a href="#決賽" class="headerlink" title="決賽"></a>決賽</h2><p>看到決賽公布名單，那時的我正在上課，我邊喝著鮮奶茶邊顫抖著雙手，從來沒有想像沒正式比賽過的我，也能走到這一步，到這裡其實很感動，這就是我考研究所的初衷，如今的我，也正在實踐我的夢想，就這樣邊喝著奶茶邊思考著下一步該怎麼做。</p><p>知道決賽主題後的第一次討論，我們在研究室寫滿了好幾次白板，也沙盤推演了各種過程好幾次，其中我們也一直思索著推薦商品後所提供的誘因適不適合，在茫茫的收尋之中，無意間點開了國泰的概念店網站，我依稀有印象在那裡聽過，而且這個網頁給人一種清新的感覺，我們三個後來就決定找個假日探險去了。</p><p><a href="https://campaigns.cathaylife.com.tw/OAWeb/servlet/HttpDispatcher/OACS/index" target="_blank" rel="noopener">概念店網址</a><br><img src="https://i.imgur.com/heVRWqG.jpg" alt=""></p><p>我們約了繳件前的禮拜天，去到位於忠孝新生的概念店，3個人依序點了不同的飲品，我還是點了鮮奶茶，坐在那邊感受國泰概念店的氣氛，在這邊真的蠻適合討論保險，也很適合進行課程活動之類的，討論差不多後，我跟展源假扮成上班族的身分去諮詢保單，那因為我從小就是國泰的保戶，但我也不知道我有哪些保單，我就以我的資訊進行詢問，雖然諮詢的客服小姊姊一開始用驚訝疑惑的眼神看著我們兩個人，不過隨著時間慢慢閒聊跟諮詢，客服姊姊也漸漸開始幫我分析我的保單，在這諮詢之中，我逐漸了解到保險合約內容的適當性，也開始瞭解通常諮詢會遇到的問題有那些，要是複賽時有了解這些資訊，其實很多步驟都不用清理了，套句阿祥大大說的，你沒有足夠的Domain Knowledge是很難當好資料科學家的，我們用了網路上各種方法來訓練模型，卻時常忘記放入這變數的意義是甚麼，這也算是我從展源身上學習到的地方吧，展源很注重欄位的解釋，一定要能夠合理解釋才能放進模型，而我們卻常常一股腦將擁有的資訊全部丟入模型訓練，像是我們到決賽才知道投資型保單有限定年齡跟繳費方式，它跟一般保障保單是不同的，而且國泰的許多網頁也是將其險種分開，這也影響了我們後來推薦系統的建立，與小姐姐熱絡的聊天之後，我拿了厚厚的保險推薦回來，後來大家一起去了松菸走走，最後搭車回政大把整個流程跟時間規劃討論出來，這一步討論到我跟展源回家時都已經錯過了末班車，只能搭計程車回家，兩人各噴了500，不過我想這時候我已有足夠大的信心能將這次決賽完成，就完成了第二次討論。</p><p><img src="https://i.imgur.com/pHd8q7u.jpg" alt=""></p><p>繳件前的禮拜一，我們討論要如何設計流程與呈現畫面的最終版，之後我查詢了許多推薦系統介面的論文，很多都需要一定的UI程式設計，我看著看著，發現網路上有許多漂亮的畫面呈現在我眼前，我就心想著我還有多少時間，可不可以把所有事情都推掉專心做一次就好，就順利的把助教課跟教授開會排開，花了一個晚上，到禮拜二早上8點大概學習了60%的操作，就這樣回宿舍睡覺了，睡醒後大概下午，接著就繼續排版製作我的第一個UI介面，也順便讓展源回去休息，這樣我才能跟宜謙拍神秘計畫的影片(哈哈)。</p><p><img src="https://i.imgur.com/a7mT0R3.png" alt=""></p><p>繳件前的禮拜三，我難得早起去上清祥的課，深怕他是評審又一直看到我沒來上課，我最後還是到課堂上繼續製作我的ADOBE XD，這時候大概進入完成40%介面，這天我後來就繼續在國貿所的討論室製作，轉眼間一天又過去。<br>那時候初版的製作大概長這樣，不過看到自己的作品一步步慢慢製作出來，就有更多動力繼續做下去了，旁邊顯示的保險都是小姐姐推薦給我的。<br><img src="https://i.imgur.com/8InNpkR.png" alt=""></p><p>繳件前的禮拜四五，這兩天很簡單，就是一直趕趕趕，中間過程我都沒有睡覺，整個Demo製作完後，開始要寫複賽的QA，展源負責製作PPT，我們兩個腦力激盪了一整晚跟早，終於在最後整合出PPT，並將QA一同交出，我回去洗澡完也接著上讀書會，不過想當然的睡倒在椅子上。</p><p>繳件後的幾天，我們從擬稿到請同學來觀摩，過程很匆忙，感謝柏龍跟初初，願意花時間給予我們回饋跟建議，也不斷的練習台風、口號，希望在決賽場上有好表現。</p><h2 id="決賽當日"><a href="#決賽當日" class="headerlink" title="決賽當日"></a>決賽當日</h2><p>前往國泰金融會議中心!!!!!<br><img src="https://i.imgur.com/sfAnCKj.png" alt=""></p><p>我穿著著第一次套裝的西裝，腦中不斷演練待會要報告的詞語，如何在舞台上不要怯場，將整個隊伍在總結的時候帶出氣勢，展源抽到了第4組，跟柯P一樣的號碼，這時我內心多了幾分平穩，也許是半夜練稿的時候我都會看柯P的演講的關係吧，這次我們的主題「革命發生，改變成真」，就那麼剛好發生在我們身上，我們上台報告的時候，看到別老師一直觀看我們講解核心理念，頻頻的點頭如搗蒜，真的給予我很大的信心，演示系統的時候看到台下開心的掃描QR CODE，又給予了我很大的勇氣，在宜謙和展源介紹完之後，就一路由我從回饋機制講到QA時間，但是這時候我站在台上的時候心情卻很雀躍，這就是我的舞台呀，我的夢想呀，聽到評審給予我們極高的評價之後，我認真思索著評審提出的問題並回答，其實有些問題是我們來不及製作完UI時就有想到的，也曾經在我們的沙盤推演中出現過，不過還好在QA時評審有提出，讓我們能更完整講解我們的系統。</p><p>等待其他組報告中~<br><img src="https://i.imgur.com/UBFqSbA.jpg" alt=""></p><p>團隊合照<br><img src="https://i.imgur.com/uQuwgee.jpg" alt=""></p><p>緊張的頒獎時間啦，我們在不經意的時候聽到評審說到我們名字，我們在還搞不懂得獎名稱的時候上台頒獎，我想應該是我們的作品真的DEMO出來大家很喜歡吧，不然其他組的PPT我都好喜歡呀，但似乎還差一個名次這次比賽才完美。<br><img src="https://i.imgur.com/P1ux8TY.jpg" alt=""></p><p>在前面頒發三個獎之後，中途又經過了一段等待時間，到了最最最刺激的時候，這個時候誰會拿前3名真的很難預料，這次參賽隊伍水準超高，各種動畫網頁呈現，當我看到評審翻開名冊後的眼神，那隊伍名稱一定很奇怪，而且好像跟剛剛有重複，沒錯!就是我們泰泰我喜歡你R，一想到這4個月努力的成果有收穫，內心狂奮不已，開心的上台領獎。<br><img src="https://i.imgur.com/qQDQERC.jpg" alt=""></p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>說長不長，說短不短的四個月，其中伴隨著助教課，跟教授開會，十月時接了三個統計諮詢，不過我想時間是可以壓榨出來，只要有奶茶，一切都沒問題。我們一路走來都是意外，意料之外的意外，應該沒有人覺得我們這組是來報名拚名次的。也因為這樣，我們越能心無旁騖地往正確的方向邁進，每次討論時，都經歷漫長的沙盤推演及眾人不斷的討論，最後整合意見，將每次結果記錄下來。在這之中，我發現跟展源和宜謙討論時，我們往往都有不同領域的思考跟想法，我把我的經驗跟邏輯講解給隊友們聽，你們也會給予我不同領域的思考邏輯，這是我在系上學不到的經驗，像有時候你們會在一些設計上的缺陷給予我建議，這是我自己一個人做不到的事，就這樣我們一路奮鬥到了決賽場上，靠著我們的優勢報告我們的商品內容，最後也拿到了不錯的名次。我想這次瘋狂的競賽經驗，對於我們未來也都有很大幫助，之後想進國泰應該直接拿畢業證書跟競賽獎狀就沒問題了吧。</p><p>最後附上一首歌曲代表這陣子的努力</p><div class="video-container"><iframe src="//www.youtube.com/embed/NbNPJr_0tqA" frameborder="0" allowfullscreen></iframe></div><p>給我一奶茶<br>再給我一盞燈<br>說敖就熬<br>我有的是時間<br>我不想在未來的日子裡<br>留下遺憾沒有目標</p><p>給我一奶茶<br>再給我一盞燈<br>說做就做<br>我有的是時間<br>我不想在未來的日子裡<br>看著過去懊悔萬分</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;國泰大數據競賽心得&quot;&gt;&lt;a href=&quot;#國泰大數據競賽心得&quot; class=&quot;headerlink&quot; title=&quot;國泰大數據競賽心得&quot;&gt;&lt;/a&gt;國泰大數據競賽心得&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://campaign.cathaylife.com.t
      
    
    </summary>
    
      <category term="國泰人壽" scheme="https://caocharles.github.io/categories/%E5%9C%8B%E6%B3%B0%E4%BA%BA%E5%A3%BD/"/>
    
      <category term="國泰大數據競賽" scheme="https://caocharles.github.io/categories/%E5%9C%8B%E6%B3%B0%E4%BA%BA%E5%A3%BD/%E5%9C%8B%E6%B3%B0%E5%A4%A7%E6%95%B8%E6%93%9A%E7%AB%B6%E8%B3%BD/"/>
    
    
      <category term="Competition" scheme="https://caocharles.github.io/tags/Competition/"/>
    
      <category term="CathyLife" scheme="https://caocharles.github.io/tags/CathyLife/"/>
    
  </entry>
  
  <entry>
    <title>商業簡報製作</title>
    <link href="https://caocharles.github.io/%E5%95%86%E6%A5%AD%E7%B0%A1%E5%A0%B1%E8%A3%BD%E4%BD%9C/"/>
    <id>https://caocharles.github.io/商業簡報製作/</id>
    <published>2018-11-10T15:42:01.000Z</published>
    <updated>2018-12-08T17:46:59.056Z</updated>
    
    <content type="html"><![CDATA[<h1 id="簡報課in國泰-11-10"><a href="#簡報課in國泰-11-10" class="headerlink" title="簡報課in國泰 (11/10)"></a>簡報課in國泰 (11/10)</h1><ul><li>時間 : 2018/11/10 13:30-15:30</li><li>地點 : 國泰演講廳</li><li>茶點 : 貓茶町</li></ul><h2 id="Lecture-1-商業簡報製作"><a href="#Lecture-1-商業簡報製作" class="headerlink" title="Lecture 1 商業簡報製作"></a>Lecture 1 商業簡報製作</h2><h3 id="簡報內容"><a href="#簡報內容" class="headerlink" title="簡報內容"></a>簡報內容</h3><ul><li>講師 : 黃紹峰</li><li>課程 : 商業簡報製作</li></ul><p><img src="https://i.imgur.com/cuFRjCr.png" alt=""></p><p>聽得懂、坐得住的簡報</p><p>只有15分鐘要怎麼做簡報</p><p><img src="https://i.imgur.com/ajnhJ8z.png" alt=""></p><h3 id="14-mins-報告"><a href="#14-mins-報告" class="headerlink" title="14 mins 報告"></a>14 mins 報告</h3><p><strong>流程安排</strong></p><ul><li>Hello 30s<ul><li>簡單介紹自己隊伍</li></ul></li><li>Problem 1m<ul><li>闡述問題 : 定義問題越簡單越好</li></ul></li><li>Solution 4m<ul><li>F,A,B</li><li>Feature 功能</li><li>Advantage 優點</li><li>Benifit 效益</li></ul></li><li>Demo 4m30s<ul><li>Steps</li><li>before/after</li></ul></li><li>Feedback 1m<ul><li>Who backs you up?</li></ul></li><li>Review 3m<ul><li>Recap,sum up</li></ul></li></ul><h3 id="接棒的問題"><a href="#接棒的問題" class="headerlink" title="接棒的問題"></a>接棒的問題</h3><ul><li>上台的人要怎麼湊在一起，如何接棒</li><li>避免重複性的話語</li><li>重複演練減少重複性</li></ul><h2 id="10mins-QA"><a href="#10mins-QA" class="headerlink" title="10mins QA"></a>10mins QA</h2><ul><li><p>Take notes(準備紙筆)</p></li><li><p>Appendix(附件)</p></li></ul><h2 id="製作簡報"><a href="#製作簡報" class="headerlink" title="製作簡報"></a>製作簡報</h2><h3 id="第1步-不要套用模板"><a href="#第1步-不要套用模板" class="headerlink" title="第1步(不要套用模板)"></a>第1步(不要套用模板)</h3><h4 id="Make-your-temps"><a href="#Make-your-temps" class="headerlink" title="Make your temps"></a>Make your temps</h4><p><img src="https://i.imgur.com/eNpYZCu.png" alt=""><br><img src="https://i.imgur.com/m7U30FR.png" alt=""><br><img src="https://i.imgur.com/yBZJIfD.png" alt=""></p><ul><li>查爾斯的模板</li></ul><h3 id="第2步-字型"><a href="#第2步-字型" class="headerlink" title="第2步(字型)"></a>第2步(字型)</h3><p><img src="https://i.imgur.com/N8pCLlz.png" alt=""></p><ul><li>選擇沒有襯線的字型比較好看</li><li>Google NOTO Sans(思源體)</li><li>微軟正黑</li><li><p>微軟雅黑</p></li><li><p>字的大小(28以上)、粗細、間距</p></li><li>用字的大小來排表(做出圖表效果)</li></ul><h3 id="第3步-訊息"><a href="#第3步-訊息" class="headerlink" title="第3步(訊息)"></a>第3步(訊息)</h3><p><img src="https://i.imgur.com/xsmvApn.png" alt=""></p><ul><li>Message = Info - Noise </li><li>強調重點(顏色、粗細、)</li><li>次重點做弱化(做一個透明度80%的灰片把資訊覆蓋住)</li></ul><h3 id="第4步-圖片"><a href="#第4步-圖片" class="headerlink" title="第4步(圖片)"></a>第4步(圖片)</h3><p><img src="https://i.imgur.com/uwxyAgE.png" alt=""></p><ul><li>選擇適合的圖片</li><li>選擇png去背好的圖片</li><li>用圖去搜圖</li><li>使用photoshop去背完再貼上</li><li>png 很棒，有去背</li></ul><h4 id="ICON"><a href="#ICON" class="headerlink" title="ICON"></a>ICON</h4><p><img src="https://i.imgur.com/hVzdAvp.png" alt=""></p><ul><li>可以找跟大樹的ICON(感覺蠻不錯的)</li><li>快速找取現成的ICON</li><li>nounproject(向量檔)<ul><li><a href="https://thenounproject.com/" target="_blank" rel="noopener">https://thenounproject.com/</a></li></ul></li><li>flaticon(查爾斯愛用的)<ul><li><a href="https://www.flaticon.com/" target="_blank" rel="noopener">https://www.flaticon.com/</a></li></ul></li><li>undraw(圖)<ul><li><a href="https://undraw.co/" target="_blank" rel="noopener">https://undraw.co/</a></li></ul></li><li>找的ICON要有一致性</li><li>漸層ICON(使用photoshop)</li><li>ICON下面最好加一行註解</li></ul><h3 id="第5步-Tone色調"><a href="#第5步-Tone色調" class="headerlink" title="第5步(Tone色調)"></a>第5步(Tone色調)</h3><p><img src="https://i.imgur.com/zRKIzLx.png" alt=""></p><ul><li>設備的新舊<ul><li>舊的設備(白底黑字較好)</li><li>新的設備(黑底白字較好)</li><li>螢幕很大(黑底白字較好)</li></ul></li><li>顏色上的取用<ul><li>配合公司的主題顏色</li><li>選擇美的、耐看的顏色</li><li>符合自己主題的顏色</li></ul></li></ul><ul><li>Powerpoint只吃MP4(比賽應該用不到)</li></ul><h3 id="第6步-Diagrams數據及資料"><a href="#第6步-Diagrams數據及資料" class="headerlink" title="第6步(Diagrams數據及資料)"></a>第6步(Diagrams數據及資料)</h3><p><img src="https://i.imgur.com/PTcjlGf.png" alt=""></p><ul><li>圖表最好都重新製作一次</li><li>使用圖的方式表達(圓餅圖)</li><li>不能亂用，會被清祥罵</li></ul><h3 id="第7步-Animation"><a href="#第7步-Animation" class="headerlink" title="第7步(Animation)"></a>第7步(Animation)</h3><p><img src="https://i.imgur.com/Rz69rDD.png" alt=""></p><ul><li>我也不想用(迪士尼的看起來還不錯耶)</li></ul><p><img src="https://i.imgur.com/XMg06ag.png" alt=""></p><ul><li>簡報沒內容可以用動畫跟華麗背景克服(尤其是清祥的課)</li></ul><h3 id="第8步-Details-我最愛的細節"><a href="#第8步-Details-我最愛的細節" class="headerlink" title="第8步(Details 我最愛的細節)"></a>第8步(Details 我最愛的細節)</h3><p><img src="https://i.imgur.com/Mr5ZntK.png" alt=""></p><ul><li>Alignment(一致性)<ul><li>字體、大小、圖表、ICON、中英文</li></ul></li><li>Use gradient()<ul><li>不要使用設計建議</li><li>忠於自己的美感</li></ul></li><li>Use less 3D()<ul><li>3D過時</li><li>不要反射(不流行)</li><li><strong>Position</strong></li></ul></li></ul><p><strong>Before v.s. After</strong></p><p><strong>Leave some balnk</strong></p><p><strong>Don’t roll slides</strong></p><p><strong>On Time!!!!!</strong></p><ul><li>要做逐字稿，減少贅詞的使用，加強說話方式(很難但是有用)</li><li>可以用不講話來<strong>強調</strong></li><li>想要表達自己做很多事<ul><li>放很多東西，或是密集的表格</li><li>不用講太多</li><li>就跟寫清祥的作業一樣</li></ul></li></ul><h3 id="好看的排版"><a href="#好看的排版" class="headerlink" title="好看的排版"></a>好看的排版</h3><p><img src="https://i.imgur.com/uxlfDQY.png" alt=""></p><p><img src="https://i.imgur.com/68vfYzV.jpg" alt=""></p><p><img src="https://i.imgur.com/arHXPL3.jpg" alt=""></p><p><img src="https://i.imgur.com/XT8QBUe.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;簡報課in國泰-11-10&quot;&gt;&lt;a href=&quot;#簡報課in國泰-11-10&quot; class=&quot;headerlink&quot; title=&quot;簡報課in國泰 (11/10)&quot;&gt;&lt;/a&gt;簡報課in國泰 (11/10)&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;時間 : 2018/11/10 
      
    
    </summary>
    
      <category term="國泰人壽" scheme="https://caocharles.github.io/categories/%E5%9C%8B%E6%B3%B0%E4%BA%BA%E5%A3%BD/"/>
    
      <category term="簡報課程在國泰" scheme="https://caocharles.github.io/categories/%E5%9C%8B%E6%B3%B0%E4%BA%BA%E5%A3%BD/%E7%B0%A1%E5%A0%B1%E8%AA%B2%E7%A8%8B%E5%9C%A8%E5%9C%8B%E6%B3%B0/"/>
    
    
      <category term="PPT" scheme="https://caocharles.github.io/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>商業簡報台風</title>
    <link href="https://caocharles.github.io/%E5%95%86%E6%A5%AD%E7%B0%A1%E5%A0%B1%E5%8F%B0%E9%A2%A8/"/>
    <id>https://caocharles.github.io/商業簡報台風/</id>
    <published>2018-11-10T15:42:00.000Z</published>
    <updated>2018-12-08T17:46:30.394Z</updated>
    
    <content type="html"><![CDATA[<h1 id="簡報課in國泰-11-10"><a href="#簡報課in國泰-11-10" class="headerlink" title="簡報課in國泰 (11/10)"></a>簡報課in國泰 (11/10)</h1><ul><li>時間 : 2018/11/10 15:30-17:00</li><li>地點 : 國泰演講廳</li><li>茶點 : 貓茶町</li></ul><h2 id="Lecture-2-簡報商業台風"><a href="#Lecture-2-簡報商業台風" class="headerlink" title="Lecture 2 簡報商業台風"></a>Lecture 2 簡報商業台風</h2><h3 id="簡報內容"><a href="#簡報內容" class="headerlink" title="簡報內容"></a>簡報內容</h3><ul><li>講師 : Ben ou</li><li>課程 : 簡報商業台風</li></ul><p><img src="https://i.imgur.com/sr0jgSW.png" alt=""></p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><img src="https://i.imgur.com/uKtEDtx.jpg" alt=""></p><h3 id="30秒電梯理論"><a href="#30秒電梯理論" class="headerlink" title="30秒電梯理論"></a>30秒電梯理論</h3><p><img src="https://i.imgur.com/EyANelg.png" alt=""></p><ul><li>語出驚人<ul><li>好的開始是成功的一半</li><li>郁達夫文集(快短命)<ul><li>快，痛快</li><li>短，簡明扼要</li><li>命，不離命題</li></ul></li></ul></li><li>短小精悍<ul><li>抓住根本，直達主體</li><li>提綱挈領，化繁為簡</li></ul></li><li>提煉觀點<ul><li>觀點要獨特響亮</li><li>歸納要緊湊，不超過三條</li></ul></li></ul><h3 id="麥肯錫金字塔方法"><a href="#麥肯錫金字塔方法" class="headerlink" title="麥肯錫金字塔方法"></a>麥肯錫金字塔方法</h3><ul><li><a href="https://www.managertoday.com.tw/articles/view/51650" target="_blank" rel="noopener">MECE原則-參考網頁</a></li><li><p>麥肯錫解決問題7步驟</p></li><li><p>解決問題的不是與生俱來的天賦，而是可以透過自我訓練培養而成，你也能像麥肯錫人一樣，倍速解決問題。</p></li></ul><p><img src="https://i.imgur.com/cC3uNcu.png" alt=""></p><ul><li>金字塔架構</li></ul><p><img src="https://i.imgur.com/z7HTicZ.png" alt=""></p><ul><li>金字塔方法</li></ul><p><img src="https://i.imgur.com/TtXXMQf.png" alt=""></p><ul><li>金字塔原理四個基本特徵</li></ul><p><img src="https://i.imgur.com/rBN0Ype.png" alt=""></p><ul><li>金字塔結構</li></ul><p><img src="https://i.imgur.com/ojYcAmT.png" alt=""></p><h3 id="SDS結果法"><a href="#SDS結果法" class="headerlink" title="SDS結果法"></a>SDS結果法</h3><p><img src="https://i.imgur.com/AJ1BQG1.jpg" alt=""></p><ul><li>先講結論</li><li>再講經過</li><li>最後再重複結論</li></ul><h3 id="注意事項"><a href="#注意事項" class="headerlink" title="注意事項"></a>注意事項</h3><p><img src="https://i.imgur.com/4PQ1goG.png" alt=""></p><h3 id="開頭"><a href="#開頭" class="headerlink" title="開頭"></a>開頭</h3><ul><li>破題</li><li>邏輯結構</li><li>記憶點</li><li>使用者場景</li><li>互動&amp;張力</li></ul><h3 id="結尾"><a href="#結尾" class="headerlink" title="結尾"></a>結尾</h3><p><strong>Key Take Away</strong><br><img src="https://i.imgur.com/ILAcEXB.jpg" alt=""></p><ul><li>至少讓觀眾記住一個畫面</li></ul><p><strong>峰尾效應</strong><br><img src="https://i.imgur.com/kbZbku4.jpg" alt=""></p><p><strong>記憶點</strong><br><img src="https://i.imgur.com/2VOINzm.jpg" alt=""></p><ul><li>有記憶點的結束!</li><li>Mamba out!</li></ul><div class="video-container"><iframe src="//www.youtube.com/embed/YK4E3nMR-KU" frameborder="0" allowfullscreen></iframe></div><ul><li>再一個記憶點的結束!</li><li>Kobe Bryant Slow Motion Shooting Compilation </li></ul><div class="video-container"><iframe src="//www.youtube.com/embed/9GkxFN8-PM0" frameborder="0" allowfullscreen></iframe></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;簡報課in國泰-11-10&quot;&gt;&lt;a href=&quot;#簡報課in國泰-11-10&quot; class=&quot;headerlink&quot; title=&quot;簡報課in國泰 (11/10)&quot;&gt;&lt;/a&gt;簡報課in國泰 (11/10)&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;時間 : 2018/11/10 
      
    
    </summary>
    
      <category term="國泰人壽" scheme="https://caocharles.github.io/categories/%E5%9C%8B%E6%B3%B0%E4%BA%BA%E5%A3%BD/"/>
    
      <category term="簡報課程在國泰" scheme="https://caocharles.github.io/categories/%E5%9C%8B%E6%B3%B0%E4%BA%BA%E5%A3%BD/%E7%B0%A1%E5%A0%B1%E8%AA%B2%E7%A8%8B%E5%9C%A8%E5%9C%8B%E6%B3%B0/"/>
    
    
      <category term="PPT" scheme="https://caocharles.github.io/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>人工智慧第4周筆記</title>
    <link href="https://caocharles.github.io/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E7%AC%AC4%E5%91%A8%E7%AD%86%E8%A8%98/"/>
    <id>https://caocharles.github.io/人工智慧第4周筆記/</id>
    <published>2018-10-08T09:30:19.000Z</published>
    <updated>2018-12-08T17:50:54.802Z</updated>
    
    <content type="html"><![CDATA[<h1 id="人工智慧與應用筆記4"><a href="#人工智慧與應用筆記4" class="headerlink" title="人工智慧與應用筆記4"></a>人工智慧與應用筆記4</h1><h2 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h2><p>其他組報告老師的講義<br>大家都在看莫凡講解類神經網路</p><h2 id="CSM-演算法"><a href="#CSM-演算法" class="headerlink" title="CSM 演算法"></a>CSM 演算法</h2><p>增加node的流程 (有7個步驟)</p><h4 id="The-proposed-CSM-learning-procedure"><a href="#The-proposed-CSM-learning-procedure" class="headerlink" title="The proposed CSM learning procedure"></a>The proposed CSM learning procedure</h4><ul><li><p>判斷是不是熟悉的case</p></li><li><p>When we encounter a new case (a new input/output relationship), we first check if it is familiar to us.</p><ul><li>If it is, there is no spontaneous learning effort involved. Later the new case is merged into our knowledge system.</li><li><p>If it is not, we might cram this unfamiliar case first. The cramming results in a strict rule with respect to this unfamiliar case. Then we will soften the strictness of the new case  and do our best to merge the new case into our knowledge system.</p></li><li><p>面對的問題如果是曾經有過的資料，則會選擇相關的資訊回應。</p></li><li>面對的問題如果是未曾有過的資料，則會選擇增加一個新回應。</li></ul></li></ul><h4 id="ASLFN-3層的類神經網路-輸入-gt-隱藏-gt-輸出層"><a href="#ASLFN-3層的類神經網路-輸入-gt-隱藏-gt-輸出層" class="headerlink" title="ASLFN (3層的類神經網路 輸入-&gt;隱藏-&gt;輸出層)"></a>ASLFN (3層的類神經網路 輸入-&gt;隱藏-&gt;輸出層)</h4><ul><li>The Adaptive Single-hidden Layer Feed-forward Neural Networks (ASLFN, i.e., the amount of adopted hidden nodes is variable)  (references: Tsaih 1993; Tsaih 1998; Tsaih and Cheng 2009; Huang, Yu, Tsaih and Huang Tsaih 2014; Kuo, Lin, and Hsu 2018)</li><li>2-class categorization problem and binary inputs {−1, 1}^𝑚 </li></ul><p><img src="https://i.imgur.com/xrGDFGu.png" alt=""></p><h4 id="Activation-Function-激活函數"><a href="#Activation-Function-激活函數" class="headerlink" title="Activation Function (激活函數)"></a>Activation Function (激活函數)</h4><ul><li>tanh, the hyperbolic tangent activation function, is used in all hidden nodes.</li></ul><p><img src="https://i.imgur.com/WbOB70X.png" alt=""></p><h4 id="The-Activation-Value-of-Hidden-Nodes-隱藏層的激活值"><a href="#The-Activation-Value-of-Hidden-Nodes-隱藏層的激活值" class="headerlink" title="The Activation Value of Hidden Nodes (隱藏層的激活值)"></a>The Activation Value of Hidden Nodes (隱藏層的激活值)</h4><ul><li>激活函數不能亂給，會有梯度爆炸的情形發生。</li></ul><p><img src="https://i.imgur.com/9SY8FZS.png" alt=""></p><p>$𝐱^𝑐≡(𝑥_1^𝑐,𝑥_2^𝑐, …,𝑥_𝑚^𝑐 )T：𝑡ℎ𝑒　input　vector　of　the　𝑐^th　case$</p><p>$𝑤_{𝑖0}^{𝐻}：the　threshold　value　of　the　𝑖^th　hidden　node$</p><p>$𝑤_𝑖𝑗^𝐻：the　weight　between　the　𝑖^th　hidden　node　and　the　𝑗^th　input　node$</p><p>$𝐰_𝑖^𝐻≡(𝑤_𝑖0^𝐻,𝑤_𝑖1^𝐻,𝑤_𝑖2^𝐻, …,𝑤_𝑖𝑚^𝐻 )^T$; $𝐰^𝐻≡{({𝐰_1^𝐻}^T,{𝐰_2^𝐻}^T,…,{𝐰_𝑝^𝐻}^T)}^T$</p><h4 id="The-Activation-Value-of-the-Output-Node-輸出層的激活數值"><a href="#The-Activation-Value-of-the-Output-Node-輸出層的激活數值" class="headerlink" title="The Activation Value of the Output Node (輸出層的激活數值)"></a>The Activation Value of the Output Node (輸出層的激活數值)</h4><p><img src="https://i.imgur.com/UfkCp1N.png" alt=""></p><h4 id="Parameters-and-Indexes-參數"><a href="#Parameters-and-Indexes-參數" class="headerlink" title="Parameters and Indexes (參數)"></a>Parameters and Indexes (參數)</h4><ul><li>N denotes the number of all reference observations</li><li>m denotes the number of input nodes</li><li>p denotes the number of adopted hidden nodes; p equals 1 at the beginning and is adaptive</li><li>$𝑦^𝑐$ denotes the desired output of the $𝑐^th$ case, with 1.0 and -1.0 being the desired outputs of classes 1 and 2</li><li>n denotes the $𝑛^th$ stage of handling n reference observations ${(𝐱^1, 𝑦^1), (𝐱^2, 𝑦^2), …, (𝐱^𝑛, 𝑦^𝑛)}$, and $𝐈(𝑛)$ is the set of indices of these observations. </li><li>At the $𝑛^th$ stage, the loss function $𝐸_𝑛 (𝐰)≡\frac{∑_{𝑐=1}^{𝑛}(𝑓(𝐱^𝑐,𝐰)−𝑦^𝑐 )^2}{𝑛}+10^{-3}‖𝐰‖^2$</li><li>$𝐈(𝑛)≡𝐈_1 (𝑛)∪𝐈_2 (𝑛)$, where$𝐈_1(𝑛)$ and $𝐈_2(𝑛)$ are the set of indices of n given cases in classes 1 and 2</li><li>At the $𝑛^th$ stage with the reference observations ${(𝐱^𝑐, 𝑦^𝑐): 𝑐∈𝐈(𝑛)}$, we look for an acceptable SLFN, in which the condition L regarding ${𝑓(𝐱^𝑐,𝐰), ∀ 𝑐∈𝐈(𝑛)}$ is satisfied.</li></ul><h4 id="The-condition-𝐿-regarding-𝑓-𝐱-𝑐-𝐰-∀-𝑐∈𝐈-𝑛"><a href="#The-condition-𝐿-regarding-𝑓-𝐱-𝑐-𝐰-∀-𝑐∈𝐈-𝑛" class="headerlink" title="The condition 𝐿 regarding ${𝑓(𝐱^𝑐,𝐰),  ∀ 𝑐∈𝐈(𝑛)}$"></a>The condition 𝐿 regarding ${𝑓(𝐱^𝑐,𝐰),  ∀ 𝑐∈𝐈(𝑛)}$</h4><p><img src="https://i.imgur.com/oi1esy8.png" alt=""></p><h4 id="The-Learning-Goal-學習目標"><a href="#The-Learning-Goal-學習目標" class="headerlink" title="The Learning Goal (學習目標)"></a>The Learning Goal (學習目標)</h4><ul><li>At the $𝑛^{th}$ stage, through minimizing the loss function </li><li>$𝐸_𝑛 (𝐰)≡\frac{∑_{𝑐=1}^{𝑛}(𝑓(𝐱^𝑐,𝐰)−𝑦^𝑐 )^2}{𝑛}+10^{-3} (∑_{𝑖=0}^{𝑝}{(w_𝑖^o)^2} + ∑_{𝑖=1}^{𝑝}∑_{𝑗=0}^{𝑚}(𝑤_{𝑖𝑗}^{𝐻})^2)$,the learning goal is to seek $𝐰$ where $𝑓(𝐱^𝑐,𝐰)&gt;𝜈,∀ 𝑐∈𝐈_1 (𝑛)$  and $𝑓(𝐱^𝑐,𝐰)≤−𝜈,∀ 𝑐∈𝐈_2(𝑛)$, with $1&gt;𝜈&gt;0$. An alternative goal of learning is to seek 𝐰 that satisfies the condition 𝐿 regarding ${𝑓(𝐱^𝑐,𝐰),  ∀ 𝑐∈𝐈(𝑛)}$ </li><li>When $𝛼&gt;𝛽$ is satisfied, $𝑓(𝐱^𝑐,𝐰)≥𝑣 ,∀ 𝑐∈𝐈_1(𝑛)$and $𝑓(𝐱^𝑐,𝐰)≤−𝑣,∀ 𝑐∈𝐈_2(𝑛)$ can be achieved by directly adjusting $𝐰^𝑜$ according to the following:</li><li><img src="https://i.imgur.com/Ab3gKdl.png" alt=""></li></ul><h4 id="The-Proposed-CSM-Learning-Algorithm"><a href="#The-Proposed-CSM-Learning-Algorithm" class="headerlink" title="The Proposed CSM Learning Algorithm"></a>The Proposed CSM Learning Algorithm</h4><ul><li><p>Step 1: Initialize a SLFN with one hidden node with a randomized w. Obtain the first reference observation $(𝐱^1, 𝑦^1)$. Set $n = 2$.</p></li><li><p>Step 2: If $n &gt; N$, STOP.</p></li><li><p>Step 3: Present the n reference observations ${(𝐱^𝑐, 𝑦^𝑐): c ∈ I(n)}$.</p></li><li><p>Step 4: If the condition L regarding ${𝑓(𝐱^𝑐,𝐰),  ∀ 𝑐∈𝐈(𝑛)}$ is satisfied, go to Step 7.1.</p></li><li><p>Step 5: Save $w$.</p></li><li><p>Step 6: Apply the weight-tuning mechanism to $min_{𝐰}{⁡𝐸(𝐰)}$ to adjust $w$ until one of the following two cases occurs:</p><ul><li>a. If the condition L regarding {𝑓(𝐱^𝑐,𝐰),  ∀ 𝑐∈𝐈(𝑛)} is satisfied, go to step 7.1.</li><li>b. If an unacceptable result is obtained, then<ul><li>Restore w</li><li>Let $p + 1 → p$ and add to the existing SLFN the new $𝑝^{th}$ hidden node with the following $𝐰_𝑝^𝐻$ and $𝐰_𝑝^𝑜$:</li></ul></li></ul></li></ul><p><img src="https://i.imgur.com/LXvc9b4.png" alt=""></p><p><img src="https://i.imgur.com/E8JbXdq.png" alt=""></p><ul><li>Step 7.1: Apply the weight-tuning mechanism one hundred times to minimizing $𝐸_𝑛(𝐰)$ to adjust $𝐰$, while keeping the condition L regarding ${𝑓(𝐱^𝑐,𝐰),  ∀ 𝑐∈𝐈(𝑛)}$ satisfied. </li><li>Step 7.2: Calculate $𝑔_𝑘^′  ∀ 𝑘$, where $𝐰_𝑘^{′}≡ 𝐰 – ({𝑤_𝑘^𝑜, 𝐰_𝑘^𝐻})$, $𝑓(𝐱^𝑐,𝐰_𝑘^′ )≡𝑓(𝐱^𝑐,𝐰)- 𝑤_{𝑘}^{𝑜}𝑎_{𝑘}^{𝑐}$,$𝛼_𝑘^′≡ min_{𝑐ϵ𝐈_1(𝑛)}⁡𝑓(𝐱^𝑐,𝐰_𝑘^′ )$, $𝛽_𝑘^′≡max_{𝑐ϵ𝐈_2(𝑛)}⁡𝑓(𝐱^𝑐,𝐰_𝑘^′)$,  and $𝑔_𝑘^′≡ 𝛼_𝑘^′−𝛽_𝑘^′$.</li><li>Step 7.3: If $- \theta &gt;max_{1&lt;k&lt;p}{g_{k}^{‘}}$, where $\theta$ is a given constant, go to Step 2. </li><li>Step 7.4: If $max_{1&lt;k&lt;p}^⁡{𝑔_𝑘^′} &gt; 0$, prune the $𝑖^{th}$ hidden node, in which 𝑖 is the first index of $arg　max_{1&lt;k&lt;p}𝑔_{𝑘}^{′}$, $p-1-&gt;p$, $𝐰_𝑖^′-&gt;w$, and go to Step 7.1.<br><img src="https://i.imgur.com/PC3Kvtw.png" alt=""><br><img src="https://i.imgur.com/SmjWEFq.png" alt=""></li></ul><h4 id="Flowchart-of-the-proposed-algorithm-流程圖"><a href="#Flowchart-of-the-proposed-algorithm-流程圖" class="headerlink" title="Flowchart of the proposed algorithm (流程圖)"></a>Flowchart of the proposed algorithm (流程圖)</h4><p><img src="https://i.imgur.com/beUgMB8.png" alt=""></p><h4 id="Explanation-of-the-Proposed-CSM-Learning-Algorithm-解釋CSM學習演算法"><a href="#Explanation-of-the-Proposed-CSM-Learning-Algorithm-解釋CSM學習演算法" class="headerlink" title="Explanation of the Proposed CSM Learning Algorithm (解釋CSM學習演算法)"></a>Explanation of the Proposed CSM Learning Algorithm (解釋CSM學習演算法)</h4><ul><li>Step 6 conducts the cramming mechanism.</li><li>The Appendix shows the properness of the cramming mechanism.</li><li>All hidden nodes use the same activation function, but some of them have the heterogeneity due to their large associated weights.</li><li>The total amount of used hidden nodes will be large if new hidden nodes are added frequently.</li><li>Step 7.1 and Step 7.5.c are designed to soften the heterogeneity.</li><li>At Step 6, Step 7.1 and Step 7.5.c, the weight-tuning mechanism is applied to minimizing $𝐸_𝑛(𝐰)$ to adjust weights, while there is the regularization term in $𝐸_𝑛(𝐰)$.</li><li>The total amount of used hidden nodes will be large if new hidden nodes are added frequently.</li><li>Step 7.2 to Step 7.5 are designed to merge the unfamiliar case into the knowledge system via reducing the total amount of used hidden nodes.</li><li>The pruning issue file shows the logic of the merging mechanism.</li></ul><h4 id="The-irrelevant-hidden-node"><a href="#The-irrelevant-hidden-node" class="headerlink" title="The irrelevant hidden node"></a>The irrelevant hidden node</h4><p><img src="https://i.imgur.com/YZjNMXi.png" alt=""></p><h4 id="The-irrelevance-examination-mechanism"><a href="#The-irrelevance-examination-mechanism" class="headerlink" title="The irrelevance examination mechanism"></a>The irrelevance examination mechanism</h4><p><img src="https://i.imgur.com/LtBrY6n.png" alt=""></p><h4 id="No-under-fitting"><a href="#No-under-fitting" class="headerlink" title="No under-fitting"></a>No under-fitting</h4><p><img src="https://i.imgur.com/he4OjoM.png" alt=""></p><h4 id="The-Regularization-term-in-𝐸-𝑛-𝐰"><a href="#The-Regularization-term-in-𝐸-𝑛-𝐰" class="headerlink" title="The Regularization term in $𝐸_𝑛(𝐰)$"></a>The Regularization term in $𝐸_𝑛(𝐰)$</h4><ul><li>Regarding the overfitting issue, the regularization term in the loss function $𝐸_𝑛(𝐰)$ may prevent the model from doing too well on training data.</li></ul><h4 id="筆記"><a href="#筆記" class="headerlink" title="筆記"></a>筆記</h4><ul><li>演算法的停止準則與學習目標不一定會有關係</li></ul><h4 id="Cramming-硬背下去"><a href="#Cramming-硬背下去" class="headerlink" title="Cramming (硬背下去)"></a>Cramming (硬背下去)</h4><h4 id="Softening-軟化"><a href="#Softening-軟化" class="headerlink" title="Softening (軟化)"></a>Softening (軟化)</h4><h4 id="Merging-合併"><a href="#Merging-合併" class="headerlink" title="Merging (合併)"></a>Merging (合併)</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;人工智慧與應用筆記4&quot;&gt;&lt;a href=&quot;#人工智慧與應用筆記4&quot; class=&quot;headerlink&quot; title=&quot;人工智慧與應用筆記4&quot;&gt;&lt;/a&gt;人工智慧與應用筆記4&lt;/h1&gt;&lt;h2 id=&quot;第四周&quot;&gt;&lt;a href=&quot;#第四周&quot; class=&quot;header
      
    
    </summary>
    
      <category term="碩二課程" scheme="https://caocharles.github.io/categories/%E7%A2%A9%E4%BA%8C%E8%AA%B2%E7%A8%8B/"/>
    
      <category term="人工智慧與應用" scheme="https://caocharles.github.io/categories/%E7%A2%A9%E4%BA%8C%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E8%88%87%E6%87%89%E7%94%A8/"/>
    
    
      <category term="AI" scheme="https://caocharles.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>進階創新科技技術(第一周)</title>
    <link href="https://caocharles.github.io/%E9%80%B2%E9%9A%8E%E5%89%B5%E6%96%B0%E7%A7%91%E6%8A%80%E6%8A%80%E8%A1%93/"/>
    <id>https://caocharles.github.io/進階創新科技技術/</id>
    <published>2018-09-26T02:38:16.000Z</published>
    <updated>2018-12-08T17:51:33.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="進階創新科技技術共編-第一個禮拜"><a href="#進階創新科技技術共編-第一個禮拜" class="headerlink" title="進階創新科技技術共編(第一個禮拜)"></a>進階創新科技技術共編(第一個禮拜)</h1><ul><li>每周的錄音檔都放在這裡</li></ul><p><a href="https://drive.google.com/drive/folders/1jQl4cLGAwekYkz6gSCDlR7wwxA4dqbzP?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/drive/folders/1jQl4cLGAwekYkz6gSCDlR7wwxA4dqbzP?usp=sharing</a></p><h2 id="曾老師-要求"><a href="#曾老師-要求" class="headerlink" title="曾老師 (要求)"></a>曾老師 (要求)</h2><ul><li>爬蟲 PTT、FB</li><li>特定網站 (文章)</li><li>參考文獻 (有興趣的主題)</li><li>老師只有上六週的課程</li><li>文字分析為主 結構化資料也可以</li><li>Kagle、政府的公開資料、Researched Data</li><li>從NLP上知道商品的行情</li><li>專家系統是甚麼?</li></ul><h2 id="第一周"><a href="#第一周" class="headerlink" title="第一周"></a>第一周</h2><ul><li>要啥的資料集，NLTK</li><li>一組報一篇 NLP 的研究、論文、文獻探討</li><li>下禮拜就可以做這件事</li><li>我有錄音嘻嘻</li><li>臭噁男博士生(人家只是年齡比較大ㄅ…) 我說禮拜一那個</li><li>HI-EXPEND 是啥意思 (花比較多錢來讀書的人)</li><li>說得好，我華山派…</li><li>要幹嘛<ul><li>語意分析</li><li>情感分析</li><li>網路聲量分析，都不容易</li></ul></li></ul><h3 id="Sources-of-Big-Data"><a href="#Sources-of-Big-Data" class="headerlink" title="Sources of Big Data"></a>Sources of Big Data</h3><ul><li><p>In addition to accumulation of traditional data of transactions:</p><ul><li><p>Data warehousing (資料倉儲)</p></li><li><p>Cloud computing</p></li><li><p>Social network</p></li><li><p>Internet of Things (IOT)</p></li></ul></li><li><p>The business data volume is therefore increasing dramatically.</p></li><li><p>Some important attributes may be embedded in or mined from the big volume of data.</p></li><li><p>Therefore, data management issues for the big data are getting 蝦咪挖歌的.</p></li></ul><h3 id="Common-Framework-of-Big-Data"><a href="#Common-Framework-of-Big-Data" class="headerlink" title="Common Framework of Big Data"></a>Common Framework of Big Data</h3><ul><li>六個V(自己查)<ul><li>Volume</li><li>Velocity</li><li>Variety</li><li>Veracity</li><li>Value</li><li>深V</li></ul></li></ul><p><img src="https://i.imgur.com/dqC1lrY.png" alt=""></p><p><a href="https://i.imgur.com/U98MFAU.png" target="_blank" rel="noopener">我的帥照</a>(屁啦)</p><h3 id="非監督式學習"><a href="#非監督式學習" class="headerlink" title="非監督式學習"></a>非監督式學習</h3><ul><li>牛奶跟啤酒(關聯式資料)- </li></ul><h3 id="監督式學習"><a href="#監督式學習" class="headerlink" title="監督式學習"></a>監督式學習</h3><ul><li>一生只督你一人</li><li>有Y那一條的可以督</li><li>督的越準、越夯(XGBOOST、DNN)</li><li>決策樹(Desision tree) (最老的那種)</li></ul><h3 id="增強式學習"><a href="#增強式學習" class="headerlink" title="增強式學習"></a>增強式學習</h3><h3 id="深度學習-類神經網路"><a href="#深度學習-類神經網路" class="headerlink" title="深度學習(類神經網路)"></a>深度學習(類神經網路)</h3><h3 id="AI的三大應用"><a href="#AI的三大應用" class="headerlink" title="AI的三大應用"></a>AI的三大應用</h3><ul><li>語言辨識</li></ul><h3 id="Prescriptive-Analysis"><a href="#Prescriptive-Analysis" class="headerlink" title="Prescriptive Analysis"></a>Prescriptive Analysis</h3><ul><li>名詞:指導性分析</li><li>釋義:根據預測分析的結果，總結及建議不同結果的優化行動。</li><li>方法:透過預測分析結果，進行決策</li></ul><h3 id="NTLK-好像這才是重點"><a href="#NTLK-好像這才是重點" class="headerlink" title="NTLK (好像這才是重點)"></a>NTLK (好像這才是重點)</h3><ul><li>找一篇paper來報告</li><li><p><a href="https://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk" target="_blank" rel="noopener">textminingonline.nltk</a></p></li><li><p>載一下套件</p></li><li><p><a href="https://www.nltk.org/install.html" target="_blank" rel="noopener">網頁在這</a> 載好久</p></li></ul><p><img src="https://i.imgur.com/FiEFnmf.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 語料庫在這</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">brown.words()[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">brown.tagged_words()[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">len(brown.words())</span><br><span class="line">dir(brown)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 斷句</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 斷字</span></span><br><span class="line">tokens = word_tokenize(text)</span><br></pre></td></tr></table></figure><h3 id="Tokenizers"><a href="#Tokenizers" class="headerlink" title="Tokenizers"></a>Tokenizers</h3><ul><li>好多Token，好想玩桌遊。(傻眼….)</li><li>Token -&gt; 璀璨寶石<h3 id="第三堂課"><a href="#第三堂課" class="headerlink" title="第三堂課"></a>第三堂課</h3></li><li><p>Part-Of-Speech Tagging-1 </p><ul><li>最重要的文字分析之一</li><li>…投影片都有(也是)</li></ul></li><li><p>Part of speech tagging-2</p><ul><li>.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> treebank</span><br><span class="line">len(treebank.tagged_sents())</span><br><span class="line">train_data = treebank.tagged_sents()[:<span class="number">3000</span>]</span><br><span class="line">print(train_data)</span><br><span class="line">test_data = treebank.tagged_sents()[<span class="number">3000</span>:]</span><br><span class="line">print(test_data)</span><br></pre></td></tr></table></figure><p>竟能如此優秀 &lt;3(謝東森)</p><h3 id="助教精華-身材跟柏龍一樣-還蠻可愛的"><a href="#助教精華-身材跟柏龍一樣-還蠻可愛的" class="headerlink" title="助教精華(身材跟柏龍一樣)還蠻可愛的"></a>助教精華(身材跟柏龍一樣)還蠻可愛的</h3><ul><li>需要加入記憶的結構</li><li>裝備很重要嗎 ?<ul><li>…</li><li>…</li></ul></li><li>助教除了聲音檔都可以幫我們爬</li><li>柏龍想抓AV女優圖(乾我闢是)</li><li>表特版抓圖(我好像有抓過) 我抓過西斯&lt;3(想要++) (太讚啦~)<ul><li><a href="https://hackmd.io/6RIZ7tcyRymbihSUfCyIEw?view" target="_blank" rel="noopener">https://hackmd.io/6RIZ7tcyRymbihSUfCyIEw?view</a></li></ul></li><li>助教剛剛說甚麼模型是最差的 ?</li><li>容易過度配飾的模型很差(應該吧)(適拉幹)</li><li>87萬張圖(好猛)變成8萬張而已</li><li>感覺就跟AI相機一樣</li><li>混淆矩陣(到底會不會進複賽呢~好刺激)</li></ul><h3 id="一個對話機器人"><a href="#一個對話機器人" class="headerlink" title="一個對話機器人"></a>一個對話機器人</h3><ol><li>AI is A Brand’s New Face</li><li>Mind the tech Details</li><li>Know the difference between conversation AI and conventional chatbots</li><li>Integrate Key Data Sets</li></ol><h3 id="Crawler-amp-Data-Cleanup"><a href="#Crawler-amp-Data-Cleanup" class="headerlink" title="Crawler &amp; Data Cleanup"></a>Crawler &amp; Data Cleanup</h3><p>1.爬蟲，隨機抽資料<br>2.採用Python packages或 BASH shells皆可<br>3.實驗室有PTT的爬蟲資料<br>4.建議先學習基本linux指令<br>5.高頻字與低頻字，都比不上可以清理因素的字</p><h3 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h3><ul><li>幸好還有助教…</li><li>有人想跟博班一組ㄇ</li><li>我們四個一組不是嗎</li><li>NLP的文章(要找啥)-各自找在混一波嗎</li><li>產品的聲量(上游不太知道通路購買的消費者資料)</li><li>topical model</li><li>我的鮮奶茶機器人</li></ul><h3 id="下禮拜的作業"><a href="#下禮拜的作業" class="headerlink" title="下禮拜的作業"></a>下禮拜的作業</h3><ul><li>找出分析NLP的方法嗎 ?</li><li>我先讀東森說的那兩篇，一些小摘要、心得和 murmur 記錄在<a href="https://hackmd.io/XT1NAWXDT8KwmPgdO2gfIg" target="_blank" rel="noopener">這邊</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;進階創新科技技術共編-第一個禮拜&quot;&gt;&lt;a href=&quot;#進階創新科技技術共編-第一個禮拜&quot; class=&quot;headerlink&quot; title=&quot;進階創新科技技術共編(第一個禮拜)&quot;&gt;&lt;/a&gt;進階創新科技技術共編(第一個禮拜)&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;每周的錄音檔
      
    
    </summary>
    
      <category term="碩二課程" scheme="https://caocharles.github.io/categories/%E7%A2%A9%E4%BA%8C%E8%AA%B2%E7%A8%8B/"/>
    
      <category term="進階創新科技技術" scheme="https://caocharles.github.io/categories/%E7%A2%A9%E4%BA%8C%E8%AA%B2%E7%A8%8B/%E9%80%B2%E9%9A%8E%E5%89%B5%E6%96%B0%E7%A7%91%E6%8A%80%E6%8A%80%E8%A1%93/"/>
    
    
      <category term="Pytorch" scheme="https://caocharles.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>統研深度學習讀書會</title>
    <link href="https://caocharles.github.io/%E7%B5%B1%E7%A0%94%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E8%AE%80%E6%9B%B8%E6%9C%83/"/>
    <id>https://caocharles.github.io/統研深度學習讀書會/</id>
    <published>2018-09-20T15:43:20.000Z</published>
    <updated>2018-09-26T02:31:58.205Z</updated>
    
    <content type="html"><![CDATA[<h2 id="讀書會簡介"><a href="#讀書會簡介" class="headerlink" title="讀書會簡介"></a>讀書會簡介</h2><p>大家好，這是我和立諭發起的讀書會，這個讀書會的目的是希望透過討論及分享來學習Machine Learning + Deep Learning，希望大家以後不要失業(來自東森的提醒)</p><h2 id="先備知識"><a href="#先備知識" class="headerlink" title="先備知識"></a>先備知識</h2><ul><li>微積分(微分吧)</li><li>線性代數(向量輸入後的傳導及運算)</li></ul><p>我們這學年的進度大致簡單介紹如下(暫定)</p><h2 id="上學期"><a href="#上學期" class="headerlink" title="上學期"></a>上學期</h2><ul><li>一開始先帶大家入門程式，但還不確定要從 Keras 或是 TensorFlow 入手</li><li>之後大家就每周做個作業，然後也要看個課程</li><li>課程方面應該以李宏毅的 Machine Learing 為主，他的課有以下特點<ul><li>強調無痛入門，而且涉及的領域很廣，圖像及語音辨識、語意分析等等</li><li>上課方式有趣，基本上不會有枯燥的感覺</li><li>更重要的是李老師上課講的都是很新的內容，總之入門真的推薦</li></ul></li><li>實作方面的資源很多，再慢慢介紹</li><li>大家有時間的話可以報名一些比賽，例如黑克松或是 Kaggle </li><li>討論實作上的技巧、觀念及問題</li><li>Scikit-Learn (機器學習 ML 套件)，比較次要，<br>但還是希望大家能大致了解，下學期的多變量會輕鬆許多</li></ul><h2 id="寒假"><a href="#寒假" class="headerlink" title="寒假"></a>寒假</h2><p>寒假大家可能很忙，但希望大家還是能撥點時間看些相關的東西</p><ul><li>林軒田的機器學習基石跟技法<ul><li>蠻理論的不過大家是碩士會理論應該的所以還是硬著頭皮看吧</li><li>講的東西其實都蠻原始的，有些東西其實已經沒人在用了</li><li>但是起源的東西我覺得聽一聽多少會有幫助</li><li>可能會聽到崩潰，應該是我太笨(可能只有AI真正懂)</li><li>聽說其實作業超爆難(by 資科-劉昭麟教授)</li><li>但是大家也不用寫作業，就大家自己看一看就好(吧)</li></ul></li></ul><h2 id="下學期"><a href="#下學期" class="headerlink" title="下學期"></a>下學期</h2><p>碩一下課業會挺重的，但好像也還好，<br>多變量這門課如果上學期有在讀書會學到東西其實應該會蠻輕鬆的</p><ul><li>論文探討，如果是很新的希望我們能把它實作出來看看</li><li>期望大家有找到自已想做的項目，我們很樂意一起討論或是提供幫助，<br>最好的情況是你們可以指導我們嘻嘻</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;讀書會簡介&quot;&gt;&lt;a href=&quot;#讀書會簡介&quot; class=&quot;headerlink&quot; title=&quot;讀書會簡介&quot;&gt;&lt;/a&gt;讀書會簡介&lt;/h2&gt;&lt;p&gt;大家好，這是我和立諭發起的讀書會，這個讀書會的目的是希望透過討論及分享來學習Machine Learning + D
      
    
    </summary>
    
      <category term="讀書會" scheme="https://caocharles.github.io/categories/study-group/"/>
    
    
      <category term="Deep Learning" scheme="https://caocharles.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度學習</title>
    <link href="https://caocharles.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    <id>https://caocharles.github.io/深度學習/</id>
    <published>2018-08-28T16:12:21.000Z</published>
    <updated>2018-09-04T14:30:28.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="類神經網路-歷史"><a href="#類神經網路-歷史" class="headerlink" title="類神經網路 歷史"></a>類神經網路 歷史</h2><p>人工智慧最早出現於1950年代。人工智慧的目標是希望能讓電腦像人一班思考與學習。被視為人工智慧之父的圖靈(Alan Mathison Turing)，提出了有名的圖靈測試:人類與機器對話，如果人類無法根據這些對話過程判斷對方是人或機器，即通過測試，認為這台電腦具有人工智慧。</p><p>隨著AI的發展日益茁壯，1980年代(Jhon Searle)，提出了對人工智慧的分類方式:</p><ul><li><p>強人工智慧(Strong AI) : 機器與人具有完整的認知能力。</p></li><li><p>弱人工智慧(Weak AI) : 機器設計看起來具有智慧即可。</p></li></ul><p>而深度學習是人工智慧中，現今成長最快的領域，隨著電腦的普及以及其CPU、GPU運算能力增強，我們可以透過世界上各個資料庫收集大量資料，將資料整理成類神經網路的格式，藉由模擬人類神經網路的運作方式，加上數學的演算法進行更新參數，就可以讓電腦學習分辨日常生活的事物，常見的深度學習架構，如多層感知器MLP、深度神經網路DNN、卷積神經網路CNN、遞迴神經網路RNN。</p><p>而這些深度學習的架構應用在視覺辨識、語音辨識、自然語言處理、生物醫學等領域，皆有非常好的效果。</p><p><img src="https://i.imgur.com/5BFVrX2.png" alt=""></p><p>人工智慧現在已廣泛運用在我們的日常生活之中，像是手寫辨識、圖像辨識、語音辨識，幾乎都存在於人手一台的手機之中，還有更進一步的運用，如自動駕駛，透過硬體上的更新，我們可以裝設感測器感應車子周圍的環境狀況，加上圖像辨識，並整合成資訊讓電腦判斷是否停車或繼續行駛，透過演算法讓電腦知道該如何做決定，到最後將會實現在市區行駛之中，隨著科技越來越進步，深度學習所應用的領域就越來越廣。</p><h2 id="類神經網路-簡介"><a href="#類神經網路-簡介" class="headerlink" title="類神經網路 簡介"></a>類神經網路 簡介</h2><p>[維基百科]</p><p>類神經網路簡稱（英語：Artificial Neural Network，ANN），簡稱<strong>神經網路</strong>（Neural Network，NN）或<strong>類神經網路</strong>，在機器學習和認知科學領域，是一種模仿生物神經網路（動物的中樞神經系統，特別是大腦）的結構和功能的數學模型或計算模型，用於對函式進行估計或近似。神經網路由大量的人工神經元聯結進行計算。大多數情況下人工神經網路能在外界資訊的基礎上改變內部結構，是一種自適應系統，通俗的講就是具備學習功能。</p><p>現代神經網路是一種非線性統計性資料建模工具。典型的神經網路具有以下三個部分：</p><ul><li><p><strong>結構</strong>（<strong>Architecture</strong>）<br>結構指定了網路中的變數和它們的拓撲關係。<br>例如:神經網路中的變數可以是神經元連接的權重（weights）和神經元的激勵值（activities of the neurons）。</p></li><li><p><strong>激勵函式（Activity Rule）</strong><br>大部分神經網路模型具有一個短時間尺度的動力學規則，來定義神經元如何根據其他神經元的活動來改變自己的激勵值。一般激勵函式依賴於網路中的權重（即該網路的參數）。</p></li></ul><ul><li><strong>學習規則（Learning Rule）</strong><br>學習規則指定了網路中的權重如何隨著時間推進而調整。</li></ul><p><img src="https://i.imgur.com/Gmo3LfJ.png" alt=""></p><p>上圖為一個簡單的多層感知器，我們利用這個網路</p><h2 id="類神經網路-應用"><a href="#類神經網路-應用" class="headerlink" title="類神經網路 應用"></a>類神經網路 應用</h2><p><img src="https://i.imgur.com/ZK2OR9D.png" alt=""></p><p>應用</p><ul><li><p>手寫辨識</p><p>  利用Python下tensorflow模組中的MNIST資料，收集了數萬筆掃描過的圖檔及標籤，我們可以利用各種類神經網路模型建立分類器模型，依據模型的特性加以訓練，並可將訓練過的模型儲存，用來預測手寫板上的數字。</p></li><li><p>圖像辨識</p><p>  現今電腦辨識一般的圖片已經等同於人類的辨識率，甚至在反應的速度上會超過人類，但在動態影像的辨識度還有加強的空間，透過各種演算法的測試與改進，最終將會應用在自動駕駛等應用上。 </p></li><li><p>自然語言處理</p><p>  自然語言(Nature Language Processing, NLP)，是讓電腦學習理解人類所說的話語與文字，透過分析詞性，計算其詞向量等等，並運用樸素貝氏分類器將句子分類，來分析日常對話的含意，其產出有現在熱門的聊天機器人及FB所提供的(Facebook Messenger Platform)和line所推出的(Messaging API)。</p></li></ul><p><img src="https://i.imgur.com/Lc1XSE5.png" alt=""></p><p>AI與大數據的應用只會越來越深、越來越廣、越來越快，只要對科技發展有興趣的人，就可以踏入這塊領域了解AI的偉大。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;類神經網路-歷史&quot;&gt;&lt;a href=&quot;#類神經網路-歷史&quot; class=&quot;headerlink&quot; title=&quot;類神經網路 歷史&quot;&gt;&lt;/a&gt;類神經網路 歷史&lt;/h2&gt;&lt;p&gt;人工智慧最早出現於1950年代。人工智慧的目標是希望能讓電腦像人一班思考與學習。被視為人工智
      
    
    </summary>
    
      <category term="深度學習" scheme="https://caocharles.github.io/categories/deep-learning/"/>
    
    
      <category term="Deep Learning" scheme="https://caocharles.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>EM演算法</title>
    <link href="https://caocharles.github.io/EM%E6%BC%94%E7%AE%97%E6%B3%95/"/>
    <id>https://caocharles.github.io/EM演算法/</id>
    <published>2018-08-28T09:31:35.000Z</published>
    <updated>2018-09-10T04:33:56.835Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/1121509ac1dc" target="_blank" rel="noopener">參考網址</a><br><a href="http://web.mit.edu/6.435/www/Dempster77.pdf" target="_blank" rel="noopener">Maximum Likelihood from Incomplete Data via the EM Algorithm</a></p><hr><p><strong>最大期望演算法</strong>（<strong>Expectation-maximization algorithm</strong>，又稱<strong>期望最大化算法</strong>）<br>在資料分析中常用於分群，在給定群數及機率模型下，去尋找觀測值變數間的所隱藏的訊息，可用此演算法來估計機率模型中的參數估計或遺失值填補。</p><hr><h2 id="名詞介紹"><a href="#名詞介紹" class="headerlink" title="名詞介紹"></a>名詞介紹</h2><ul><li><p>樣本: $x$</p></li><li><p>概似函數:  $L(\theta|x)=p(x|\theta)$</p></li><li><p>目的: 找到能讓 $L(\theta|x)$ 最大化的參數</p></li></ul><p>我的想法就是找到符合我們樣本資料的”最大”概似函數的機率模型參數，<br>以往都是假設好機率模型中的參數，並計算其MLE，<br>現在透過樣本及樣本所隱藏的隱變數，來推導適合這些樣本的模型參數。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li>argument : $y$ (遺失值或是隱變數)</li><li>complete-data likelihood : $L(\theta|x,y)=p(x,y|\theta)$ 加入隱變數後完整的概似函數</li><li>What about $max_\theta L(x,y)$?</li><li>need to recursively update y and $\hat\theta$?</li></ul><h2 id="E步驟-更新y"><a href="#E步驟-更新y" class="headerlink" title="E步驟(更新y)"></a>E步驟(更新y)</h2><p>根據現在給定的模型參數及樣本觀測值，我們去計算其log完整概似函數的期望值如下:<br>$$Q(\theta|\theta^{(t)})\equiv E_y[log p(x,y|\theta)|x,\theta^{(t)}]$$</p><h2 id="M步驟-更新參數-theta"><a href="#M步驟-更新參數-theta" class="headerlink" title="M步驟(更新參數$\theta$)"></a>M步驟(更新參數$\theta$)</h2><p>最大化E步驟獲得的期望值<br>$$\theta^{(t+1)} = arg max_\theta  Q(\theta|\theta^{t})$$</p><p><strong>重複上述過程直到收斂</strong></p><h2 id="EM推導"><a href="#EM推導" class="headerlink" title="EM推導"></a>EM推導</h2><ul><li>$X:observed \space data$</li><li>$Y:latent\space variable$</li></ul><p>$Log\space likelihood \space function$</p><p>$$ l(\theta) $$</p><p>$$= lnP(X|\theta) $$</p><p>$$= ln\sum_yP(X,y|\theta) $$</p><p>$$= ln(\sum_yP(X,y|\theta))\frac{Q(y)}{Q(y)}$$</p><p>$$\ge\sum_yQ(y)ln\frac{P(X,y|\theta)}{Q(y)} $$</p><p>$$\because log \in concave\space by\space Jensen’s\space inequality $$</p><p>$$(解決ln\sum 不好計算的問題，Q:機率分配) $$</p><p>$$= E_Q(ln\frac{P(X,y|\theta)}{Q(y)})$$</p><p>在$Jensen’s \: inequality 中，當E(x)中，x=常數時，等號成立。$</p><p>$$\Rightarrow\frac{P(X,y|\theta)}{Q(y)}=c \in Constant，且\sum_yQ(y)=1$$</p><p>$$\Rightarrow\sum_yP(X,y|\theta)=c\sum_yQ(y)=c$$</p><p>$$\Rightarrow Q(y)=\frac{P(X,y|\theta)}{\sum_yP(X,y|\theta)}=P(y|x_i,\theta)$$，</p><p>$$Q:在樣本給定下之隱藏變數條件分布$$</p><p>$$\therefore E_Q(ln\frac{P(X,y|\theta)}{Q(y)})=E_y(ln\frac{P(X,y|\theta)}{P(y|x_i,\theta)}|X,\theta)$$</p><p>$$\theta^{(t+1)}=arg\max\limits_\theta l(\theta)\Longleftrightarrow arg\max\limits_\theta\sum_{y}P(y|x_i,\theta^{(t)})ln\frac{P(X,y|\theta)}{P(y|x_i,\theta^{(t)})}$$</p><p>$$\: \: \Longleftrightarrow arg\max\limits_\theta\sum_{y}P(y|x_i,\theta^{(t)})lnP(X,y|\theta)=E_y(lnP(X,y|\theta)|X,\theta^{(t)})=Q(\theta|\theta^{(t)})$$</p><p>$$\therefore E-step :Find \: Q(\theta|\theta^{(t)})$$</p><p>$\Longleftrightarrow$ Find the expectation of the complete-data loglikelihood with respect to the missing data y given the observed data x and the current parameter estimates $\theta^{(t)}$.</p><p>$$M-step=\theta^{(t+1)}=arg\max\limits_\theta Q(\theta|\theta^{(t)})$$</p><h2 id="EM收斂性"><a href="#EM收斂性" class="headerlink" title="EM收斂性"></a>EM收斂性</h2><hr><h2 id="範例1"><a href="#範例1" class="headerlink" title="範例1"></a>範例1</h2><p>197 animals are distributed multinomially into four categories with cell-probabilities$(\frac{1}{2}+ \frac{\theta}{4}, \frac{(1-\theta)}{4}, \frac{(1-\theta)}{4}, \frac{\theta}{4})$, where $\theta \in (0,1)$is unknown</p><p>Observed Data:<br>$$x=(x_1,x_2,x_3,x_4)=(125,18,20,34)$$</p><p>Likelihood:<br>$$L(\theta；x)=\frac{n!}{x_1!x_2!x_3!x_4!}(\frac{1}{2}+\frac{\theta}{4})^{x_1}(\frac{1}{4}-\frac{\theta}{4})^{x_2}(\frac{1}{4}-\frac{\theta}{4})^{x_3}(\frac{\theta}{4})^{x_4}$$</p><p>Find MLE by maximizing loglikelihood</p><p>Now use EM to find MLE</p><p>假設我們的隱藏變數在a裡面，令$y=x_{11}+x_{12}$</p><p>完整的變數擴展為$(x_{11},x_{12},x_{2},x_{3},x_{4})$有5個</p><p>初始其參數  $(\frac{1}{2}, \frac{\theta}{4}, \frac{1}{4}-\frac{\theta}{4}, \frac{\theta}{4})$</p><p>其概似函數如下</p><p>$$L(\theta；x)=\frac{n!}{x_{11}!x_{12}!x_2!x_3!x_4!}(\frac{1}{2})^{x_{11}}(\frac{\theta}{4})^{x_{12}}(\frac{1}{4}-\frac{\theta}{4})^{x_{2}+x_{3}}(\frac{\theta}{4})^{x_{4}}$$</p><ul><li>E 步驟</li></ul><p>給定機率模型參數$\theta^{(t)}$和$(x_1,x_2,x_3,x_4)$，$$x_{11}=\frac{2x_{1}}{2+\theta}\quad and \quad x_{12}=\frac{\theta x_{1}}{2+\theta}$$</p><p>推導Q函數(令$y=x_{12}$)<br>$$Q(\theta |\theta^{(t)})=E_y[log p(x,y|\theta)|x, \theta^{(t)}]$$</p><p>$$=E_y[(x_{12}+x_{4})log\theta +(x_{2}+x_{3})log(1-\theta)|x,\theta^{(t)}]$$</p><p>$$=(E_y[x_{12}|x, \theta^{(t)}]+x_{4})log\theta +(x_{2}+x_{3})log(1-\theta)$$</p><p>$$=(\frac{\theta^{(t)}x_{1}}{2+\theta^{(t)}}+x_{4})log\theta + (x_{2}+x_{3})log(1-\theta)$$</p><p>其中$$x_{12}|_{x,\theta^{(t)}}\sim Binomial(x_{1},\frac{\theta^{(t)}}{2+\theta^{(t)}})$$</p><p>$$x_{12}^{(t)}=E_y[x_{12}|x,\theta^{(t)}]=\frac{\theta^{(t)}x_{1}}{2+\theta^{(t)}}$$</p><ul><li>M 步驟</li></ul><p>對Q函數進行微分</p><p>給定$(x_{11},x_{12},x_{3},x_{4},x_{5})$</p><p>$$\theta^{(t+1)}=\frac{x_{12}^{(t)}+x_{4}}{x_{12}^{(t)}+x_{2}+x_{3}+x_{4}}$$</p><p><strong>重複以上步驟到參數迭代至穩定</strong></p><h2 id="R-實作"><a href="#R-實作" class="headerlink" title="R 實作"></a>R 實作</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mult = <span class="keyword">function</span>(theta, x, n)&#123;</span><br><span class="line">    tmp = theta</span><br><span class="line">    <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:n)&#123;</span><br><span class="line">        <span class="comment"># E-step</span></span><br><span class="line">        x12 = x[<span class="number">1</span>]*(theta/ (<span class="number">2</span> + theta))</span><br><span class="line">        <span class="comment"># M-step</span></span><br><span class="line">        theta = (x12 + x[<span class="number">4</span>])/(x12 + x[<span class="number">2</span>] + x[<span class="number">3</span>] + x[<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">        tmp = c(tmp, theta)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">x=c(<span class="number">125</span>, <span class="number">18</span>, <span class="number">20</span>, <span class="number">34</span>)</span><br><span class="line">mult(<span class="number">0.1</span>, x, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><h2 id="範例2"><a href="#範例2" class="headerlink" title="範例2"></a>範例2</h2><h2 id="範例3"><a href="#範例3" class="headerlink" title="範例3"></a>範例3</h2><p>假設現在有兩枚硬幣A、B</p><ul><li><p><step1>我們用一枚公正的硬幣來決定，投擲 A 或 B 硬幣</step1></p></li><li><p><step2>依據<step1>結果，投擲 A 或 B 硬幣 1次，記錄其結果</step1></step2></p></li><li><p><step3>反覆進行n次，最終可得到類似如下結果: 10111011….</step3></p><ul><li>1表示正面，0表示反面</li></ul></li></ul><p>如果我們今天只能觀察到最終結果<step3>，無法知道每一次投擲來自哪一枚硬幣，該如何估計出兩個硬幣出現正面機率?</step3></p><p><sol>:</sol></p><p>Observed Data : $X=(x_1,x_2,…,x_n), x_i:正面出現次數$<br>A、B 出現正面機率 : $\theta =(p,q)$</p><p>$1^{\circ}$  從MLE想法出發</p><p>概似函數 :<br>$$L(\theta|X)=P(X|\theta)=\prod_{i=1}^nP(x_i|\theta)$$</p><p>$$\hat p=\frac{使用A硬幣骰到正面次數}{使用A硬幣總投擲次數}$$</p><p>$$\hat q=\frac{使用B硬幣骰到正面次數}{使用B硬幣總投擲次數}$$</p><p>但因為我們並不知道 $x_i$來自哪個硬幣(機率模型)，所以無法進行計算。</p><p>$2^{\circ}$ 嘗試添加隱藏變數，使其變成complete data，運用EM演算法</p><ul><li><p>根據observed data，我們無從得知 $x_i$來自哪個硬幣(機率模型)，    </p></li><li><p>因此我們添加一個隱藏變數 $y_i$，其表示 $x_i$ 來自哪個硬幣，$Y=(y_1,y_2,…,y_n)$</p></li></ul><p>$y_i = 1$ or $2$，$x_i|y_{i}=1 \sim Ber(p_{1})$，$x_i|y_{i}=2 \sim Ber(p_{2})$</p><p>E-step :</p><p>$$\Rightarrow Q(\theta|\theta^{(t)})=E_y[ln(p(x,y|\theta))|x,\theta ^{(t)}]=E_y[\sum_{i=1}^{n}ln(p(y_i|\theta)p(x_i|y_i,\theta))|x,\theta^{(t)}]$$</p><p>$$=\sum_{i=1}^{n}E_y[ln(p(y_i|\theta)p(x_i|y_i,\theta))|x,\theta^{(t)}]=\sum_{i=1}^{n}\sum_{y_i=0}^{1}[ln(p(y_i|\theta)p(x_i|y_i,\theta))p(y_i|x_i,\theta^{(t)} )]$$</p><p>$$=\sum_{i=1}^{n}\sum_{j=0}^{1}[ln(p(y_i=j|\theta)p(x_i|y_i,\theta))p(y_i=j|x_i,\theta^{(t)} )]$$</p><p>其中$p(y_i=j|x_i,\theta^{(t)} )$ : 在第t次迭代下，當前數據來自哪個硬幣的機率</p><p>Q-step : </p><1><p>$$\frac{\partial Q}{\partial p}=\frac{\partial (\sum_{i=1}^{n}ln(\frac{1}{2}p^{x_i}(1-p)^{1-x_i})p(y_i=1|x_i,\theta^{(t)} )}{\partial p}$$</p><p>$$=\frac{\partial (\sum_{i=1}^{n}ln(\frac{1}{2})+x_iln(p)+(1-x_i)ln(1-p)p(y_i=1|x_i,\theta^{(t)} )}{\partial p}$$</p><p>$$=\sum_{i=1}^{n}(\frac{xi}{p}-\frac{(1-x_i)}{1-  p})p(y_i=1|x_i,\theta^{(t)} )=0$$</p><p>$$\Rightarrow p^{(t+1)}=\frac{\sum_{i=1}^{n}x_ip(y_i=1|x_i,\theta^{(t)})}{\sum_{i=1}^{n}p(y_i=1|x_i,\theta^{(t)})}$$</p><2><p>$$\frac{\partial Q}{\partial q}=0$$</p><p>同理可得,$$q^{(t+1)}=\frac{\sum_{i=1}^{n}x_ip(y_i=2|x_i,\theta^{(t)})}{\sum_{i=1}^{n}p(y_i=2|x_i,\theta^{(t)})}$$</p><p>$3^{\circ}$ 綜觀以上結果，可以發現實際上我們只需要計算出$p(y_i=j|x_i,\theta^{(t)} )$，就可以拿來進行EM迭代。</p><p>$\Rightarrow$ 給定初始值$(p^{(0)},q^{(0)})$，計算出$p(y_i=j|x_i,\theta^{(0)} )$，代入更新參數$p^{(t+1)},q^{(t+1)}$，重複迭代，直到收斂或者達到自行給定tolerance內.</p><p>$4^{\circ}$ 若<step2>改成依據<step1>結果，連續投擲 A 或 B 硬幣 10次，記錄其結果</step1></step2></p><ul><li><step3>反覆進行n次，最終可得到類似如下結果:<ul><li>1表示正面，0表示反面</li></ul></step3></li></ul><p><img src="https://i.imgur.com/TIuGBly.png" alt=""></p><p>最終Q-step 的參數公式 :<br>$$p^{(t+1)}=\frac{\sum_{i=1}^{n}x_ip(y_i=1|x_i,\theta^{(t)})}{\sum_{i=1}^{n}10p(y_i=1|x_i,\theta^{(t)}}$$</p><p>$$q^{(t+1)}=\frac{\sum_{i=1}^{n}x_ip(y_i=2|x_i,\theta^{(t)})}{\sum_{i=1}^{n}10p(y_i=2|x_i,\theta^{(t)})}$$</p><h2 id="Python-實作"><a href="#Python-實作" class="headerlink" title="Python 實作"></a>Python 實作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5組硬幣投擲結果(n=5,k=10)，1代表正面，0代表反面</span></span><br><span class="line">observations = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                         [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                         [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">da=pd.DataFrame(observations,</span><br><span class="line">                index=[<span class="string">"第一次"</span>,<span class="string">"第二次"</span>,<span class="string">"第三次"</span>,<span class="string">"第四次"</span>,<span class="string">"第五次"</span>])</span><br><span class="line">da.columns = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>];da</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">em_single</span><span class="params">(priors,observations)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    EM算法-單次疊代</span></span><br><span class="line"><span class="string">    ------------</span></span><br><span class="line"><span class="string">    priors:[theta_A,theta_B]</span></span><br><span class="line"><span class="string">    observation:[m X n matrix]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ---------------</span></span><br><span class="line"><span class="string">    new_priors:[new_theta_A,new_theta_B]</span></span><br><span class="line"><span class="string">    :param priors:</span></span><br><span class="line"><span class="string">    :param observations:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    counts = &#123;<span class="string">'A'</span>: &#123;<span class="string">'H'</span>: <span class="number">0</span>, <span class="string">'T'</span>: <span class="number">0</span>&#125;, <span class="string">'B'</span>: &#123;<span class="string">'H'</span>: <span class="number">0</span>, <span class="string">'T'</span>: <span class="number">0</span>&#125;&#125;</span><br><span class="line">    theta_A = priors[<span class="number">0</span>]</span><br><span class="line">    theta_B = priors[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">#E step</span></span><br><span class="line">    <span class="keyword">for</span> observation <span class="keyword">in</span> observations:</span><br><span class="line">        len_observation = len(observation)      <span class="comment">#計算投擲次數</span></span><br><span class="line">        num_heads = observation.sum()           <span class="comment">#正面次數</span></span><br><span class="line">        num_tails = len_observation-num_heads   <span class="comment">#反面次數</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#二項分配公式求解</span></span><br><span class="line">        contribution_A = scipy.stats.binom.pmf(num_heads,len_observation,theta_A)    <span class="comment">#Bin(x,n,p)</span></span><br><span class="line">        contribution_B = scipy.stats.binom.pmf(num_heads,len_observation,theta_B)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#計算在給定資料、當前參數下，資料來自哪個硬幣的機率</span></span><br><span class="line">        weight_A = contribution_A / (contribution_A + contribution_B)     <span class="comment"># p(y=1|x,theta)</span></span><br><span class="line">        weight_B = contribution_B / (contribution_A + contribution_B)     <span class="comment"># p(y=0|x,theta)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#更新在當前參數下A，B硬幣產生的正反面次數</span></span><br><span class="line">        counts[<span class="string">'A'</span>][<span class="string">'H'</span>] += weight_A * num_heads  <span class="comment"># num += 1  =&gt; num = num+1， sum p(y_i=1|x,theta)*x_i</span></span><br><span class="line">        counts[<span class="string">'A'</span>][<span class="string">'T'</span>] += weight_A * num_tails</span><br><span class="line">        counts[<span class="string">'B'</span>][<span class="string">'H'</span>] += weight_B * num_heads</span><br><span class="line">        counts[<span class="string">'B'</span>][<span class="string">'T'</span>] += weight_B * num_tails</span><br><span class="line"></span><br><span class="line">    <span class="comment"># M step</span></span><br><span class="line">    new_theta_A = counts[<span class="string">'A'</span>][<span class="string">'H'</span>] / (counts[<span class="string">'A'</span>][<span class="string">'H'</span>] + counts[<span class="string">'A'</span>][<span class="string">'T'</span>])  <span class="comment">#sum p(y_i=1|x,theta)*x_i / sum 10*p(y_i=1|x,theta)</span></span><br><span class="line">    new_theta_B = counts[<span class="string">'B'</span>][<span class="string">'H'</span>] / (counts[<span class="string">'B'</span>][<span class="string">'H'</span>] + counts[<span class="string">'B'</span>][<span class="string">'T'</span>])  <span class="comment">#sum p(y_i=0|x,theta)*x_i / sum 10*p(y_i=1|x,theta)</span></span><br><span class="line">    <span class="keyword">return</span> [new_theta_A,new_theta_B]</span><br><span class="line">    </span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">em</span><span class="params">(observations,prior,tol = <span class="number">1e-6</span>,iterations=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    EM算法</span></span><br><span class="line"><span class="string">    ：param observations :觀測數據</span></span><br><span class="line"><span class="string">    ：param prior：模型初始值</span></span><br><span class="line"><span class="string">    ：param tol：迭代结束阈值</span></span><br><span class="line"><span class="string">    ：param iterations：最大迭代次數</span></span><br><span class="line"><span class="string">    ：return：局部最佳的模型參數</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    iteration = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> iteration &lt; iterations:</span><br><span class="line">        new_prior = em_single(prior,observations)</span><br><span class="line">        delta_change = np.abs(prior[<span class="number">0</span>]-new_prior[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> delta_change &lt; tol:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prior = new_prior</span><br><span class="line">            iteration +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> new_prior,iteration</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"(p,q,iteration)="</span>,em(observations,[<span class="number">0.7</span>,<span class="number">0.5</span>]))</span><br></pre></td></tr></table></figure><p>Ans : (p,q,iteration)= ([0.79678865844706648, 0.51958340803243785], 12)</p></2></1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/1121509ac1dc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;參考網址&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://web.mit.edu/6.435/www/Dempste
      
    
    </summary>
    
      <category term="機器學習" scheme="https://caocharles.github.io/categories/machine-learning/"/>
    
    
      <category term="EM" scheme="https://caocharles.github.io/tags/EM/"/>
    
      <category term="Algorithm" scheme="https://caocharles.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>我的測試文章</title>
    <link href="https://caocharles.github.io/%E6%88%91%E7%9A%84%E6%B8%AC%E8%A9%A6%E6%96%87%E7%AB%A0/"/>
    <id>https://caocharles.github.io/我的測試文章/</id>
    <published>2018-08-27T08:23:30.000Z</published>
    <updated>2018-09-04T13:47:53.821Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://caocharles.github.io/">連結測試</a></p><p>打打看文字。</p><p>12345</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x+2y =5</span><br></pre></td></tr></table></figure><p>$$\sigma$$ </p><p>$$\frac{1}{2}$$ </p><p><a href="https://www.bigdatauniversity.com" target="_blank" rel="noopener"><img src="https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png" width="300," align="center"></a></p><h1 align="center"><font size="5"> TENSORFLOW’S HELLO WORLD</font></h1><div class="alert alert-block alert-info" style="margin-top: 20px"><br><font size="3"><strong>In this notebook we will overview the basics of TensorFlow, learn it’s structures and see what is the motivation to use it</strong></font><br><br><br> - <p><a href="#ref2">How does TensorFlow work?</a></p><br> - <p><a href="#ref3">Building a Graph</a></p><br> - <p><a href="#ref4">Defining multidimensional arrays using TensorFlow</a></p><br> - <p><a href="#ref5">Why Tensors?</a></p><br> - <p><a href="#ref6">Variables</a></p><br> - <p><a href="#ref7">Placeholders</a></p><br> - <p><a href="#ref8">Operations</a></p><br><p></p><br></div><br><br><br><br>—————-<br><br><a id="ref2"></a><br><br># How does TensorFlow work?<br><br>TensorFlow’s capability to execute the code on different devices such as CPUs and GPUs is a consequence of it’s specific structure:<br><br>TensorFlow defines computations as Graphs, and these are made with operations (also know as “ops”). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.<br><br>To execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU.<br><br>For example, the image below represents a graph in TensorFlow. _W_, _x_ and b are tensors over the edges of this graph. _MatMul_ is an operation over the tensors _W_ and _x_, after that _Add_ is called and add the result of the previous operator with _b_. The resultant tensors of each operation cross the next one until the end where it’s possible to get the wanted result.<br><br><img src="https://ibm.box.com/shared/static/a94cgezzwbkrq02jzfjjljrcaozu5s2q.png"><br><br><br>### Importing TensorFlow<br><p>To use TensorFlow, we need to import the library. We imported it and optionally gave it the name “tf”, so the modules can be accessed by <strong>tf.module-name</strong>:<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><br><br>—————–<br><br><a id="ref3"></a><br># Building a Graph<br><br>As we said before, TensorFlow works as a graph computational model. Let’s create our first graph.<br><br>To create our first graph we will utilize <strong>source operations</strong>, which do not need any information input. These source operations or <strong>source ops</strong> will pass their information to other operations which will execute computations.<br><br>To create two source operations which will output numbers we will define two constants:<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">2</span>])</span><br><span class="line">b = tf.constant([<span class="number">3</span>])</span><br></pre></td></tr></table></figure><br><br>After that, let’s make an operation over these variables. The function <strong>tf.add()</strong> adds two elements (you could also use <code>c = a + b</code>).<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = tf.add(a,b)</span><br><span class="line"><span class="comment">#c = a + b is also a way to define the sum of the terms</span></span><br></pre></td></tr></table></figure><br><br>Then TensorFlow needs to initialize a session to run our code. Sessions are, in a way, a context for creating a graph inside TensorFlow. Let’s define our session:<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session = tf.Session()</span><br></pre></td></tr></table></figure><br><br>Let’s run the session to get the result from the previous defined ‘c’ operation:<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = session.run(c)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><br><br>Close the session to release resources:<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session.close()</span><br></pre></td></tr></table></figure><br><br>To avoid having to close sessions every time, we can define them in a <strong>with</strong> block, so after running the <strong>with</strong> block the session will close automatically:<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(c)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><br><br>Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your edge (In this case our constants), include nodes (operations, like _tf.add_), and start a session to build a graph.<br><br><br><br>### What is the meaning of Tensor?<br><br><div class="alert alert-success alertsuccess" style="margin-top: 20px"><br><font size="3"><strong>In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.</strong></font><br><br><br><br><br>The word <strong>tensor</strong> from new latin means “that which stretches”. It is a mathematical object that is named <strong>tensor</strong>  because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays.<br><br><p></p><br><br></div></p><p>That’s great, but… what are these multidimensional arrays? </p><p>Going back a little bit to physics to understand the concept of dimensions:<br><br><img src="https://ibm.box.com/shared/static/ymn0hl3hf8s3xb4k15v22y5vmuodnue1.svg"></p><div style="text-align:center"><a href="https://en.wikipedia.org/wiki/Dimension" target="_blank" rel="noopener">[Image Source]</a> </div><p>The zero dimension can be seen as a point, a single object or a single item.</p><p>The first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements.</p><p>The second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. </p><p>The third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line.</p><p>The Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on…</p><p>As mathematical objects: <br><br><br><img src="https://ibm.box.com/shared/static/kmxz570uai8eeg6i6ynqdz6kmlx1m422.png"></p><div style="text-align:center"><a href="https://book.mql4.com/variables/arrays" target="_blank" rel="noopener">[Image Source]</a></div><p>Summarizing:<br><br></p><table style="width:100%"><br>  <tr><br>    <td><b>Dimension</b></td><br>    <td><b>Physical Representation</b></td><br>    <td><b>Mathematical Object</b></td><br>    <td><b>In Code</b></td><br>  </tr><br><br>  <tr><br>    <td>Zero </td><br>    <td>Point</td><br>    <td>Scalar (Single Number)</td><br>    <td>[ 1 ]</td><br>  </tr><br><br>  <tr><br>    <td>One</td><br>    <td>Line</td><br>    <td>Vector (Series of Numbers) </td><br>    <td>[ 1,2,3,4,… ]</td><br>  </tr><br><br>   <tr><br>    <td>Two</td><br>    <td>Surface</td><br>    <td>Matrix (Table of Numbers)</td><br>    <td>[ [1,2,3,4,…], [1,2,3,4,…], [1,2,3,4,…],… ]</td><br>  </tr><br><br>   <tr><br>    <td>Three</td><br>    <td>Volume</td><br>    <td>Tensor (Cube of Numbers)</td><br>    <td>[ [[1,2,…], [1,2,…], [1,2,…],…], [[1,2,…], [1,2,…], [1,2,…],…], [[1,2,…], [1,2,…], [1,2,…] ,…]… ]</td><br>  </tr><br><br></table><hr><p><a id="ref4"></a></p><h1 id="Defining-multidimensional-arrays-using-TensorFlow"><a href="#Defining-multidimensional-arrays-using-TensorFlow" class="headerlink" title="Defining multidimensional arrays using TensorFlow"></a>Defining multidimensional arrays using TensorFlow</h1><p>Now we will try to define such arrays using TensorFlow:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Scalar = tf.constant([<span class="number">2</span>])</span><br><span class="line">Vector = tf.constant([<span class="number">5</span>,<span class="number">6</span>,<span class="number">2</span>])</span><br><span class="line">Matrix = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">Tensor = tf.constant( [ [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]] , [[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]] , [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>]] ] )</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(Scalar)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Scalar (1 entry):\n %s \n"</span> % result)</span><br><span class="line">    result = session.run(Vector)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Vector (3 entries) :\n %s \n"</span> % result)</span><br><span class="line">    result = session.run(Matrix)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Matrix (3x3 entries):\n %s \n"</span> % result)</span><br><span class="line">    result = session.run(Tensor)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Tensor (3x3x3 entries) :\n %s \n"</span> % result)</span><br></pre></td></tr></table></figure><p>Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Matrix_one = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">Matrix_two = tf.constant([[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">first_operation = tf.add(Matrix_one, Matrix_two)</span><br><span class="line">second_operation = Matrix_one + Matrix_two</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(first_operation)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Defined using tensorflow function :"</span>)</span><br><span class="line">    print(result)</span><br><span class="line">    result = session.run(second_operation)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Defined using normal expressions :"</span>)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>With the regular symbol definition and also the TensorFlow function we were able to get an element-wise multiplication, also known as Hadamard product. <br></p><p>But what if we want the regular matrix product?</p><p>We then need to use another TensorFlow function called <strong>tf.matmul()</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Matrix_one = tf.constant([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">Matrix_two = tf.constant([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">first_operation = tf.matmul(Matrix_one, Matrix_two)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(first_operation)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Defined using tensorflow function :"</span>)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>We could also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel!</p><hr><p><a id="ref5"></a></p><h1 id="Why-Tensors"><a href="#Why-Tensors" class="headerlink" title="Why Tensors?"></a>Why Tensors?</h1><p>The Tensor structure helps us by giving the freedom to shape the dataset the way we want.</p><p>And it is particularly helpful when dealing with images, due to the nature of how information in images are encoded,</p><p>Thinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional strucutre (a matrix)… until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particulary helpful.</p><p>Images are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this:</p><p><img src="https://ibm.box.com/shared/static/xlpv9h5xws248c09k1rlx7cer69y4grh.png"></p><div style="text-align:center"><a href="https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131.aspx" target="_blank" rel="noopener">[Image Source]</a></div><p>So the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. </p><hr><p><a id="ref6"></a></p><h1 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h1><p>Now that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables.</p><p>To define variables we use the command <strong>tf.variable()</strong>.<br>To be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. This is done by running <strong>tf.global_variables_initializer()</strong>.</p><p>To update the value of a variable, we simply run an assign operation that assigns a value to the variable:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state = tf.Variable(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Let’s first create a simple counter, a variable that increases one unit at a time:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br></pre></td></tr></table></figure><p>Variables must be initialized by running an initialization operation after having launched the graph.  We first have to add the initialization operation to the graph:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><p>We then start a session to run the graph, first initialize the variables, then print the initial value of the <strong>state</strong> variable, and then run the operation of updating the <strong>state</strong> variable and printing the result after each update:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">  session.run(init_op)</span><br><span class="line">  print(session.run(state))</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    session.run(update)</span><br><span class="line">    print(session.run(state))</span><br></pre></td></tr></table></figure><hr><p><a id="ref7"></a></p><h1 id="Placeholders"><a href="#Placeholders" class="headerlink" title="Placeholders"></a>Placeholders</h1><p>Now we know how to manipulate variables inside TensorFlow, but what about feeding data outside of a TensorFlow model? </p><p>If you want to feed data to a TensorFlow model from outside a model, you will need to use placeholders.</p><p>So what are these placeholders and what do they do? </p><p>Placeholders can be seen as “holes” in your model, “holes” which you will pass the data to, you can create them using <br> <b>tf.placeholder(_datatype_)</b>, where <b>_datatype_</b> specifies the type of data (integers, floating points, strings, booleans) along with its precision (8, 16, 32, 64) bits.</p><p>The definition of each data type with the respective python sintax is defined as:</p><table><thead><tr><th>Data type</th><th>Python type</th><th>Description</th></tr></thead><tbody><tr><td>DT_FLOAT</td><td>tf.float32</td><td>32 bits floating point.</td></tr><tr><td>DT_DOUBLE</td><td>tf.float64</td><td>64 bits floating point.</td></tr><tr><td>DT_INT8</td><td>tf.int8</td><td>8 bits signed integer.</td></tr><tr><td>DT_INT16</td><td>tf.int16</td><td>16 bits signed integer.</td></tr><tr><td>DT_INT32</td><td>tf.int32</td><td>32 bits signed integer.</td></tr><tr><td>DT_INT64</td><td>tf.int64</td><td>64 bits signed integer.</td></tr><tr><td>DT_UINT8</td><td>tf.uint8</td><td>8 bits unsigned integer.</td></tr><tr><td>DT_STRING</td><td>tf.string</td><td>Variable length byte arrays. Each element of a Tensor is a byte array.</td></tr><tr><td>DT_BOOL</td><td>tf.bool</td><td>Boolean.</td></tr><tr><td>DT_COMPLEX64</td><td>tf.complex64</td><td>Complex number made of two 32 bits floating points: real and imaginary parts.</td></tr><tr><td>DT_COMPLEX128</td><td>tf.complex128</td><td>Complex number made of two 64 bits floating points: real and imaginary parts.</td></tr><tr><td>DT_QINT8</td><td>tf.qint8</td><td>8 bits signed integer used in quantized Ops.</td></tr><tr><td>DT_QINT32</td><td>tf.qint32</td><td>32 bits signed integer used in quantized Ops.</td></tr><tr><td>DT_QUINT8</td><td>tf.quint8</td><td>8 bits unsigned integer used in quantized Ops.</td></tr></tbody></table><div style="text-align:center"><a href="https://www.tensorflow.org/versions/r0.9/resources/dims_types.html" target="_blank" rel="noopener">[Table Source]</a></div><p>So we create a placeholder:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure><p>And define a simple multiplication operation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a*<span class="number">2</span></span><br></pre></td></tr></table></figure><p>Now we need to define and run the session, but since we created a “hole” in the model to pass the data, when we initialize the session we are obligated to pass an argument with the data, otherwise we would get an error.</p><p>To pass the data to the model we call the session with an extra argument <b> feed_dict</b> in which we should pass a dictionary with each placeholder name folowed by its respective data, just like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(b,feed_dict=&#123;a:<span class="number">3.5</span>&#125;)</span><br><span class="line">    <span class="keyword">print</span> (result)</span><br></pre></td></tr></table></figure><p>Since data in TensorFlow is passed in form of multidimensional arrays we can pass any kind of tensor through the placeholders to get the answer to the simple multiplication operation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dictionary=&#123;a: [ [ [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>] ] , [ [<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>],[<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>],[<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>],[<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>] ] ] &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(b,feed_dict=dictionary)</span><br><span class="line">    <span class="keyword">print</span> (result)</span><br></pre></td></tr></table></figure><hr><p><a id="ref8"></a></p><h1 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h1><p>Operations are nodes that represent the mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensor or maybe an activation function.</p><p>_tf.matmul_, _tf.add_, _tf.nn.sigmoid_ are some of the operations in TensorFlow. These are like functions in python but operate directly over tensors and each one does a specific thing. </p><div class="alert alert-success alertsuccess" style="margin-top: 20px">Other operations can be easily found in: <a href="https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html</a></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">5</span>])</span><br><span class="line">b = tf.constant([<span class="number">2</span>])</span><br><span class="line">c = tf.add(a,b)</span><br><span class="line">d = tf.subtract(a,b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    result = session.run(c)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'c =: %s'</span> % result)</span><br><span class="line">    result = session.run(d)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'d =: %s'</span> % result)</span><br></pre></td></tr></table></figure><p>_tf.nn.sigmoid_ is an activiation function, it’s a little more complicated, but this function helps learning models to evaluate what kind of information is good or not.</p><hr><h2 id="Want-to-learn-more"><a href="#Want-to-learn-more" class="headerlink" title="Want to learn more?"></a>Want to learn more?</h2><p>Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM’s Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a <a href="https://cocl.us/ML0120EN_PAI" target="_blank" rel="noopener">free version of PowerAI</a>.</p><p>Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at <a href="https://cocl.us/ML0120EN_DSX" target="_blank" rel="noopener">Data Science Experience</a>This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies.</p><h3 id="Thanks-for-completing-this-lesson"><a href="#Thanks-for-completing-this-lesson" class="headerlink" title="Thanks for completing this lesson!"></a>Thanks for completing this lesson!</h3><p>Notebook created by: <a href="https://ca.linkedin.com/in/rafaelblsilva" target="_blank" rel="noopener"> Rafael Belo Da Silva </a> </p><h3 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h3><p><a href="https://www.tensorflow.org/versions/r0.9/get_started/index.html" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r0.9/get_started/index.html</a><br><br><a href="http://jrmeyer.github.io/tutorial/2016/02/01/TensorFlow-Tutorial.html" target="_blank" rel="noopener">http://jrmeyer.github.io/tutorial/2016/02/01/TensorFlow-Tutorial.html</a><br><br><a href="https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html</a><br><br><a href="https://www.tensorflow.org/versions/r0.9/resources/dims_types.html" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r0.9/resources/dims_types.html</a><br><br><a href="https://en.wikipedia.org/wiki/Dimension" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Dimension</a><br><br><a href="https://book.mql4.com/variables/arrays" target="_blank" rel="noopener">https://book.mql4.com/variables/arrays</a><br><br><a href="https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131(v=vs.85).aspx" target="_blank" rel="noopener">https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131(v=vs.85).aspx</a><br></p><p><hr><br>Copyright &copy; 2016 <a href="https://bigdatauniversity.com/?utm_source=bducopyrightlink&amp;utm_medium=dswb&amp;utm_campaign=bdu" target="_blank" rel="noopener">Big Data University</a>. This notebook and its source code are released under the terms of the <a href="https://bigdatauniversity.com/mit-license/" target="_blank" rel="noopener">MIT License</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://caocharles.github.io/&quot;&gt;連結測試&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;打打看文字。&lt;/p&gt;
&lt;p&gt;12345&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut
      
    
    </summary>
    
      <category term="這是分類" scheme="https://caocharles.github.io/categories/%E9%80%99%E6%98%AF%E5%88%86%E9%A1%9E/"/>
    
      <category term="這是子分類" scheme="https://caocharles.github.io/categories/%E9%80%99%E6%98%AF%E5%88%86%E9%A1%9E/%E9%80%99%E6%98%AF%E5%AD%90%E5%88%86%E9%A1%9E/"/>
    
    
      <category term="這是標籤" scheme="https://caocharles.github.io/tags/%E9%80%99%E6%98%AF%E6%A8%99%E7%B1%A4/"/>
    
      <category term="這是標籤2" scheme="https://caocharles.github.io/tags/%E9%80%99%E6%98%AF%E6%A8%99%E7%B1%A42/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://caocharles.github.io/hello-world/"/>
    <id>https://caocharles.github.io/hello-world/</id>
    <published>2018-08-27T07:58:15.056Z</published>
    <updated>2018-08-27T07:58:15.057Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
