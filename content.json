{"pages":[{"title":"categories","date":"2018-08-28T09:23:35.000Z","path":"categories/index.html","text":""},{"title":"tags","date":"2018-08-28T09:23:14.000Z","path":"tags/index.html","text":""},{"title":"關於","date":"2018-09-02T08:56:09.963Z","path":"about/index.html","text":"查爾斯的天地這是一個我整理知識及分享心得的部落格，以前就想要自己建立一個部落格，剛好在碩一升碩二的這個暑假，我來到信邦電子實習，開始了我的程式訓練，在這個過程中我發現我的程式能力還有待加強，希望藉由這個部落格及碩二舉辦的讀書會，能精進我表達能力和程式的撰寫能力。"}],"posts":[{"title":"統研深度學習讀書會","date":"2018-08-29T01:43:20.000Z","path":"統研深度學習讀書會/","text":"讀書會簡介開學討論","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://caocharles.github.io/tags/Deep-Learning/"}],"categories":[{"name":"讀書會","slug":"study-group","permalink":"https://caocharles.github.io/categories/study-group/"}]},{"title":"深度學習","date":"2018-08-28T16:12:21.000Z","path":"深度學習/","text":"類神經網路 歷史人工智慧最早出現於1950年代。人工智慧的目標是希望能讓電腦像人一班思考與學習。被視為人工智慧之父的圖靈(Alan Mathison Turing)，提出了有名的圖靈測試:人類與機器對話，如果人類無法根據這些對話過程判斷對方是人或機器，即通過測試，認為這台電腦具有人工智慧。 隨著AI的發展日益茁壯，1980年代(Jhon Searle)，提出了對人工智慧的分類方式: 強人工智慧(Strong AI) : 機器與人具有完整的認知能力。 弱人工智慧(Weak AI) : 機器設計看起來具有智慧即可。 而深度學習是人工智慧中，現今成長最快的領域，隨著電腦的普及以及其CPU、GPU運算能力增強，我們可以透過世界上各個資料庫收集大量資料，將資料整理成類神經網路的格式，藉由模擬人類神經網路的運作方式，加上數學的演算法進行更新參數，就可以讓電腦學習分辨日常生活的事物，常見的深度學習架構，如多層感知器MLP、深度神經網路DNN、卷積神經網路CNN、遞迴神經網路RNN。 而這些深度學習的架構應用在視覺辨識、語音辨識、自然語言處理、生物醫學等領域，皆有非常好的效果。 人工智慧現在已廣泛運用在我們的日常生活之中，像是手寫辨識、圖像辨識、語音辨識，幾乎都存在於人手一台的手機之中，還有更進一步的運用，如自動駕駛，透過硬體上的更新，我們可以裝設感測器感應車子周圍的環境狀況，加上圖像辨識，並整合成資訊讓電腦判斷是否停車或繼續行駛，透過演算法讓電腦知道該如何做決定，到最後將會實現在市區行駛之中，隨著科技越來越進步，深度學習所應用的領域就越來越廣。 類神經網路 簡介[維基百科] 類神經網路簡稱（英語：Artificial Neural Network，ANN），簡稱神經網路（Neural Network，NN）或類神經網路，在機器學習和認知科學領域，是一種模仿生物神經網路（動物的中樞神經系統，特別是大腦）的結構和功能的數學模型或計算模型，用於對函式進行估計或近似。神經網路由大量的人工神經元聯結進行計算。大多數情況下人工神經網路能在外界資訊的基礎上改變內部結構，是一種自適應系統，通俗的講就是具備學習功能。 現代神經網路是一種非線性統計性資料建模工具。典型的神經網路具有以下三個部分： 結構（Architecture）結構指定了網路中的變數和它們的拓撲關係。例如:神經網路中的變數可以是神經元連接的權重（weights）和神經元的激勵值（activities of the neurons）。 激勵函式（Activity Rule）大部分神經網路模型具有一個短時間尺度的動力學規則，來定義神經元如何根據其他神經元的活動來改變自己的激勵值。一般激勵函式依賴於網路中的權重（即該網路的參數）。 學習規則（Learning Rule）學習規則指定了網路中的權重如何隨著時間推進而調整。 上圖為一個簡單的多層感知器，我們利用這個網路 類神經網路 應用 應用 手寫辨識 利用Python下tensorflow模組中的MNIST資料，收集了數萬筆掃描過的圖檔及標籤，我們可以利用各種類神經網路模型建立分類器模型，依據模型的特性加以訓練，並可將訓練過的模型儲存，用來預測手寫板上的數字。 圖像辨識 現今電腦辨識一般的圖片已經等同於人類的辨識率，甚至在反應的速度上會超過人類，但在動態影像的辨識度還有加強的空間，透過各種演算法的測試與改進，最終將會應用在自動駕駛等應用上。 自然語言處理 自然語言(Nature Language Processing, NLP)，是讓電腦學習理解人類所說的話語與文字，透過分析詞性，計算其詞向量等等，並運用樸素貝氏分類器將句子分類，來分析日常對話的含意，其產出有現在熱門的聊天機器人及FB所提供的(Facebook Messenger Platform)和line所推出的(Messaging API)。 AI與大數據的應用只會越來越深、越來越廣、越來越快，只要對科技發展有興趣的人，就可以踏入這塊領域了解AI的偉大。","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://caocharles.github.io/tags/Deep-Learning/"}],"categories":[{"name":"深度學習","slug":"deep-learning","permalink":"https://caocharles.github.io/categories/deep-learning/"}]},{"title":"EM演算法","date":"2018-08-28T09:31:35.000Z","path":"EM演算法/","text":"參考網址Maximum Likelihood from Incomplete Data via the EM Algorithm 最大期望演算法（Expectation-maximization algorithm，又稱期望最大化算法）在資料分析中常用於分群，在給定群數及機率模型下，去尋找觀測值變數間的所隱藏的訊息，可用此演算法來估計機率模型中的參數估計或遺失值填補。 名詞介紹 樣本: $x$ 概似函數: $L(\\theta|x)=p(x|\\theta)$ 目的: 找到能讓 $L(\\theta|x)$ 最大化的參數 我的想法就是找到符合我們樣本資料的”最大”概似函數的機率模型參數，以往都是假設好機率模型中的參數，並計算其MLE，現在透過樣本及樣本所隱藏的隱變數，來推導適合這些樣本的模型參數。 思考 argument : $y$ (遺失值或是隱變數) complete-data likelihood : $L(\\theta|x,y)=p(x,y|\\theta)$ 加入隱變數後完整的概似函數 What about $max_\\theta L(x,y)$? need to recursively update y and $\\hat\\theta$? E步驟(更新y)根據現在給定的模型參數及樣本觀測值，我們去計算其log完整概似函數的期望值如下:$$Q(\\theta|\\theta^{(t)})\\equiv E_y[log p(x,y|\\theta)|x,\\theta^{(t)}]$$ M步驟(更新參數$\\theta$)最大化E步驟獲得的期望值$$\\theta^{(t+1)} = arg max_\\theta Q(\\theta|\\theta^{t})$$ 重複上述過程直到收斂 EM推導 $X:observed \\space data$ $Y:latent\\space variable$ $Log\\space likelihood \\space function$ $$ l(\\theta) $$ $$= lnP(X|\\theta) $$ $$= ln\\sum_yP(X,y|\\theta) $$ $$= ln(\\sum_yP(X,y|\\theta))\\frac{Q(y)}{Q(y)}$$ $$\\ge\\sum_yQ(y)ln\\frac{P(X,y|\\theta)}{Q(y)} $$ $$\\because log \\in concave\\space by\\space Jensen’s\\space inequality $$ $$(解決ln\\sum 不好計算的問題，Q:機率分配) $$ $$= E_Q(ln\\frac{P(X,y|\\theta)}{Q(y)})$$ 在$Jensen’s \\: inequality 中，當E(x)中，x=常數時，等號成立。$ $$\\Rightarrow\\frac{P(X,y|\\theta)}{Q(y)}=c \\in Constant，且\\sum_yQ(y)=1$$ $$\\Rightarrow\\sum_yP(X,y|\\theta)=c\\sum_yQ(y)=c$$ $$\\Rightarrow Q(y)=\\frac{P(X,y|\\theta)}{\\sum_yP(X,y|\\theta)}=P(y|x_i,\\theta)$$， $$Q:在樣本給定下之隱藏變數條件分布$$ $$\\therefore E_Q(ln\\frac{P(X,y|\\theta)}{Q(y)})=E_y(ln\\frac{P(X,y|\\theta)}{P(y|x_i,\\theta)}|X,\\theta)$$ $$\\theta^{(t+1)}=arg\\max\\limits_\\theta l(\\theta)\\Longleftrightarrow arg\\max\\limits_\\theta\\sum_{y}P(y|x_i,\\theta^{(t)})ln\\frac{P(X,y|\\theta)}{P(y|x_i,\\theta^{(t)})}$$ $$\\: \\: \\Longleftrightarrow arg\\max\\limits_\\theta\\sum_{y}P(y|x_i,\\theta^{(t)})lnP(X,y|\\theta)=E_y(lnP(X,y|\\theta)|X,\\theta^{(t)})=Q(\\theta|\\theta^{(t)})$$ $$\\therefore E-step :Find \\: Q(\\theta|\\theta^{(t)})$$ $\\Longleftrightarrow$ Find the expectation of the complete-data loglikelihood with respect to the missing data y given the observed data x and the current parameter estimates $\\theta^{(t)}$. $$M-step=\\theta^{(t+1)}=arg\\max\\limits_\\theta Q(\\theta|\\theta^{(t)})$$ EM收斂性 範例1197 animals are distributed multinomially into four categories with cell-probabilities$(\\frac{1}{2}+ \\frac{\\theta}{4}, \\frac{(1-\\theta)}{4}, \\frac{(1-\\theta)}{4}, \\frac{\\theta}{4})$, where $\\theta \\in (0,1)$is unknown Observed Data:$$x=(x_1,x_2,x_3,x_4)=(125,18,20,34)$$ Likelihood:$$L(\\theta；x)=\\frac{n!}{x_1!x_2!x_3!x_4!}(\\frac{1}{2}+\\frac{\\theta}{4})^{x_1}(\\frac{1}{4}-\\frac{\\theta}{4})^{x_2}(\\frac{1}{4}-\\frac{\\theta}{4})^{x_3}(\\frac{\\theta}{4})^{x_4}$$ Find MLE by maximizing loglikelihood Now use EM to find MLE 假設我們的隱藏變數在a裡面，令$y=x_{11}+x_{12}$ 完整的變數擴展為$(x_{11},x_{12},x_{2},x_{3},x_{4})$有5個 初始其參數 $(\\frac{1}{2}, \\frac{\\theta}{4}, \\frac{1}{4}-\\frac{\\theta}{4}, \\frac{\\theta}{4})$ 其概似函數如下 $$L(\\theta；x)=\\frac{n!}{x_{11}!x_{12}!x_2!x_3!x_4!}(\\frac{1}{2})^{x_{11}}(\\frac{\\theta}{4})^{x_{12}}(\\frac{1}{4}-\\frac{\\theta}{4})^{x_{2}+x_{3}}(\\frac{\\theta}{4})^{x_{4}}$$ E 步驟 給定機率模型參數$\\theta^{(t)}$和$(x_1,x_2,x_3,x_4)$，$$x_{11}=\\frac{2x_{1}}{2+\\theta}\\quad and \\quad x_{12}=\\frac{\\theta x_{1}}{2+\\theta}$$ 推導Q函數(令$y=x_{12}$)$$Q(\\theta |\\theta^{(t)})=E_y[log p(x,y|\\theta)|x, \\theta^{(t)}]$$ $$=E_y[(x_{12}+x_{4})log\\theta +(x_{2}+x_{3})log(1-\\theta)|x,\\theta^{(t)}]$$ $$=(E_y[x_{12}|x, \\theta^{(t)}]+x_{4})log\\theta +(x_{2}+x_{3})log(1-\\theta)$$ $$=(\\frac{\\theta^{(t)}x_{1}}{2+\\theta^{(t)}}+x_{4})log\\theta + (x_{2}+x_{3})log(1-\\theta)$$ 其中$$x_{12}|_{x,\\theta^{(t)}}\\sim Binomial(x_{1},\\frac{\\theta^{(t)}}{2+\\theta^{(t)}})$$ $$x_{12}^{(t)}=E_y[x_{12}|x,\\theta^{(t)}]=\\frac{\\theta^{(t)}x_{1}}{2+\\theta^{(t)}}$$ M 步驟 對Q函數進行微分 給定$(x_{11},x_{12},x_{3},x_{4},x_{5})$ $$\\theta^{(t+1)}=\\frac{x_{12}^{(t)}+x_{4}}{x_{12}^{(t)}+x_{2}+x_{3}+x_{4}}$$ 重複以上步驟到參數迭代至穩定 R 實作12345678910111213mult = function(theta, x, n)&#123; tmp = theta for(i in 1:n)&#123; # E-step x12 = x[1]*(theta/ (2 + theta)) # M-step theta = (x12 + x[4])/(x12 + x[2] + x[3] + x[4]) tmp = c(tmp, theta) &#125;&#125;x=c(125, 18, 20, 34)mult(0.1, x, 10) 範例2範例3假設現在有兩枚硬幣A、B 我們用一枚公正的硬幣來決定，投擲 A 或 B 硬幣 依據結果，投擲 A 或 B 硬幣 1次，記錄其結果 反覆進行n次，最終可得到類似如下結果: 10111011…. 1表示正面，0表示反面 如果我們今天只能觀察到最終結果，無法知道每一次投擲來自哪一枚硬幣，該如何估計出兩個硬幣出現正面機率? : Observed Data : $X=(x_1,x_2,…,x_n), x_i:正面出現次數$A、B 出現正面機率 : $\\theta =(p,q)$ $1^{\\circ}$ 從MLE想法出發 概似函數 :$$L(\\theta|X)=P(X|\\theta)=\\prod_{i=1}^nP(x_i|\\theta)$$ $$\\hat p=\\frac{使用A硬幣骰到正面次數}{使用A硬幣總投擲次數}$$ $$\\hat q=\\frac{使用B硬幣骰到正面次數}{使用B硬幣總投擲次數}$$ 但因為我們並不知道 $x_i$來自哪個硬幣(機率模型)，所以無法進行計算。 $2^{\\circ}$ 嘗試添加隱藏變數，使其變成complete data，運用EM演算法 根據observed data，我們無從得知 $x_i$來自哪個硬幣(機率模型)， 因此我們添加一個隱藏變數 $y_i$，其表示 $x_i$ 來自哪個硬幣，$Y=(y_1,y_2,…,y_n)$ $y_i = 1$ or $2$，$x_i|y_{i}=1 \\sim Ber(p_{1})$，$x_i|y_{i}=2 \\sim Ber(p_{2})$ E-step : $$\\Rightarrow Q(\\theta|\\theta^{(t)})=E_y[ln(p(x,y|\\theta))|x,\\theta ^{(t)}]=E_y[\\sum_{i=1}^{n}ln(p(y_i|\\theta)p(x_i|y_i,\\theta))|x,\\theta^{(t)}]$$ $$=\\sum_{i=1}^{n}E_y[ln(p(y_i|\\theta)p(x_i|y_i,\\theta))|x,\\theta^{(t)}]=\\sum_{i=1}^{n}\\sum_{y_i=0}^{1}[ln(p(y_i|\\theta)p(x_i|y_i,\\theta))p(y_i|x_i,\\theta^{(t)} )]$$ $$=\\sum_{i=1}^{n}\\sum_{j=0}^{1}[ln(p(y_i=j|\\theta)p(x_i|y_i,\\theta))p(y_i=j|x_i,\\theta^{(t)} )]$$ 其中$p(y_i=j|x_i,\\theta^{(t)} )$ : 在第t次迭代下，當前數據來自哪個硬幣的機率 Q-step : $$\\frac{\\partial Q}{\\partial p}=\\frac{\\partial (\\sum_{i=1}^{n}ln(\\frac{1}{2}p^{x_i}(1-p)^{1-x_i})p(y_i=1|x_i,\\theta^{(t)} )}{\\partial p}$$ $$=\\frac{\\partial (\\sum_{i=1}^{n}ln(\\frac{1}{2})+x_iln(p)+(1-x_i)ln(1-p)p(y_i=1|x_i,\\theta^{(t)} )}{\\partial p}$$ $$=\\sum_{i=1}^{n}(\\frac{xi}{p}-\\frac{(1-x_i)}{1- p})p(y_i=1|x_i,\\theta^{(t)} )=0$$ $$\\Rightarrow p^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=1|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}p(y_i=1|x_i,\\theta^{(t)})}$$ $$\\frac{\\partial Q}{\\partial q}=0$$ 同理可得,$$q^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=2|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}p(y_i=2|x_i,\\theta^{(t)})}$$ $3^{\\circ}$ 綜觀以上結果，可以發現實際上我們只需要計算出$p(y_i=j|x_i,\\theta^{(t)} )$，就可以拿來進行EM迭代。 $\\Rightarrow$ 給定初始值$(p^{(0)},q^{(0)})$，計算出$p(y_i=j|x_i,\\theta^{(0)} )$，代入更新參數$p^{(t+1)},q^{(t+1)}$，重複迭代，直到收斂或者達到自行給定tolerance內. $4^{\\circ}$ 若改成依據結果，連續投擲 A 或 B 硬幣 10次，記錄其結果 反覆進行n次，最終可得到類似如下結果: 1表示正面，0表示反面 最終Q-step 的參數公式 :$$p^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=1|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}10p(y_i=1|x_i,\\theta^{(t)}}$$ $$q^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=2|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}10p(y_i=2|x_i,\\theta^{(t)})}$$ Python 實作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import numpy as npimport pandas as pdfrom scipy import stats# 5組硬幣投擲結果(n=5,k=10)，1代表正面，0代表反面observations = np.array([[1, 0, 0, 0, 1, 1, 0, 1, 0, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1]])da=pd.DataFrame(observations, index=[\"第一次\",\"第二次\",\"第三次\",\"第四次\",\"第五次\"])da.columns = [1,2,3,4,5,6,7,8,9,10];da###def em_single(priors,observations): \"\"\" EM算法-單次疊代 ------------ priors:[theta_A,theta_B] observation:[m X n matrix] Returns --------------- new_priors:[new_theta_A,new_theta_B] :param priors: :param observations: :return: \"\"\" counts = &#123;'A': &#123;'H': 0, 'T': 0&#125;, 'B': &#123;'H': 0, 'T': 0&#125;&#125; theta_A = priors[0] theta_B = priors[1] #E step for observation in observations: len_observation = len(observation) #計算投擲次數 num_heads = observation.sum() #正面次數 num_tails = len_observation-num_heads #反面次數 #二項分配公式求解 contribution_A = scipy.stats.binom.pmf(num_heads,len_observation,theta_A) #Bin(x,n,p) contribution_B = scipy.stats.binom.pmf(num_heads,len_observation,theta_B) #計算在給定資料、當前參數下，資料來自哪個硬幣的機率 weight_A = contribution_A / (contribution_A + contribution_B) # p(y=1|x,theta) weight_B = contribution_B / (contribution_A + contribution_B) # p(y=0|x,theta) #更新在當前參數下A，B硬幣產生的正反面次數 counts['A']['H'] += weight_A * num_heads # num += 1 =&gt; num = num+1， sum p(y_i=1|x,theta)*x_i counts['A']['T'] += weight_A * num_tails counts['B']['H'] += weight_B * num_heads counts['B']['T'] += weight_B * num_tails # M step new_theta_A = counts['A']['H'] / (counts['A']['H'] + counts['A']['T']) #sum p(y_i=1|x,theta)*x_i / sum 10*p(y_i=1|x,theta) new_theta_B = counts['B']['H'] / (counts['B']['H'] + counts['B']['T']) #sum p(y_i=0|x,theta)*x_i / sum 10*p(y_i=1|x,theta) return [new_theta_A,new_theta_B] ###def em(observations,prior,tol = 1e-6,iterations=10000): \"\"\" EM算法 ：param observations :觀測數據 ：param prior：模型初始值 ：param tol：迭代结束阈值 ：param iterations：最大迭代次數 ：return：局部最佳的模型參數 \"\"\" iteration = 0; while iteration &lt; iterations: new_prior = em_single(prior,observations) delta_change = np.abs(prior[0]-new_prior[0]) if delta_change &lt; tol: break else: prior = new_prior iteration +=1 return new_prior,iterationprint (\"(p,q,iteration)=\",em(observations,[0.7,0.5])) Ans : (p,q,iteration)= ([0.79678865844706648, 0.51958340803243785], 12)","tags":[{"name":"EM","slug":"EM","permalink":"https://caocharles.github.io/tags/EM/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://caocharles.github.io/tags/Algorithm/"}],"categories":[{"name":"機器學習","slug":"machine-learning","permalink":"https://caocharles.github.io/categories/machine-learning/"}]},{"title":"我的測試文章","date":"2018-08-27T08:23:30.000Z","path":"我的測試文章/","text":"連結測試 打打看文字。 12345 1x+2y =5 $$\\sigma$$ $$\\frac{1}{2}$$ TENSORFLOW’S HELLO WORLD In this notebook we will overview the basics of TensorFlow, learn it’s structures and see what is the motivation to use it - How does TensorFlow work? - Building a Graph - Defining multidimensional arrays using TensorFlow - Why Tensors? - Variables - Placeholders - Operations—————-# How does TensorFlow work?TensorFlow’s capability to execute the code on different devices such as CPUs and GPUs is a consequence of it’s specific structure:TensorFlow defines computations as Graphs, and these are made with operations (also know as “ops”). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.To execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU.For example, the image below represents a graph in TensorFlow. _W_, _x_ and b are tensors over the edges of this graph. _MatMul_ is an operation over the tensors _W_ and _x_, after that _Add_ is called and add the result of the previous operator with _b_. The resultant tensors of each operation cross the next one until the end where it’s possible to get the wanted result.### Importing TensorFlowTo use TensorFlow, we need to import the library. We imported it and optionally gave it the name “tf”, so the modules can be accessed by tf.module-name:1import tensorflow as tf—————–# Building a GraphAs we said before, TensorFlow works as a graph computational model. Let’s create our first graph.To create our first graph we will utilize source operations, which do not need any information input. These source operations or source ops will pass their information to other operations which will execute computations.To create two source operations which will output numbers we will define two constants:12a = tf.constant([2])b = tf.constant([3])After that, let’s make an operation over these variables. The function tf.add() adds two elements (you could also use c = a + b).12c = tf.add(a,b)#c = a + b is also a way to define the sum of the termsThen TensorFlow needs to initialize a session to run our code. Sessions are, in a way, a context for creating a graph inside TensorFlow. Let’s define our session:1session = tf.Session()Let’s run the session to get the result from the previous defined ‘c’ operation:12result = session.run(c)print(result)Close the session to release resources:1session.close()To avoid having to close sessions every time, we can define them in a with block, so after running the with block the session will close automatically:123with tf.Session() as session: result = session.run(c) print(result)Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your edge (In this case our constants), include nodes (operations, like _tf.add_), and start a session to build a graph.### What is the meaning of Tensor?In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.The word tensor from new latin means “that which stretches”. It is a mathematical object that is named tensor because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays. That’s great, but… what are these multidimensional arrays? Going back a little bit to physics to understand the concept of dimensions: [Image Source] The zero dimension can be seen as a point, a single object or a single item. The first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements. The second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. The third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line. The Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on… As mathematical objects: [Image Source] Summarizing: Dimension Physical Representation Mathematical Object In Code Zero Point Scalar (Single Number) [ 1 ] One Line Vector (Series of Numbers) [ 1,2,3,4,… ] Two Surface Matrix (Table of Numbers) [ [1,2,3,4,…], [1,2,3,4,…], [1,2,3,4,…],… ] Three Volume Tensor (Cube of Numbers) [ [[1,2,…], [1,2,…], [1,2,…],…], [[1,2,…], [1,2,…], [1,2,…],…], [[1,2,…], [1,2,…], [1,2,…] ,…]… ] Defining multidimensional arrays using TensorFlowNow we will try to define such arrays using TensorFlow: 12345678910111213Scalar = tf.constant([2])Vector = tf.constant([5,6,2])Matrix = tf.constant([[1,2,3],[2,3,4],[3,4,5]])Tensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )with tf.Session() as session: result = session.run(Scalar) print (\"Scalar (1 entry):\\n %s \\n\" % result) result = session.run(Vector) print (\"Vector (3 entries) :\\n %s \\n\" % result) result = session.run(Matrix) print (\"Matrix (3x3 entries):\\n %s \\n\" % result) result = session.run(Tensor) print (\"Tensor (3x3x3 entries) :\\n %s \\n\" % result) Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types: 12345678910111213Matrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])Matrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])first_operation = tf.add(Matrix_one, Matrix_two)second_operation = Matrix_one + Matrix_twowith tf.Session() as session: result = session.run(first_operation) print (\"Defined using tensorflow function :\") print(result) result = session.run(second_operation) print (\"Defined using normal expressions :\") print(result) With the regular symbol definition and also the TensorFlow function we were able to get an element-wise multiplication, also known as Hadamard product. But what if we want the regular matrix product? We then need to use another TensorFlow function called tf.matmul(): 123456789Matrix_one = tf.constant([[2,3],[3,4]])Matrix_two = tf.constant([[2,3],[3,4]])first_operation = tf.matmul(Matrix_one, Matrix_two)with tf.Session() as session: result = session.run(first_operation) print (\"Defined using tensorflow function :\") print(result) We could also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel! Why Tensors?The Tensor structure helps us by giving the freedom to shape the dataset the way we want. And it is particularly helpful when dealing with images, due to the nature of how information in images are encoded, Thinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional strucutre (a matrix)… until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particulary helpful. Images are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this: [Image Source] So the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. VariablesNow that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables. To define variables we use the command tf.variable().To be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. This is done by running tf.global_variables_initializer(). To update the value of a variable, we simply run an assign operation that assigns a value to the variable: 1state = tf.Variable(0) Let’s first create a simple counter, a variable that increases one unit at a time: 123one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value) Variables must be initialized by running an initialization operation after having launched the graph. We first have to add the initialization operation to the graph: 1init_op = tf.global_variables_initializer() We then start a session to run the graph, first initialize the variables, then print the initial value of the state variable, and then run the operation of updating the state variable and printing the result after each update: 123456with tf.Session() as session: session.run(init_op) print(session.run(state)) for _ in range(3): session.run(update) print(session.run(state)) PlaceholdersNow we know how to manipulate variables inside TensorFlow, but what about feeding data outside of a TensorFlow model? If you want to feed data to a TensorFlow model from outside a model, you will need to use placeholders. So what are these placeholders and what do they do? Placeholders can be seen as “holes” in your model, “holes” which you will pass the data to, you can create them using tf.placeholder(_datatype_), where _datatype_ specifies the type of data (integers, floating points, strings, booleans) along with its precision (8, 16, 32, 64) bits. The definition of each data type with the respective python sintax is defined as: Data type Python type Description DT_FLOAT tf.float32 32 bits floating point. DT_DOUBLE tf.float64 64 bits floating point. DT_INT8 tf.int8 8 bits signed integer. DT_INT16 tf.int16 16 bits signed integer. DT_INT32 tf.int32 32 bits signed integer. DT_INT64 tf.int64 64 bits signed integer. DT_UINT8 tf.uint8 8 bits unsigned integer. DT_STRING tf.string Variable length byte arrays. Each element of a Tensor is a byte array. DT_BOOL tf.bool Boolean. DT_COMPLEX64 tf.complex64 Complex number made of two 32 bits floating points: real and imaginary parts. DT_COMPLEX128 tf.complex128 Complex number made of two 64 bits floating points: real and imaginary parts. DT_QINT8 tf.qint8 8 bits signed integer used in quantized Ops. DT_QINT32 tf.qint32 32 bits signed integer used in quantized Ops. DT_QUINT8 tf.quint8 8 bits unsigned integer used in quantized Ops. [Table Source] So we create a placeholder: 1a = tf.placeholder(tf.float32) And define a simple multiplication operation: 1b = a*2 Now we need to define and run the session, but since we created a “hole” in the model to pass the data, when we initialize the session we are obligated to pass an argument with the data, otherwise we would get an error. To pass the data to the model we call the session with an extra argument feed_dict in which we should pass a dictionary with each placeholder name folowed by its respective data, just like this: 123with tf.Session() as sess: result = sess.run(b,feed_dict=&#123;a:3.5&#125;) print (result) Since data in TensorFlow is passed in form of multidimensional arrays we can pass any kind of tensor through the placeholders to get the answer to the simple multiplication operation: 12345dictionary=&#123;a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] &#125;with tf.Session() as sess: result = sess.run(b,feed_dict=dictionary) print (result) OperationsOperations are nodes that represent the mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensor or maybe an activation function. _tf.matmul_, _tf.add_, _tf.nn.sigmoid_ are some of the operations in TensorFlow. These are like functions in python but operate directly over tensors and each one does a specific thing. Other operations can be easily found in: https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html 12345678910a = tf.constant([5])b = tf.constant([2])c = tf.add(a,b)d = tf.subtract(a,b)with tf.Session() as session: result = session.run(c) print ('c =: %s' % result) result = session.run(d) print ('d =: %s' % result) _tf.nn.sigmoid_ is an activiation function, it’s a little more complicated, but this function helps learning models to evaluate what kind of information is good or not. Want to learn more?Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM’s Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a free version of PowerAI. Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at Data Science ExperienceThis is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies. Thanks for completing this lesson!Notebook created by: Rafael Belo Da Silva References:https://www.tensorflow.org/versions/r0.9/get_started/index.htmlhttp://jrmeyer.github.io/tutorial/2016/02/01/TensorFlow-Tutorial.htmlhttps://www.tensorflow.org/versions/r0.9/api_docs/python/index.htmlhttps://www.tensorflow.org/versions/r0.9/resources/dims_types.htmlhttps://en.wikipedia.org/wiki/Dimensionhttps://book.mql4.com/variables/arrayshttps://msdn.microsoft.com/en-us/library/windows/desktop/dn424131(v=vs.85).aspx Copyright &copy; 2016 Big Data University. This notebook and its source code are released under the terms of the MIT License.","tags":[{"name":"這是標籤","slug":"這是標籤","permalink":"https://caocharles.github.io/tags/這是標籤/"},{"name":"這是標籤2","slug":"這是標籤2","permalink":"https://caocharles.github.io/tags/這是標籤2/"}],"categories":[{"name":"這是分類","slug":"這是分類","permalink":"https://caocharles.github.io/categories/這是分類/"},{"name":"這是子分類","slug":"這是分類/這是子分類","permalink":"https://caocharles.github.io/categories/這是分類/這是子分類/"}]},{"title":"Hello World","date":"2018-08-27T07:58:15.056Z","path":"hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]}]}