{"pages":[{"title":"é—œæ–¼","date":"2018-09-02T08:56:09.963Z","path":"about/index.html","text":"æŸ¥çˆ¾æ–¯çš„å¤©åœ°é€™æ˜¯ä¸€å€‹æˆ‘æ•´ç†çŸ¥è­˜åŠåˆ†äº«å¿ƒå¾—çš„éƒ¨è½æ ¼ï¼Œä»¥å‰å°±æƒ³è¦è‡ªå·±å»ºç«‹ä¸€å€‹éƒ¨è½æ ¼ï¼Œå‰›å¥½åœ¨ç¢©ä¸€å‡ç¢©äºŒçš„é€™å€‹æš‘å‡ï¼Œæˆ‘ä¾†åˆ°ä¿¡é‚¦é›»å­å¯¦ç¿’ï¼Œé–‹å§‹äº†æˆ‘çš„ç¨‹å¼è¨“ç·´ï¼Œåœ¨é€™å€‹éç¨‹ä¸­æˆ‘ç™¼ç¾æˆ‘çš„ç¨‹å¼èƒ½åŠ›é‚„æœ‰å¾…åŠ å¼·ï¼Œå¸Œæœ›è—‰ç”±é€™å€‹éƒ¨è½æ ¼åŠç¢©äºŒèˆ‰è¾¦çš„è®€æ›¸æœƒï¼Œèƒ½ç²¾é€²æˆ‘è¡¨é”èƒ½åŠ›å’Œç¨‹å¼çš„æ’°å¯«èƒ½åŠ›ã€‚"},{"title":"categories","date":"2018-08-28T09:23:35.000Z","path":"categories/index.html","text":""},{"title":"tags","date":"2018-08-28T09:23:14.000Z","path":"tags/index.html","text":""}],"posts":[{"title":"å•†æ¥­ç°¡å ±è£½ä½œ","date":"2018-11-10T15:42:01.000Z","path":"å•†æ¥­ç°¡å ±è£½ä½œ/","text":"ç°¡å ±èª²inåœ‹æ³° (11/10) æ™‚é–“ : 2018/11/10 13:30-15:30 åœ°é» : åœ‹æ³°æ¼”è¬›å»³ èŒ¶é» : è²“èŒ¶ç”º Lecture 1 å•†æ¥­ç°¡å ±è£½ä½œç°¡å ±å…§å®¹ è¬›å¸« : é»ƒç´¹å³° èª²ç¨‹ : å•†æ¥­ç°¡å ±è£½ä½œ è½å¾—æ‡‚ã€åå¾—ä½çš„ç°¡å ± åªæœ‰15åˆ†é˜è¦æ€éº¼åšç°¡å ± 14 mins å ±å‘Šæµç¨‹å®‰æ’ Hello 30s ç°¡å–®ä»‹ç´¹è‡ªå·±éšŠä¼ Problem 1m é—¡è¿°å•é¡Œ : å®šç¾©å•é¡Œè¶Šç°¡å–®è¶Šå¥½ Solution 4m F,A,B Feature åŠŸèƒ½ Advantage å„ªé» Benifit æ•ˆç›Š Demo 4m30s Steps before/after Feedback 1m Who backs you up? Review 3m Recap,sum up æ¥æ£’çš„å•é¡Œ ä¸Šå°çš„äººè¦æ€éº¼æ¹Šåœ¨ä¸€èµ·ï¼Œå¦‚ä½•æ¥æ£’ é¿å…é‡è¤‡æ€§çš„è©±èª é‡è¤‡æ¼”ç·´æ¸›å°‘é‡è¤‡æ€§ 10mins QA Take notes(æº–å‚™ç´™ç­†) Appendix(é™„ä»¶) è£½ä½œç°¡å ±ç¬¬1æ­¥(ä¸è¦å¥—ç”¨æ¨¡æ¿)Make your temps æŸ¥çˆ¾æ–¯çš„æ¨¡æ¿ ç¬¬2æ­¥(å­—å‹) é¸æ“‡æ²’æœ‰è¥¯ç·šçš„å­—å‹æ¯”è¼ƒå¥½çœ‹ Google NOTO Sans(æ€æºé«”) å¾®è»Ÿæ­£é»‘ å¾®è»Ÿé›…é»‘ å­—çš„å¤§å°(28ä»¥ä¸Š)ã€ç²—ç´°ã€é–“è· ç”¨å­—çš„å¤§å°ä¾†æ’è¡¨(åšå‡ºåœ–è¡¨æ•ˆæœ) ç¬¬3æ­¥(è¨Šæ¯) Message = Info - Noise å¼·èª¿é‡é»(é¡è‰²ã€ç²—ç´°ã€) æ¬¡é‡é»åšå¼±åŒ–(åšä¸€å€‹é€æ˜åº¦80%çš„ç°ç‰‡æŠŠè³‡è¨Šè¦†è“‹ä½) ç¬¬4æ­¥(åœ–ç‰‡) é¸æ“‡é©åˆçš„åœ–ç‰‡ é¸æ“‡pngå»èƒŒå¥½çš„åœ–ç‰‡ ç”¨åœ–å»æœåœ– ä½¿ç”¨photoshopå»èƒŒå®Œå†è²¼ä¸Š png å¾ˆæ£’ï¼Œæœ‰å»èƒŒ ICON å¯ä»¥æ‰¾è·Ÿå¤§æ¨¹çš„ICON(æ„Ÿè¦ºè »ä¸éŒ¯çš„) å¿«é€Ÿæ‰¾å–ç¾æˆçš„ICON nounproject(å‘é‡æª”) https://thenounproject.com/ flaticon(æŸ¥çˆ¾æ–¯æ„›ç”¨çš„) https://www.flaticon.com/ undraw(åœ–) https://undraw.co/ æ‰¾çš„ICONè¦æœ‰ä¸€è‡´æ€§ æ¼¸å±¤ICON(ä½¿ç”¨photoshop) ICONä¸‹é¢æœ€å¥½åŠ ä¸€è¡Œè¨»è§£ ç¬¬5æ­¥(Toneè‰²èª¿) è¨­å‚™çš„æ–°èˆŠ èˆŠçš„è¨­å‚™(ç™½åº•é»‘å­—è¼ƒå¥½) æ–°çš„è¨­å‚™(é»‘åº•ç™½å­—è¼ƒå¥½) è¢å¹•å¾ˆå¤§(é»‘åº•ç™½å­—è¼ƒå¥½) é¡è‰²ä¸Šçš„å–ç”¨ é…åˆå…¬å¸çš„ä¸»é¡Œé¡è‰² é¸æ“‡ç¾çš„ã€è€çœ‹çš„é¡è‰² ç¬¦åˆè‡ªå·±ä¸»é¡Œçš„é¡è‰² PowerpointåªåƒMP4(æ¯”è³½æ‡‰è©²ç”¨ä¸åˆ°) ç¬¬6æ­¥(Diagramsæ•¸æ“šåŠè³‡æ–™) åœ–è¡¨æœ€å¥½éƒ½é‡æ–°è£½ä½œä¸€æ¬¡ ä½¿ç”¨åœ–çš„æ–¹å¼è¡¨é”(åœ“é¤…åœ–) ä¸èƒ½äº‚ç”¨ï¼Œæœƒè¢«æ¸…ç¥¥ç½µ ç¬¬7æ­¥(Animation) æˆ‘ä¹Ÿä¸æƒ³ç”¨(è¿ªå£«å°¼çš„çœ‹èµ·ä¾†é‚„ä¸éŒ¯è€¶) ç°¡å ±æ²’å…§å®¹å¯ä»¥ç”¨å‹•ç•«è·Ÿè¯éº—èƒŒæ™¯å…‹æœ(å°¤å…¶æ˜¯æ¸…ç¥¥çš„èª²) ç¬¬8æ­¥(Details æˆ‘æœ€æ„›çš„ç´°ç¯€) Alignment(ä¸€è‡´æ€§) å­—é«”ã€å¤§å°ã€åœ–è¡¨ã€ICONã€ä¸­è‹±æ–‡ Use gradient() ä¸è¦ä½¿ç”¨è¨­è¨ˆå»ºè­° å¿ æ–¼è‡ªå·±çš„ç¾æ„Ÿ Use less 3D() 3Déæ™‚ ä¸è¦åå°„(ä¸æµè¡Œ) Position Before v.s. After Leave some balnk Donâ€™t roll slides On Time!!!!! è¦åšé€å­—ç¨¿ï¼Œæ¸›å°‘è´…è©çš„ä½¿ç”¨ï¼ŒåŠ å¼·èªªè©±æ–¹å¼(å¾ˆé›£ä½†æ˜¯æœ‰ç”¨) å¯ä»¥ç”¨ä¸è¬›è©±ä¾†å¼·èª¿ æƒ³è¦è¡¨é”è‡ªå·±åšå¾ˆå¤šäº‹ æ”¾å¾ˆå¤šæ±è¥¿ï¼Œæˆ–æ˜¯å¯†é›†çš„è¡¨æ ¼ ä¸ç”¨è¬›å¤ªå¤š å°±è·Ÿå¯«æ¸…ç¥¥çš„ä½œæ¥­ä¸€æ¨£ å¥½çœ‹çš„æ’ç‰ˆ","tags":[{"name":"PPT","slug":"PPT","permalink":"https://caocharles.github.io/tags/PPT/"}],"categories":[{"name":"ç°¡å ±èª²ç¨‹åœ¨åœ‹æ³°","slug":"ç°¡å ±èª²ç¨‹åœ¨åœ‹æ³°","permalink":"https://caocharles.github.io/categories/ç°¡å ±èª²ç¨‹åœ¨åœ‹æ³°/"}]},{"title":"å•†æ¥­ç°¡å ±å°é¢¨","date":"2018-11-10T15:42:00.000Z","path":"å•†æ¥­ç°¡å ±å°é¢¨/","text":"ç°¡å ±èª²inåœ‹æ³° (11/10) æ™‚é–“ : 2018/11/10 15:30-17:00 åœ°é» : åœ‹æ³°æ¼”è¬›å»³ èŒ¶é» : è²“èŒ¶ç”º Lecture 2 ç°¡å ±å•†æ¥­å°é¢¨ç°¡å ±å…§å®¹ è¬›å¸« : Ben ou èª²ç¨‹ : ç°¡å ±å•†æ¥­å°é¢¨ æµç¨‹ 30ç§’é›»æ¢¯ç†è«– èªå‡ºé©šäºº å¥½çš„é–‹å§‹æ˜¯æˆåŠŸçš„ä¸€åŠ éƒé”å¤«æ–‡é›†(å¿«çŸ­å‘½) å¿«ï¼Œç—›å¿« çŸ­ï¼Œç°¡æ˜æ‰¼è¦ å‘½ï¼Œä¸é›¢å‘½é¡Œ çŸ­å°ç²¾æ‚ æŠ“ä½æ ¹æœ¬ï¼Œç›´é”ä¸»é«” æç¶±æŒˆé ˜ï¼ŒåŒ–ç¹ç‚ºç°¡ æç…‰è§€é» è§€é»è¦ç¨ç‰¹éŸ¿äº® æ­¸ç´è¦ç·Šæ¹Šï¼Œä¸è¶…éä¸‰æ¢ éº¥è‚¯éŒ«é‡‘å­—å¡”æ–¹æ³• MECEåŸå‰‡-åƒè€ƒç¶²é  éº¥è‚¯éŒ«è§£æ±ºå•é¡Œ7æ­¥é©Ÿ è§£æ±ºå•é¡Œçš„ä¸æ˜¯èˆ‡ç”Ÿä¿±ä¾†çš„å¤©è³¦ï¼Œè€Œæ˜¯å¯ä»¥é€éè‡ªæˆ‘è¨“ç·´åŸ¹é¤Šè€Œæˆï¼Œä½ ä¹Ÿèƒ½åƒéº¥è‚¯éŒ«äººä¸€æ¨£ï¼Œå€é€Ÿè§£æ±ºå•é¡Œã€‚ é‡‘å­—å¡”æ¶æ§‹ é‡‘å­—å¡”æ–¹æ³• é‡‘å­—å¡”åŸç†å››å€‹åŸºæœ¬ç‰¹å¾µ é‡‘å­—å¡”çµæ§‹ SDSçµæœæ³• å…ˆè¬›çµè«– å†è¬›ç¶“é æœ€å¾Œå†é‡è¤‡çµè«– æ³¨æ„äº‹é … é–‹é ­ ç ´é¡Œ é‚è¼¯çµæ§‹ è¨˜æ†¶é» ä½¿ç”¨è€…å ´æ™¯ äº’å‹•&amp;å¼µåŠ› çµå°¾Key Take Away è‡³å°‘è®“è§€çœ¾è¨˜ä½ä¸€å€‹ç•«é¢ å³°å°¾æ•ˆæ‡‰ è¨˜æ†¶é» æœ‰è¨˜æ†¶é»çš„çµæŸ! Mamba out!","tags":[{"name":"PPT","slug":"PPT","permalink":"https://caocharles.github.io/tags/PPT/"}],"categories":[{"name":"ç°¡å ±èª²ç¨‹åœ¨åœ‹æ³°","slug":"ç°¡å ±èª²ç¨‹åœ¨åœ‹æ³°","permalink":"https://caocharles.github.io/categories/ç°¡å ±èª²ç¨‹åœ¨åœ‹æ³°/"}]},{"title":"äººå·¥æ™ºæ…§ç¬¬4å‘¨ç­†è¨˜","date":"2018-10-08T09:30:19.000Z","path":"äººå·¥æ™ºæ…§ç¬¬4å‘¨ç­†è¨˜/","text":"äººå·¥æ™ºæ…§èˆ‡æ‡‰ç”¨ç­†è¨˜4ç¬¬å››å‘¨å…¶ä»–çµ„å ±å‘Šè€å¸«çš„è¬›ç¾©å¤§å®¶éƒ½åœ¨çœ‹è«å‡¡è¬›è§£é¡ç¥ç¶“ç¶²è·¯ CSM æ¼”ç®—æ³•å¢åŠ nodeçš„æµç¨‹ (æœ‰7å€‹æ­¥é©Ÿ) The proposed CSM learning procedure åˆ¤æ–·æ˜¯ä¸æ˜¯ç†Ÿæ‚‰çš„case When we encounter a new case (a new input/output relationship), we first check if it is familiar to us. If it is, there is no spontaneous learning effort involved. Later the new case is merged into our knowledge system. If it is not, we might cram this unfamiliar case first. The cramming results in a strict rule with respect to this unfamiliar case. Then we will soften the strictness of the new case and do our best to merge the new case into our knowledge system. é¢å°çš„å•é¡Œå¦‚æœæ˜¯æ›¾ç¶“æœ‰éçš„è³‡æ–™ï¼Œå‰‡æœƒé¸æ“‡ç›¸é—œçš„è³‡è¨Šå›æ‡‰ã€‚ é¢å°çš„å•é¡Œå¦‚æœæ˜¯æœªæ›¾æœ‰éçš„è³‡æ–™ï¼Œå‰‡æœƒé¸æ“‡å¢åŠ ä¸€å€‹æ–°å›æ‡‰ã€‚ ASLFN (3å±¤çš„é¡ç¥ç¶“ç¶²è·¯ è¼¸å…¥-&gt;éš±è—-&gt;è¼¸å‡ºå±¤) The Adaptive Single-hidden Layer Feed-forward Neural Networks (ASLFN, i.e., the amount of adopted hidden nodes is variable) (references: Tsaih 1993; Tsaih 1998; Tsaih and Cheng 2009; Huang, Yu, Tsaih and Huang Tsaih 2014; Kuo, Lin, and Hsu 2018) 2-class categorization problem and binary inputs {âˆ’1, 1}^ğ‘š Activation Function (æ¿€æ´»å‡½æ•¸) tanh, the hyperbolic tangent activation function, is used in all hidden nodes. The Activation Value of Hidden Nodes (éš±è—å±¤çš„æ¿€æ´»å€¼) æ¿€æ´»å‡½æ•¸ä¸èƒ½äº‚çµ¦ï¼Œæœƒæœ‰æ¢¯åº¦çˆ†ç‚¸çš„æƒ…å½¢ç™¼ç”Ÿã€‚ $ğ±^ğ‘â‰¡(ğ‘¥_1^ğ‘,ğ‘¥_2^ğ‘, â€¦,ğ‘¥_ğ‘š^ğ‘ )Tï¼šğ‘¡â„ğ‘’ input vector of the ğ‘^th case$ $ğ‘¤_{ğ‘–0}^{ğ»}ï¼šthe threshold value of the ğ‘–^th hidden node$ $ğ‘¤_ğ‘–ğ‘—^ğ»ï¼šthe weight between the ğ‘–^th hidden node and the ğ‘—^th input node$ $ğ°_ğ‘–^ğ»â‰¡(ğ‘¤_ğ‘–0^ğ»,ğ‘¤_ğ‘–1^ğ»,ğ‘¤_ğ‘–2^ğ», â€¦,ğ‘¤_ğ‘–ğ‘š^ğ» )^T$; $ğ°^ğ»â‰¡{({ğ°_1^ğ»}^T,{ğ°_2^ğ»}^T,â€¦,{ğ°_ğ‘^ğ»}^T)}^T$ The Activation Value of the Output Node (è¼¸å‡ºå±¤çš„æ¿€æ´»æ•¸å€¼) Parameters and Indexes (åƒæ•¸) N denotes the number of all reference observations m denotes the number of input nodes p denotes the number of adopted hidden nodes; p equals 1 at the beginning and is adaptive $ğ‘¦^ğ‘$ denotes the desired output of the $ğ‘^th$ case, with 1.0 and -1.0 being the desired outputs of classes 1 and 2 n denotes the $ğ‘›^th$ stage of handling n reference observations ${(ğ±^1, ğ‘¦^1), (ğ±^2, ğ‘¦^2), â€¦, (ğ±^ğ‘›, ğ‘¦^ğ‘›)}$, and $ğˆ(ğ‘›)$ is the set of indices of these observations. At the $ğ‘›^th$ stage, the loss function $ğ¸_ğ‘› (ğ°)â‰¡\\frac{âˆ‘_{ğ‘=1}^{ğ‘›}(ğ‘“(ğ±^ğ‘,ğ°)âˆ’ğ‘¦^ğ‘ )^2}{ğ‘›}+10^{-3}â€–ğ°â€–^2$ $ğˆ(ğ‘›)â‰¡ğˆ_1 (ğ‘›)âˆªğˆ_2 (ğ‘›)$, where$ğˆ_1(ğ‘›)$ and $ğˆ_2(ğ‘›)$ are the set of indices of n given cases in classes 1 and 2 At the $ğ‘›^th$ stage with the reference observations ${(ğ±^ğ‘, ğ‘¦^ğ‘): ğ‘âˆˆğˆ(ğ‘›)}$, we look for an acceptable SLFN, in which the condition L regarding ${ğ‘“(ğ±^ğ‘,ğ°), âˆ€ ğ‘âˆˆğˆ(ğ‘›)}$ is satisfied. The condition ğ¿ regarding ${ğ‘“(ğ±^ğ‘,ğ°), âˆ€ ğ‘âˆˆğˆ(ğ‘›)}$ The Learning Goal (å­¸ç¿’ç›®æ¨™) At the $ğ‘›^{th}$ stage, through minimizing the loss function $ğ¸_ğ‘› (ğ°)â‰¡\\frac{âˆ‘_{ğ‘=1}^{ğ‘›}(ğ‘“(ğ±^ğ‘,ğ°)âˆ’ğ‘¦^ğ‘ )^2}{ğ‘›}+10^{-3} (âˆ‘_{ğ‘–=0}^{ğ‘}{(w_ğ‘–^o)^2} + âˆ‘_{ğ‘–=1}^{ğ‘}âˆ‘_{ğ‘—=0}^{ğ‘š}(ğ‘¤_{ğ‘–ğ‘—}^{ğ»})^2)$,the learning goal is to seek $ğ°$ where $ğ‘“(ğ±^ğ‘,ğ°)&gt;ğœˆ,âˆ€ ğ‘âˆˆğˆ_1 (ğ‘›)$ and $ğ‘“(ğ±^ğ‘,ğ°)â‰¤âˆ’ğœˆ,âˆ€ ğ‘âˆˆğˆ_2(ğ‘›)$, with $1&gt;ğœˆ&gt;0$. An alternative goal of learning is to seek ğ° that satisfies the condition ğ¿ regarding ${ğ‘“(ğ±^ğ‘,ğ°), âˆ€ ğ‘âˆˆğˆ(ğ‘›)}$ When $ğ›¼&gt;ğ›½$ is satisfied, $ğ‘“(ğ±^ğ‘,ğ°)â‰¥ğ‘£ ,âˆ€ ğ‘âˆˆğˆ_1(ğ‘›)$and $ğ‘“(ğ±^ğ‘,ğ°)â‰¤âˆ’ğ‘£,âˆ€ ğ‘âˆˆğˆ_2(ğ‘›)$ can be achieved by directly adjusting $ğ°^ğ‘œ$ according to the following: The Proposed CSM Learning Algorithm Step 1: Initialize a SLFN with one hidden node with a randomized w. Obtain the first reference observation $(ğ±^1, ğ‘¦^1)$. Set $n = 2$. Step 2: If $n &gt; N$, STOP. Step 3: Present the n reference observations ${(ğ±^ğ‘, ğ‘¦^ğ‘): c âˆˆ I(n)}$. Step 4: If the condition L regarding ${ğ‘“(ğ±^ğ‘,ğ°), âˆ€ ğ‘âˆˆğˆ(ğ‘›)}$ is satisfied, go to Step 7.1. Step 5: Save $w$. Step 6: Apply the weight-tuning mechanism to $min_{ğ°}{â¡ğ¸(ğ°)}$ to adjust $w$ until one of the following two cases occurs: a. If the condition L regarding {ğ‘“(ğ±^ğ‘,ğ°), âˆ€ ğ‘âˆˆğˆ(ğ‘›)} is satisfied, go to step 7.1. b. If an unacceptable result is obtained, then Restore w Let $p + 1 â†’ p$ and add to the existing SLFN the new $ğ‘^{th}$ hidden node with the following $ğ°_ğ‘^ğ»$ and $ğ°_ğ‘^ğ‘œ$: Step 7.1: Apply the weight-tuning mechanism one hundred times to minimizing $ğ¸_ğ‘›(ğ°)$ to adjust $ğ°$, while keeping the condition L regarding ${ğ‘“(ğ±^ğ‘,ğ°), âˆ€ ğ‘âˆˆğˆ(ğ‘›)}$ satisfied. Step 7.2: Calculate $ğ‘”_ğ‘˜^â€² âˆ€ ğ‘˜$, where $ğ°_ğ‘˜^{â€²}â‰¡ ğ° â€“ ({ğ‘¤_ğ‘˜^ğ‘œ, ğ°_ğ‘˜^ğ»})$, $ğ‘“(ğ±^ğ‘,ğ°_ğ‘˜^â€² )â‰¡ğ‘“(ğ±^ğ‘,ğ°)- ğ‘¤_{ğ‘˜}^{ğ‘œ}ğ‘_{ğ‘˜}^{ğ‘}$,$ğ›¼_ğ‘˜^â€²â‰¡ min_{ğ‘Ïµğˆ_1(ğ‘›)}â¡ğ‘“(ğ±^ğ‘,ğ°_ğ‘˜^â€² )$, $ğ›½_ğ‘˜^â€²â‰¡max_{ğ‘Ïµğˆ_2(ğ‘›)}â¡ğ‘“(ğ±^ğ‘,ğ°_ğ‘˜^â€²)$, and $ğ‘”_ğ‘˜^â€²â‰¡ ğ›¼_ğ‘˜^â€²âˆ’ğ›½_ğ‘˜^â€²$. Step 7.3: If $- \\theta &gt;max_{1&lt;k&lt;p}{g_{k}^{â€˜}}$, where $\\theta$ is a given constant, go to Step 2. Step 7.4: If $max_{1&lt;k&lt;p}^â¡{ğ‘”_ğ‘˜^â€²} &gt; 0$, prune the $ğ‘–^{th}$ hidden node, in which ğ‘– is the first index of $arg max_{1&lt;k&lt;p}ğ‘”_{ğ‘˜}^{â€²}$, $p-1-&gt;p$, $ğ°_ğ‘–^â€²-&gt;w$, and go to Step 7.1. Flowchart of the proposed algorithm (æµç¨‹åœ–) Explanation of the Proposed CSM Learning Algorithm (è§£é‡‹CSMå­¸ç¿’æ¼”ç®—æ³•) Step 6 conducts the cramming mechanism. The Appendix shows the properness of the cramming mechanism. All hidden nodes use the same activation function, but some of them have the heterogeneity due to their large associated weights. The total amount of used hidden nodes will be large if new hidden nodes are added frequently. Step 7.1 and Step 7.5.c are designed to soften the heterogeneity. At Step 6, Step 7.1 and Step 7.5.c, the weight-tuning mechanism is applied to minimizing $ğ¸_ğ‘›(ğ°)$ to adjust weights, while there is the regularization term in $ğ¸_ğ‘›(ğ°)$. The total amount of used hidden nodes will be large if new hidden nodes are added frequently. Step 7.2 to Step 7.5 are designed to merge the unfamiliar case into the knowledge system via reducing the total amount of used hidden nodes. The pruning issue file shows the logic of the merging mechanism. The irrelevant hidden node The irrelevance examination mechanism No under-fitting The Regularization term in $ğ¸_ğ‘›(ğ°)$ Regarding the overfitting issue, the regularization term in the loss function $ğ¸_ğ‘›(ğ°)$ may prevent the model from doing too well on training data. ç­†è¨˜ æ¼”ç®—æ³•çš„åœæ­¢æº–å‰‡èˆ‡å­¸ç¿’ç›®æ¨™ä¸ä¸€å®šæœƒæœ‰é—œä¿‚ Cramming (ç¡¬èƒŒä¸‹å»)Softening (è»ŸåŒ–)Merging (åˆä½µ)","tags":[{"name":"AI","slug":"AI","permalink":"https://caocharles.github.io/tags/AI/"}],"categories":[{"name":"äººå·¥æ™ºæ…§èˆ‡æ‡‰ç”¨","slug":"äººå·¥æ™ºæ…§èˆ‡æ‡‰ç”¨","permalink":"https://caocharles.github.io/categories/äººå·¥æ™ºæ…§èˆ‡æ‡‰ç”¨/"}]},{"title":"ç¬¬ä¸€å‘¨æ•™å­¸é€²åº¦","date":"2018-10-08T06:49:59.000Z","path":"ç¬¬ä¸€å‘¨æ•™å­¸é€²åº¦/","text":"","tags":[],"categories":[]},{"title":"é€²éšå‰µæ–°ç§‘æŠ€æŠ€è¡“(ç¬¬ä¸€å‘¨)","date":"2018-09-26T02:38:16.000Z","path":"é€²éšå‰µæ–°ç§‘æŠ€æŠ€è¡“/","text":"é€²éšå‰µæ–°ç§‘æŠ€æŠ€è¡“å…±ç·¨(ç¬¬ä¸€å€‹ç¦®æ‹œ) æ¯å‘¨çš„éŒ„éŸ³æª”éƒ½æ”¾åœ¨é€™è£¡ https://drive.google.com/drive/folders/1jQl4cLGAwekYkz6gSCDlR7wwxA4dqbzP?usp=sharing æ›¾è€å¸« (è¦æ±‚) çˆ¬èŸ² PTTã€FB ç‰¹å®šç¶²ç«™ (æ–‡ç« ) åƒè€ƒæ–‡ç» (æœ‰èˆˆè¶£çš„ä¸»é¡Œ) è€å¸«åªæœ‰ä¸Šå…­é€±çš„èª²ç¨‹ æ–‡å­—åˆ†æç‚ºä¸» çµæ§‹åŒ–è³‡æ–™ä¹Ÿå¯ä»¥ Kagleã€æ”¿åºœçš„å…¬é–‹è³‡æ–™ã€Researched Data å¾NLPä¸ŠçŸ¥é“å•†å“çš„è¡Œæƒ… å°ˆå®¶ç³»çµ±æ˜¯ç”šéº¼? ç¬¬ä¸€å‘¨ è¦å•¥çš„è³‡æ–™é›†ï¼ŒNLTK ä¸€çµ„å ±ä¸€ç¯‡ NLP çš„ç ”ç©¶ã€è«–æ–‡ã€æ–‡ç»æ¢è¨ ä¸‹ç¦®æ‹œå°±å¯ä»¥åšé€™ä»¶äº‹ æˆ‘æœ‰éŒ„éŸ³å˜»å˜» è‡­å™ç”·åšå£«ç”Ÿ(äººå®¶åªæ˜¯å¹´é½¡æ¯”è¼ƒå¤§ã„…â€¦) æˆ‘èªªç¦®æ‹œä¸€é‚£å€‹ HI-EXPEND æ˜¯å•¥æ„æ€ (èŠ±æ¯”è¼ƒå¤šéŒ¢ä¾†è®€æ›¸çš„äºº) èªªå¾—å¥½ï¼Œæˆ‘è¯å±±æ´¾â€¦ è¦å¹¹å˜› èªæ„åˆ†æ æƒ…æ„Ÿåˆ†æ ç¶²è·¯è²é‡åˆ†æï¼Œéƒ½ä¸å®¹æ˜“ Sources of Big Data In addition to accumulation of traditional data of transactions: Data warehousing (è³‡æ–™å€‰å„²) Cloud computing Social network Internet of Things (IOT) The business data volume is therefore increasing dramatically. Some important attributes may be embedded in or mined from the big volume of data. Therefore, data management issues for the big data are getting è¦å’ªæŒ–æ­Œçš„. Common Framework of Big Data å…­å€‹V(è‡ªå·±æŸ¥) Volume Velocity Variety Veracity Value æ·±V æˆ‘çš„å¸¥ç…§(å±å•¦) éç›£ç£å¼å­¸ç¿’ ç‰›å¥¶è·Ÿå•¤é…’(é—œè¯å¼è³‡æ–™)- ç›£ç£å¼å­¸ç¿’ ä¸€ç”Ÿåªç£ä½ ä¸€äºº æœ‰Yé‚£ä¸€æ¢çš„å¯ä»¥ç£ ç£çš„è¶Šæº–ã€è¶Šå¤¯(XGBOOSTã€DNN) æ±ºç­–æ¨¹(Desision tree) (æœ€è€çš„é‚£ç¨®) å¢å¼·å¼å­¸ç¿’æ·±åº¦å­¸ç¿’(é¡ç¥ç¶“ç¶²è·¯)AIçš„ä¸‰å¤§æ‡‰ç”¨ èªè¨€è¾¨è­˜ Prescriptive Analysis åè©:æŒ‡å°æ€§åˆ†æ é‡‹ç¾©:æ ¹æ“šé æ¸¬åˆ†æçš„çµæœï¼Œç¸½çµåŠå»ºè­°ä¸åŒçµæœçš„å„ªåŒ–è¡Œå‹•ã€‚ æ–¹æ³•:é€éé æ¸¬åˆ†æçµæœï¼Œé€²è¡Œæ±ºç­– NTLK (å¥½åƒé€™æ‰æ˜¯é‡é») æ‰¾ä¸€ç¯‡paperä¾†å ±å‘Š textminingonline.nltk è¼‰ä¸€ä¸‹å¥—ä»¶ ç¶²é åœ¨é€™ è¼‰å¥½ä¹… 123456789101112# èªæ–™åº«åœ¨é€™from nltk.corpus import brownbrown.words()[0:10]brown.tagged_words()[0:10]len(brown.words())dir(brown)# æ–·å¥# æ–·å­—tokens = word_tokenize(text) Tokenizers å¥½å¤šTokenï¼Œå¥½æƒ³ç©æ¡ŒéŠã€‚(å‚»çœ¼â€¦.) Token -&gt; ç’€ç’¨å¯¶çŸ³ç¬¬ä¸‰å ‚èª² Part-Of-Speech Tagging-1 æœ€é‡è¦çš„æ–‡å­—åˆ†æä¹‹ä¸€ â€¦æŠ•å½±ç‰‡éƒ½æœ‰(ä¹Ÿæ˜¯) Part of speech tagging-2 . 123456from nltk.corpus import treebanklen(treebank.tagged_sents())train_data = treebank.tagged_sents()[:3000]print(train_data)test_data = treebank.tagged_sents()[3000:]print(test_data) ç«Ÿèƒ½å¦‚æ­¤å„ªç§€ &lt;3(è¬æ±æ£®) åŠ©æ•™ç²¾è¯(èº«æè·ŸæŸé¾ä¸€æ¨£)é‚„è »å¯æ„›çš„ éœ€è¦åŠ å…¥è¨˜æ†¶çš„çµæ§‹ è£å‚™å¾ˆé‡è¦å— ? â€¦ â€¦ åŠ©æ•™é™¤äº†è²éŸ³æª”éƒ½å¯ä»¥å¹«æˆ‘å€‘çˆ¬ æŸé¾æƒ³æŠ“AVå¥³å„ªåœ–(ä¹¾æˆ‘é—¢æ˜¯) è¡¨ç‰¹ç‰ˆæŠ“åœ–(æˆ‘å¥½åƒæœ‰æŠ“é) æˆ‘æŠ“éè¥¿æ–¯&lt;3(æƒ³è¦++) (å¤ªè®šå•¦~) https://hackmd.io/6RIZ7tcyRymbihSUfCyIEw?view åŠ©æ•™å‰›å‰›èªªç”šéº¼æ¨¡å‹æ˜¯æœ€å·®çš„ ? å®¹æ˜“éåº¦é…é£¾çš„æ¨¡å‹å¾ˆå·®(æ‡‰è©²å§)(é©æ‹‰å¹¹) 87è¬å¼µåœ–(å¥½çŒ›)è®Šæˆ8è¬å¼µè€Œå·² æ„Ÿè¦ºå°±è·ŸAIç›¸æ©Ÿä¸€æ¨£ æ··æ·†çŸ©é™£(åˆ°åº•æœƒä¸æœƒé€²è¤‡è³½å‘¢~å¥½åˆºæ¿€) ä¸€å€‹å°è©±æ©Ÿå™¨äºº AI is A Brandâ€™s New Face Mind the tech Details Know the difference between conversation AI and conventional chatbots Integrate Key Data Sets Crawler &amp; Data Cleanup1.çˆ¬èŸ²ï¼Œéš¨æ©ŸæŠ½è³‡æ–™2.æ¡ç”¨Python packagesæˆ– BASH shellsçš†å¯3.å¯¦é©—å®¤æœ‰PTTçš„çˆ¬èŸ²è³‡æ–™4.å»ºè­°å…ˆå­¸ç¿’åŸºæœ¬linuxæŒ‡ä»¤5.é«˜é »å­—èˆ‡ä½é »å­—ï¼Œéƒ½æ¯”ä¸ä¸Šå¯ä»¥æ¸…ç†å› ç´ çš„å­— çµè«– å¹¸å¥½é‚„æœ‰åŠ©æ•™â€¦ æœ‰äººæƒ³è·Ÿåšç­ä¸€çµ„ã„‡ æˆ‘å€‘å››å€‹ä¸€çµ„ä¸æ˜¯å— NLPçš„æ–‡ç« (è¦æ‰¾å•¥)-å„è‡ªæ‰¾åœ¨æ··ä¸€æ³¢å— ç”¢å“çš„è²é‡(ä¸Šæ¸¸ä¸å¤ªçŸ¥é“é€šè·¯è³¼è²·çš„æ¶ˆè²»è€…è³‡æ–™) topical model æˆ‘çš„é®®å¥¶èŒ¶æ©Ÿå™¨äºº ä¸‹ç¦®æ‹œçš„ä½œæ¥­ æ‰¾å‡ºåˆ†æNLPçš„æ–¹æ³•å— ? æˆ‘å…ˆè®€æ±æ£®èªªçš„é‚£å…©ç¯‡ï¼Œä¸€äº›å°æ‘˜è¦ã€å¿ƒå¾—å’Œ murmur è¨˜éŒ„åœ¨é€™é‚Š","tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://caocharles.github.io/tags/Pytorch/"}],"categories":[{"name":"é€²éšå‰µæ–°ç§‘æŠ€æŠ€è¡“","slug":"é€²éšå‰µæ–°ç§‘æŠ€æŠ€è¡“","permalink":"https://caocharles.github.io/categories/é€²éšå‰µæ–°ç§‘æŠ€æŠ€è¡“/"}]},{"title":"çµ±ç ”æ·±åº¦å­¸ç¿’è®€æ›¸æœƒ","date":"2018-09-20T15:43:20.000Z","path":"çµ±ç ”æ·±åº¦å­¸ç¿’è®€æ›¸æœƒ/","text":"è®€æ›¸æœƒç°¡ä»‹å¤§å®¶å¥½ï¼Œé€™æ˜¯æˆ‘å’Œç«‹è«­ç™¼èµ·çš„è®€æ›¸æœƒï¼Œé€™å€‹è®€æ›¸æœƒçš„ç›®çš„æ˜¯å¸Œæœ›é€éè¨è«–åŠåˆ†äº«ä¾†å­¸ç¿’Machine Learning + Deep Learningï¼Œå¸Œæœ›å¤§å®¶ä»¥å¾Œä¸è¦å¤±æ¥­(ä¾†è‡ªæ±æ£®çš„æé†’) å…ˆå‚™çŸ¥è­˜ å¾®ç©åˆ†(å¾®åˆ†å§) ç·šæ€§ä»£æ•¸(å‘é‡è¼¸å…¥å¾Œçš„å‚³å°åŠé‹ç®—) æˆ‘å€‘é€™å­¸å¹´çš„é€²åº¦å¤§è‡´ç°¡å–®ä»‹ç´¹å¦‚ä¸‹(æš«å®š) ä¸Šå­¸æœŸ ä¸€é–‹å§‹å…ˆå¸¶å¤§å®¶å…¥é–€ç¨‹å¼ï¼Œä½†é‚„ä¸ç¢ºå®šè¦å¾ Keras æˆ–æ˜¯ TensorFlow å…¥æ‰‹ ä¹‹å¾Œå¤§å®¶å°±æ¯å‘¨åšå€‹ä½œæ¥­ï¼Œç„¶å¾Œä¹Ÿè¦çœ‹å€‹èª²ç¨‹ èª²ç¨‹æ–¹é¢æ‡‰è©²ä»¥æå®æ¯…çš„ Machine Learing ç‚ºä¸»ï¼Œä»–çš„èª²æœ‰ä»¥ä¸‹ç‰¹é» å¼·èª¿ç„¡ç—›å…¥é–€ï¼Œè€Œä¸”æ¶‰åŠçš„é ˜åŸŸå¾ˆå»£ï¼Œåœ–åƒåŠèªéŸ³è¾¨è­˜ã€èªæ„åˆ†æç­‰ç­‰ ä¸Šèª²æ–¹å¼æœ‰è¶£ï¼ŒåŸºæœ¬ä¸Šä¸æœƒæœ‰æ¯ç‡¥çš„æ„Ÿè¦º æ›´é‡è¦çš„æ˜¯æè€å¸«ä¸Šèª²è¬›çš„éƒ½æ˜¯å¾ˆæ–°çš„å…§å®¹ï¼Œç¸½ä¹‹å…¥é–€çœŸçš„æ¨è–¦ å¯¦ä½œæ–¹é¢çš„è³‡æºå¾ˆå¤šï¼Œå†æ…¢æ…¢ä»‹ç´¹ å¤§å®¶æœ‰æ™‚é–“çš„è©±å¯ä»¥å ±åä¸€äº›æ¯”è³½ï¼Œä¾‹å¦‚é»‘å…‹æ¾æˆ–æ˜¯ Kaggle è¨è«–å¯¦ä½œä¸Šçš„æŠ€å·§ã€è§€å¿µåŠå•é¡Œ Scikit-Learn (æ©Ÿå™¨å­¸ç¿’ ML å¥—ä»¶)ï¼Œæ¯”è¼ƒæ¬¡è¦ï¼Œä½†é‚„æ˜¯å¸Œæœ›å¤§å®¶èƒ½å¤§è‡´äº†è§£ï¼Œä¸‹å­¸æœŸçš„å¤šè®Šé‡æœƒè¼•é¬†è¨±å¤š å¯’å‡å¯’å‡å¤§å®¶å¯èƒ½å¾ˆå¿™ï¼Œä½†å¸Œæœ›å¤§å®¶é‚„æ˜¯èƒ½æ’¥é»æ™‚é–“çœ‹äº›ç›¸é—œçš„æ±è¥¿ æ—è»’ç”°çš„æ©Ÿå™¨å­¸ç¿’åŸºçŸ³è·ŸæŠ€æ³• è »ç†è«–çš„ä¸éå¤§å®¶æ˜¯ç¢©å£«æœƒç†è«–æ‡‰è©²çš„æ‰€ä»¥é‚„æ˜¯ç¡¬è‘—é ­çš®çœ‹å§ è¬›çš„æ±è¥¿å…¶å¯¦éƒ½è »åŸå§‹çš„ï¼Œæœ‰äº›æ±è¥¿å…¶å¯¦å·²ç¶“æ²’äººåœ¨ç”¨äº† ä½†æ˜¯èµ·æºçš„æ±è¥¿æˆ‘è¦ºå¾—è½ä¸€è½å¤šå°‘æœƒæœ‰å¹«åŠ© å¯èƒ½æœƒè½åˆ°å´©æ½°ï¼Œæ‡‰è©²æ˜¯æˆ‘å¤ªç¬¨(å¯èƒ½åªæœ‰AIçœŸæ­£æ‡‚) è½èªªå…¶å¯¦ä½œæ¥­è¶…çˆ†é›£(by è³‡ç§‘-åŠ‰æ˜­éºŸæ•™æˆ) ä½†æ˜¯å¤§å®¶ä¹Ÿä¸ç”¨å¯«ä½œæ¥­ï¼Œå°±å¤§å®¶è‡ªå·±çœ‹ä¸€çœ‹å°±å¥½(å§) ä¸‹å­¸æœŸç¢©ä¸€ä¸‹èª²æ¥­æœƒæŒºé‡çš„ï¼Œä½†å¥½åƒä¹Ÿé‚„å¥½ï¼Œå¤šè®Šé‡é€™é–€èª²å¦‚æœä¸Šå­¸æœŸæœ‰åœ¨è®€æ›¸æœƒå­¸åˆ°æ±è¥¿å…¶å¯¦æ‡‰è©²æœƒè »è¼•é¬†çš„ è«–æ–‡æ¢è¨ï¼Œå¦‚æœæ˜¯å¾ˆæ–°çš„å¸Œæœ›æˆ‘å€‘èƒ½æŠŠå®ƒå¯¦ä½œå‡ºä¾†çœ‹çœ‹ æœŸæœ›å¤§å®¶æœ‰æ‰¾åˆ°è‡ªå·²æƒ³åšçš„é …ç›®ï¼Œæˆ‘å€‘å¾ˆæ¨‚æ„ä¸€èµ·è¨è«–æˆ–æ˜¯æä¾›å¹«åŠ©ï¼Œæœ€å¥½çš„æƒ…æ³æ˜¯ä½ å€‘å¯ä»¥æŒ‡å°æˆ‘å€‘å˜»å˜»","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://caocharles.github.io/tags/Deep-Learning/"}],"categories":[{"name":"è®€æ›¸æœƒ","slug":"study-group","permalink":"https://caocharles.github.io/categories/study-group/"}]},{"title":"æ·±åº¦å­¸ç¿’","date":"2018-08-28T16:12:21.000Z","path":"æ·±åº¦å­¸ç¿’/","text":"é¡ç¥ç¶“ç¶²è·¯ æ­·å²äººå·¥æ™ºæ…§æœ€æ—©å‡ºç¾æ–¼1950å¹´ä»£ã€‚äººå·¥æ™ºæ…§çš„ç›®æ¨™æ˜¯å¸Œæœ›èƒ½è®“é›»è…¦åƒäººä¸€ç­æ€è€ƒèˆ‡å­¸ç¿’ã€‚è¢«è¦–ç‚ºäººå·¥æ™ºæ…§ä¹‹çˆ¶çš„åœ–éˆ(Alan Mathison Turing)ï¼Œæå‡ºäº†æœ‰åçš„åœ–éˆæ¸¬è©¦:äººé¡èˆ‡æ©Ÿå™¨å°è©±ï¼Œå¦‚æœäººé¡ç„¡æ³•æ ¹æ“šé€™äº›å°è©±éç¨‹åˆ¤æ–·å°æ–¹æ˜¯äººæˆ–æ©Ÿå™¨ï¼Œå³é€šéæ¸¬è©¦ï¼Œèªç‚ºé€™å°é›»è…¦å…·æœ‰äººå·¥æ™ºæ…§ã€‚ éš¨è‘—AIçš„ç™¼å±•æ—¥ç›ŠèŒå£¯ï¼Œ1980å¹´ä»£(Jhon Searle)ï¼Œæå‡ºäº†å°äººå·¥æ™ºæ…§çš„åˆ†é¡æ–¹å¼: å¼·äººå·¥æ™ºæ…§(Strong AI) : æ©Ÿå™¨èˆ‡äººå…·æœ‰å®Œæ•´çš„èªçŸ¥èƒ½åŠ›ã€‚ å¼±äººå·¥æ™ºæ…§(Weak AI) : æ©Ÿå™¨è¨­è¨ˆçœ‹èµ·ä¾†å…·æœ‰æ™ºæ…§å³å¯ã€‚ è€Œæ·±åº¦å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§ä¸­ï¼Œç¾ä»Šæˆé•·æœ€å¿«çš„é ˜åŸŸï¼Œéš¨è‘—é›»è…¦çš„æ™®åŠä»¥åŠå…¶CPUã€GPUé‹ç®—èƒ½åŠ›å¢å¼·ï¼Œæˆ‘å€‘å¯ä»¥é€éä¸–ç•Œä¸Šå„å€‹è³‡æ–™åº«æ”¶é›†å¤§é‡è³‡æ–™ï¼Œå°‡è³‡æ–™æ•´ç†æˆé¡ç¥ç¶“ç¶²è·¯çš„æ ¼å¼ï¼Œè—‰ç”±æ¨¡æ“¬äººé¡ç¥ç¶“ç¶²è·¯çš„é‹ä½œæ–¹å¼ï¼ŒåŠ ä¸Šæ•¸å­¸çš„æ¼”ç®—æ³•é€²è¡Œæ›´æ–°åƒæ•¸ï¼Œå°±å¯ä»¥è®“é›»è…¦å­¸ç¿’åˆ†è¾¨æ—¥å¸¸ç”Ÿæ´»çš„äº‹ç‰©ï¼Œå¸¸è¦‹çš„æ·±åº¦å­¸ç¿’æ¶æ§‹ï¼Œå¦‚å¤šå±¤æ„ŸçŸ¥å™¨MLPã€æ·±åº¦ç¥ç¶“ç¶²è·¯DNNã€å·ç©ç¥ç¶“ç¶²è·¯CNNã€éè¿´ç¥ç¶“ç¶²è·¯RNNã€‚ è€Œé€™äº›æ·±åº¦å­¸ç¿’çš„æ¶æ§‹æ‡‰ç”¨åœ¨è¦–è¦ºè¾¨è­˜ã€èªéŸ³è¾¨è­˜ã€è‡ªç„¶èªè¨€è™•ç†ã€ç”Ÿç‰©é†«å­¸ç­‰é ˜åŸŸï¼Œçš†æœ‰éå¸¸å¥½çš„æ•ˆæœã€‚ äººå·¥æ™ºæ…§ç¾åœ¨å·²å»£æ³›é‹ç”¨åœ¨æˆ‘å€‘çš„æ—¥å¸¸ç”Ÿæ´»ä¹‹ä¸­ï¼Œåƒæ˜¯æ‰‹å¯«è¾¨è­˜ã€åœ–åƒè¾¨è­˜ã€èªéŸ³è¾¨è­˜ï¼Œå¹¾ä¹éƒ½å­˜åœ¨æ–¼äººæ‰‹ä¸€å°çš„æ‰‹æ©Ÿä¹‹ä¸­ï¼Œé‚„æœ‰æ›´é€²ä¸€æ­¥çš„é‹ç”¨ï¼Œå¦‚è‡ªå‹•é§•é§›ï¼Œé€éç¡¬é«”ä¸Šçš„æ›´æ–°ï¼Œæˆ‘å€‘å¯ä»¥è£è¨­æ„Ÿæ¸¬å™¨æ„Ÿæ‡‰è»Šå­å‘¨åœçš„ç’°å¢ƒç‹€æ³ï¼ŒåŠ ä¸Šåœ–åƒè¾¨è­˜ï¼Œä¸¦æ•´åˆæˆè³‡è¨Šè®“é›»è…¦åˆ¤æ–·æ˜¯å¦åœè»Šæˆ–ç¹¼çºŒè¡Œé§›ï¼Œé€éæ¼”ç®—æ³•è®“é›»è…¦çŸ¥é“è©²å¦‚ä½•åšæ±ºå®šï¼Œåˆ°æœ€å¾Œå°‡æœƒå¯¦ç¾åœ¨å¸‚å€è¡Œé§›ä¹‹ä¸­ï¼Œéš¨è‘—ç§‘æŠ€è¶Šä¾†è¶Šé€²æ­¥ï¼Œæ·±åº¦å­¸ç¿’æ‰€æ‡‰ç”¨çš„é ˜åŸŸå°±è¶Šä¾†è¶Šå»£ã€‚ é¡ç¥ç¶“ç¶²è·¯ ç°¡ä»‹[ç¶­åŸºç™¾ç§‘] é¡ç¥ç¶“ç¶²è·¯ç°¡ç¨±ï¼ˆè‹±èªï¼šArtificial Neural Networkï¼ŒANNï¼‰ï¼Œç°¡ç¨±ç¥ç¶“ç¶²è·¯ï¼ˆNeural Networkï¼ŒNNï¼‰æˆ–é¡ç¥ç¶“ç¶²è·¯ï¼Œåœ¨æ©Ÿå™¨å­¸ç¿’å’ŒèªçŸ¥ç§‘å­¸é ˜åŸŸï¼Œæ˜¯ä¸€ç¨®æ¨¡ä»¿ç”Ÿç‰©ç¥ç¶“ç¶²è·¯ï¼ˆå‹•ç‰©çš„ä¸­æ¨ç¥ç¶“ç³»çµ±ï¼Œç‰¹åˆ¥æ˜¯å¤§è…¦ï¼‰çš„çµæ§‹å’ŒåŠŸèƒ½çš„æ•¸å­¸æ¨¡å‹æˆ–è¨ˆç®—æ¨¡å‹ï¼Œç”¨æ–¼å°å‡½å¼é€²è¡Œä¼°è¨ˆæˆ–è¿‘ä¼¼ã€‚ç¥ç¶“ç¶²è·¯ç”±å¤§é‡çš„äººå·¥ç¥ç¶“å…ƒè¯çµé€²è¡Œè¨ˆç®—ã€‚å¤§å¤šæ•¸æƒ…æ³ä¸‹äººå·¥ç¥ç¶“ç¶²è·¯èƒ½åœ¨å¤–ç•Œè³‡è¨Šçš„åŸºç¤ä¸Šæ”¹è®Šå…§éƒ¨çµæ§‹ï¼Œæ˜¯ä¸€ç¨®è‡ªé©æ‡‰ç³»çµ±ï¼Œé€šä¿—çš„è¬›å°±æ˜¯å…·å‚™å­¸ç¿’åŠŸèƒ½ã€‚ ç¾ä»£ç¥ç¶“ç¶²è·¯æ˜¯ä¸€ç¨®éç·šæ€§çµ±è¨ˆæ€§è³‡æ–™å»ºæ¨¡å·¥å…·ã€‚å…¸å‹çš„ç¥ç¶“ç¶²è·¯å…·æœ‰ä»¥ä¸‹ä¸‰å€‹éƒ¨åˆ†ï¼š çµæ§‹ï¼ˆArchitectureï¼‰çµæ§‹æŒ‡å®šäº†ç¶²è·¯ä¸­çš„è®Šæ•¸å’Œå®ƒå€‘çš„æ‹“æ’²é—œä¿‚ã€‚ä¾‹å¦‚:ç¥ç¶“ç¶²è·¯ä¸­çš„è®Šæ•¸å¯ä»¥æ˜¯ç¥ç¶“å…ƒé€£æ¥çš„æ¬Šé‡ï¼ˆweightsï¼‰å’Œç¥ç¶“å…ƒçš„æ¿€å‹µå€¼ï¼ˆactivities of the neuronsï¼‰ã€‚ æ¿€å‹µå‡½å¼ï¼ˆActivity Ruleï¼‰å¤§éƒ¨åˆ†ç¥ç¶“ç¶²è·¯æ¨¡å‹å…·æœ‰ä¸€å€‹çŸ­æ™‚é–“å°ºåº¦çš„å‹•åŠ›å­¸è¦å‰‡ï¼Œä¾†å®šç¾©ç¥ç¶“å…ƒå¦‚ä½•æ ¹æ“šå…¶ä»–ç¥ç¶“å…ƒçš„æ´»å‹•ä¾†æ”¹è®Šè‡ªå·±çš„æ¿€å‹µå€¼ã€‚ä¸€èˆ¬æ¿€å‹µå‡½å¼ä¾è³´æ–¼ç¶²è·¯ä¸­çš„æ¬Šé‡ï¼ˆå³è©²ç¶²è·¯çš„åƒæ•¸ï¼‰ã€‚ å­¸ç¿’è¦å‰‡ï¼ˆLearning Ruleï¼‰å­¸ç¿’è¦å‰‡æŒ‡å®šäº†ç¶²è·¯ä¸­çš„æ¬Šé‡å¦‚ä½•éš¨è‘—æ™‚é–“æ¨é€²è€Œèª¿æ•´ã€‚ ä¸Šåœ–ç‚ºä¸€å€‹ç°¡å–®çš„å¤šå±¤æ„ŸçŸ¥å™¨ï¼Œæˆ‘å€‘åˆ©ç”¨é€™å€‹ç¶²è·¯ é¡ç¥ç¶“ç¶²è·¯ æ‡‰ç”¨ æ‡‰ç”¨ æ‰‹å¯«è¾¨è­˜ åˆ©ç”¨Pythonä¸‹tensorflowæ¨¡çµ„ä¸­çš„MNISTè³‡æ–™ï¼Œæ”¶é›†äº†æ•¸è¬ç­†æƒæéçš„åœ–æª”åŠæ¨™ç±¤ï¼Œæˆ‘å€‘å¯ä»¥åˆ©ç”¨å„ç¨®é¡ç¥ç¶“ç¶²è·¯æ¨¡å‹å»ºç«‹åˆ†é¡å™¨æ¨¡å‹ï¼Œä¾æ“šæ¨¡å‹çš„ç‰¹æ€§åŠ ä»¥è¨“ç·´ï¼Œä¸¦å¯å°‡è¨“ç·´éçš„æ¨¡å‹å„²å­˜ï¼Œç”¨ä¾†é æ¸¬æ‰‹å¯«æ¿ä¸Šçš„æ•¸å­—ã€‚ åœ–åƒè¾¨è­˜ ç¾ä»Šé›»è…¦è¾¨è­˜ä¸€èˆ¬çš„åœ–ç‰‡å·²ç¶“ç­‰åŒæ–¼äººé¡çš„è¾¨è­˜ç‡ï¼Œç”šè‡³åœ¨åæ‡‰çš„é€Ÿåº¦ä¸Šæœƒè¶…éäººé¡ï¼Œä½†åœ¨å‹•æ…‹å½±åƒçš„è¾¨è­˜åº¦é‚„æœ‰åŠ å¼·çš„ç©ºé–“ï¼Œé€éå„ç¨®æ¼”ç®—æ³•çš„æ¸¬è©¦èˆ‡æ”¹é€²ï¼Œæœ€çµ‚å°‡æœƒæ‡‰ç”¨åœ¨è‡ªå‹•é§•é§›ç­‰æ‡‰ç”¨ä¸Šã€‚ è‡ªç„¶èªè¨€è™•ç† è‡ªç„¶èªè¨€(Nature Language Processing, NLP)ï¼Œæ˜¯è®“é›»è…¦å­¸ç¿’ç†è§£äººé¡æ‰€èªªçš„è©±èªèˆ‡æ–‡å­—ï¼Œé€éåˆ†æè©æ€§ï¼Œè¨ˆç®—å…¶è©å‘é‡ç­‰ç­‰ï¼Œä¸¦é‹ç”¨æ¨¸ç´ è²æ°åˆ†é¡å™¨å°‡å¥å­åˆ†é¡ï¼Œä¾†åˆ†ææ—¥å¸¸å°è©±çš„å«æ„ï¼Œå…¶ç”¢å‡ºæœ‰ç¾åœ¨ç†±é–€çš„èŠå¤©æ©Ÿå™¨äººåŠFBæ‰€æä¾›çš„(Facebook Messenger Platform)å’Œlineæ‰€æ¨å‡ºçš„(Messaging API)ã€‚ AIèˆ‡å¤§æ•¸æ“šçš„æ‡‰ç”¨åªæœƒè¶Šä¾†è¶Šæ·±ã€è¶Šä¾†è¶Šå»£ã€è¶Šä¾†è¶Šå¿«ï¼Œåªè¦å°ç§‘æŠ€ç™¼å±•æœ‰èˆˆè¶£çš„äººï¼Œå°±å¯ä»¥è¸å…¥é€™å¡Šé ˜åŸŸäº†è§£AIçš„å‰å¤§ã€‚","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://caocharles.github.io/tags/Deep-Learning/"}],"categories":[{"name":"æ·±åº¦å­¸ç¿’","slug":"deep-learning","permalink":"https://caocharles.github.io/categories/deep-learning/"}]},{"title":"EMæ¼”ç®—æ³•","date":"2018-08-28T09:31:35.000Z","path":"EMæ¼”ç®—æ³•/","text":"åƒè€ƒç¶²å€Maximum Likelihood from Incomplete Data via the EM Algorithm æœ€å¤§æœŸæœ›æ¼”ç®—æ³•ï¼ˆExpectation-maximization algorithmï¼Œåˆç¨±æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼‰åœ¨è³‡æ–™åˆ†æä¸­å¸¸ç”¨æ–¼åˆ†ç¾¤ï¼Œåœ¨çµ¦å®šç¾¤æ•¸åŠæ©Ÿç‡æ¨¡å‹ä¸‹ï¼Œå»å°‹æ‰¾è§€æ¸¬å€¼è®Šæ•¸é–“çš„æ‰€éš±è—çš„è¨Šæ¯ï¼Œå¯ç”¨æ­¤æ¼”ç®—æ³•ä¾†ä¼°è¨ˆæ©Ÿç‡æ¨¡å‹ä¸­çš„åƒæ•¸ä¼°è¨ˆæˆ–éºå¤±å€¼å¡«è£œã€‚ åè©ä»‹ç´¹ æ¨£æœ¬: $x$ æ¦‚ä¼¼å‡½æ•¸: $L(\\theta|x)=p(x|\\theta)$ ç›®çš„: æ‰¾åˆ°èƒ½è®“ $L(\\theta|x)$ æœ€å¤§åŒ–çš„åƒæ•¸ æˆ‘çš„æƒ³æ³•å°±æ˜¯æ‰¾åˆ°ç¬¦åˆæˆ‘å€‘æ¨£æœ¬è³‡æ–™çš„â€æœ€å¤§â€æ¦‚ä¼¼å‡½æ•¸çš„æ©Ÿç‡æ¨¡å‹åƒæ•¸ï¼Œä»¥å¾€éƒ½æ˜¯å‡è¨­å¥½æ©Ÿç‡æ¨¡å‹ä¸­çš„åƒæ•¸ï¼Œä¸¦è¨ˆç®—å…¶MLEï¼Œç¾åœ¨é€éæ¨£æœ¬åŠæ¨£æœ¬æ‰€éš±è—çš„éš±è®Šæ•¸ï¼Œä¾†æ¨å°é©åˆé€™äº›æ¨£æœ¬çš„æ¨¡å‹åƒæ•¸ã€‚ æ€è€ƒ argument : $y$ (éºå¤±å€¼æˆ–æ˜¯éš±è®Šæ•¸) complete-data likelihood : $L(\\theta|x,y)=p(x,y|\\theta)$ åŠ å…¥éš±è®Šæ•¸å¾Œå®Œæ•´çš„æ¦‚ä¼¼å‡½æ•¸ What about $max_\\theta L(x,y)$? need to recursively update y and $\\hat\\theta$? Eæ­¥é©Ÿ(æ›´æ–°y)æ ¹æ“šç¾åœ¨çµ¦å®šçš„æ¨¡å‹åƒæ•¸åŠæ¨£æœ¬è§€æ¸¬å€¼ï¼Œæˆ‘å€‘å»è¨ˆç®—å…¶logå®Œæ•´æ¦‚ä¼¼å‡½æ•¸çš„æœŸæœ›å€¼å¦‚ä¸‹:$$Q(\\theta|\\theta^{(t)})\\equiv E_y[log p(x,y|\\theta)|x,\\theta^{(t)}]$$ Mæ­¥é©Ÿ(æ›´æ–°åƒæ•¸$\\theta$)æœ€å¤§åŒ–Eæ­¥é©Ÿç²å¾—çš„æœŸæœ›å€¼$$\\theta^{(t+1)} = arg max_\\theta Q(\\theta|\\theta^{t})$$ é‡è¤‡ä¸Šè¿°éç¨‹ç›´åˆ°æ”¶æ–‚ EMæ¨å° $X:observed \\space data$ $Y:latent\\space variable$ $Log\\space likelihood \\space function$ $$ l(\\theta) $$ $$= lnP(X|\\theta) $$ $$= ln\\sum_yP(X,y|\\theta) $$ $$= ln(\\sum_yP(X,y|\\theta))\\frac{Q(y)}{Q(y)}$$ $$\\ge\\sum_yQ(y)ln\\frac{P(X,y|\\theta)}{Q(y)} $$ $$\\because log \\in concave\\space by\\space Jensenâ€™s\\space inequality $$ $$(è§£æ±ºln\\sum ä¸å¥½è¨ˆç®—çš„å•é¡Œï¼ŒQ:æ©Ÿç‡åˆ†é…) $$ $$= E_Q(ln\\frac{P(X,y|\\theta)}{Q(y)})$$ åœ¨$Jensenâ€™s \\: inequality ä¸­ï¼Œç•¶E(x)ä¸­ï¼Œx=å¸¸æ•¸æ™‚ï¼Œç­‰è™Ÿæˆç«‹ã€‚$ $$\\Rightarrow\\frac{P(X,y|\\theta)}{Q(y)}=c \\in Constantï¼Œä¸”\\sum_yQ(y)=1$$ $$\\Rightarrow\\sum_yP(X,y|\\theta)=c\\sum_yQ(y)=c$$ $$\\Rightarrow Q(y)=\\frac{P(X,y|\\theta)}{\\sum_yP(X,y|\\theta)}=P(y|x_i,\\theta)$$ï¼Œ $$Q:åœ¨æ¨£æœ¬çµ¦å®šä¸‹ä¹‹éš±è—è®Šæ•¸æ¢ä»¶åˆ†å¸ƒ$$ $$\\therefore E_Q(ln\\frac{P(X,y|\\theta)}{Q(y)})=E_y(ln\\frac{P(X,y|\\theta)}{P(y|x_i,\\theta)}|X,\\theta)$$ $$\\theta^{(t+1)}=arg\\max\\limits_\\theta l(\\theta)\\Longleftrightarrow arg\\max\\limits_\\theta\\sum_{y}P(y|x_i,\\theta^{(t)})ln\\frac{P(X,y|\\theta)}{P(y|x_i,\\theta^{(t)})}$$ $$\\: \\: \\Longleftrightarrow arg\\max\\limits_\\theta\\sum_{y}P(y|x_i,\\theta^{(t)})lnP(X,y|\\theta)=E_y(lnP(X,y|\\theta)|X,\\theta^{(t)})=Q(\\theta|\\theta^{(t)})$$ $$\\therefore E-step :Find \\: Q(\\theta|\\theta^{(t)})$$ $\\Longleftrightarrow$ Find the expectation of the complete-data loglikelihood with respect to the missing data y given the observed data x and the current parameter estimates $\\theta^{(t)}$. $$M-step=\\theta^{(t+1)}=arg\\max\\limits_\\theta Q(\\theta|\\theta^{(t)})$$ EMæ”¶æ–‚æ€§ ç¯„ä¾‹1197 animals are distributed multinomially into four categories with cell-probabilities$(\\frac{1}{2}+ \\frac{\\theta}{4}, \\frac{(1-\\theta)}{4}, \\frac{(1-\\theta)}{4}, \\frac{\\theta}{4})$, where $\\theta \\in (0,1)$is unknown Observed Data:$$x=(x_1,x_2,x_3,x_4)=(125,18,20,34)$$ Likelihood:$$L(\\thetaï¼›x)=\\frac{n!}{x_1!x_2!x_3!x_4!}(\\frac{1}{2}+\\frac{\\theta}{4})^{x_1}(\\frac{1}{4}-\\frac{\\theta}{4})^{x_2}(\\frac{1}{4}-\\frac{\\theta}{4})^{x_3}(\\frac{\\theta}{4})^{x_4}$$ Find MLE by maximizing loglikelihood Now use EM to find MLE å‡è¨­æˆ‘å€‘çš„éš±è—è®Šæ•¸åœ¨aè£¡é¢ï¼Œä»¤$y=x_{11}+x_{12}$ å®Œæ•´çš„è®Šæ•¸æ“´å±•ç‚º$(x_{11},x_{12},x_{2},x_{3},x_{4})$æœ‰5å€‹ åˆå§‹å…¶åƒæ•¸ $(\\frac{1}{2}, \\frac{\\theta}{4}, \\frac{1}{4}-\\frac{\\theta}{4}, \\frac{\\theta}{4})$ å…¶æ¦‚ä¼¼å‡½æ•¸å¦‚ä¸‹ $$L(\\thetaï¼›x)=\\frac{n!}{x_{11}!x_{12}!x_2!x_3!x_4!}(\\frac{1}{2})^{x_{11}}(\\frac{\\theta}{4})^{x_{12}}(\\frac{1}{4}-\\frac{\\theta}{4})^{x_{2}+x_{3}}(\\frac{\\theta}{4})^{x_{4}}$$ E æ­¥é©Ÿ çµ¦å®šæ©Ÿç‡æ¨¡å‹åƒæ•¸$\\theta^{(t)}$å’Œ$(x_1,x_2,x_3,x_4)$ï¼Œ$$x_{11}=\\frac{2x_{1}}{2+\\theta}\\quad and \\quad x_{12}=\\frac{\\theta x_{1}}{2+\\theta}$$ æ¨å°Qå‡½æ•¸(ä»¤$y=x_{12}$)$$Q(\\theta |\\theta^{(t)})=E_y[log p(x,y|\\theta)|x, \\theta^{(t)}]$$ $$=E_y[(x_{12}+x_{4})log\\theta +(x_{2}+x_{3})log(1-\\theta)|x,\\theta^{(t)}]$$ $$=(E_y[x_{12}|x, \\theta^{(t)}]+x_{4})log\\theta +(x_{2}+x_{3})log(1-\\theta)$$ $$=(\\frac{\\theta^{(t)}x_{1}}{2+\\theta^{(t)}}+x_{4})log\\theta + (x_{2}+x_{3})log(1-\\theta)$$ å…¶ä¸­$$x_{12}|_{x,\\theta^{(t)}}\\sim Binomial(x_{1},\\frac{\\theta^{(t)}}{2+\\theta^{(t)}})$$ $$x_{12}^{(t)}=E_y[x_{12}|x,\\theta^{(t)}]=\\frac{\\theta^{(t)}x_{1}}{2+\\theta^{(t)}}$$ M æ­¥é©Ÿ å°Qå‡½æ•¸é€²è¡Œå¾®åˆ† çµ¦å®š$(x_{11},x_{12},x_{3},x_{4},x_{5})$ $$\\theta^{(t+1)}=\\frac{x_{12}^{(t)}+x_{4}}{x_{12}^{(t)}+x_{2}+x_{3}+x_{4}}$$ é‡è¤‡ä»¥ä¸Šæ­¥é©Ÿåˆ°åƒæ•¸è¿­ä»£è‡³ç©©å®š R å¯¦ä½œ12345678910111213mult = function(theta, x, n)&#123; tmp = theta for(i in 1:n)&#123; # E-step x12 = x[1]*(theta/ (2 + theta)) # M-step theta = (x12 + x[4])/(x12 + x[2] + x[3] + x[4]) tmp = c(tmp, theta) &#125;&#125;x=c(125, 18, 20, 34)mult(0.1, x, 10) ç¯„ä¾‹2ç¯„ä¾‹3å‡è¨­ç¾åœ¨æœ‰å…©æšç¡¬å¹£Aã€B æˆ‘å€‘ç”¨ä¸€æšå…¬æ­£çš„ç¡¬å¹£ä¾†æ±ºå®šï¼ŒæŠ•æ“² A æˆ– B ç¡¬å¹£ ä¾æ“šçµæœï¼ŒæŠ•æ“² A æˆ– B ç¡¬å¹£ 1æ¬¡ï¼Œè¨˜éŒ„å…¶çµæœ åè¦†é€²è¡Œnæ¬¡ï¼Œæœ€çµ‚å¯å¾—åˆ°é¡ä¼¼å¦‚ä¸‹çµæœ: 10111011â€¦. 1è¡¨ç¤ºæ­£é¢ï¼Œ0è¡¨ç¤ºåé¢ å¦‚æœæˆ‘å€‘ä»Šå¤©åªèƒ½è§€å¯Ÿåˆ°æœ€çµ‚çµæœï¼Œç„¡æ³•çŸ¥é“æ¯ä¸€æ¬¡æŠ•æ“²ä¾†è‡ªå“ªä¸€æšç¡¬å¹£ï¼Œè©²å¦‚ä½•ä¼°è¨ˆå‡ºå…©å€‹ç¡¬å¹£å‡ºç¾æ­£é¢æ©Ÿç‡? : Observed Data : $X=(x_1,x_2,â€¦,x_n), x_i:æ­£é¢å‡ºç¾æ¬¡æ•¸$Aã€B å‡ºç¾æ­£é¢æ©Ÿç‡ : $\\theta =(p,q)$ $1^{\\circ}$ å¾MLEæƒ³æ³•å‡ºç™¼ æ¦‚ä¼¼å‡½æ•¸ :$$L(\\theta|X)=P(X|\\theta)=\\prod_{i=1}^nP(x_i|\\theta)$$ $$\\hat p=\\frac{ä½¿ç”¨Aç¡¬å¹£éª°åˆ°æ­£é¢æ¬¡æ•¸}{ä½¿ç”¨Aç¡¬å¹£ç¸½æŠ•æ“²æ¬¡æ•¸}$$ $$\\hat q=\\frac{ä½¿ç”¨Bç¡¬å¹£éª°åˆ°æ­£é¢æ¬¡æ•¸}{ä½¿ç”¨Bç¡¬å¹£ç¸½æŠ•æ“²æ¬¡æ•¸}$$ ä½†å› ç‚ºæˆ‘å€‘ä¸¦ä¸çŸ¥é“ $x_i$ä¾†è‡ªå“ªå€‹ç¡¬å¹£(æ©Ÿç‡æ¨¡å‹)ï¼Œæ‰€ä»¥ç„¡æ³•é€²è¡Œè¨ˆç®—ã€‚ $2^{\\circ}$ å˜—è©¦æ·»åŠ éš±è—è®Šæ•¸ï¼Œä½¿å…¶è®Šæˆcomplete dataï¼Œé‹ç”¨EMæ¼”ç®—æ³• æ ¹æ“šobserved dataï¼Œæˆ‘å€‘ç„¡å¾å¾—çŸ¥ $x_i$ä¾†è‡ªå“ªå€‹ç¡¬å¹£(æ©Ÿç‡æ¨¡å‹)ï¼Œ å› æ­¤æˆ‘å€‘æ·»åŠ ä¸€å€‹éš±è—è®Šæ•¸ $y_i$ï¼Œå…¶è¡¨ç¤º $x_i$ ä¾†è‡ªå“ªå€‹ç¡¬å¹£ï¼Œ$Y=(y_1,y_2,â€¦,y_n)$ $y_i = 1$ or $2$ï¼Œ$x_i|y_{i}=1 \\sim Ber(p_{1})$ï¼Œ$x_i|y_{i}=2 \\sim Ber(p_{2})$ E-step : $$\\Rightarrow Q(\\theta|\\theta^{(t)})=E_y[ln(p(x,y|\\theta))|x,\\theta ^{(t)}]=E_y[\\sum_{i=1}^{n}ln(p(y_i|\\theta)p(x_i|y_i,\\theta))|x,\\theta^{(t)}]$$ $$=\\sum_{i=1}^{n}E_y[ln(p(y_i|\\theta)p(x_i|y_i,\\theta))|x,\\theta^{(t)}]=\\sum_{i=1}^{n}\\sum_{y_i=0}^{1}[ln(p(y_i|\\theta)p(x_i|y_i,\\theta))p(y_i|x_i,\\theta^{(t)} )]$$ $$=\\sum_{i=1}^{n}\\sum_{j=0}^{1}[ln(p(y_i=j|\\theta)p(x_i|y_i,\\theta))p(y_i=j|x_i,\\theta^{(t)} )]$$ å…¶ä¸­$p(y_i=j|x_i,\\theta^{(t)} )$ : åœ¨ç¬¬tæ¬¡è¿­ä»£ä¸‹ï¼Œç•¶å‰æ•¸æ“šä¾†è‡ªå“ªå€‹ç¡¬å¹£çš„æ©Ÿç‡ Q-step : $$\\frac{\\partial Q}{\\partial p}=\\frac{\\partial (\\sum_{i=1}^{n}ln(\\frac{1}{2}p^{x_i}(1-p)^{1-x_i})p(y_i=1|x_i,\\theta^{(t)} )}{\\partial p}$$ $$=\\frac{\\partial (\\sum_{i=1}^{n}ln(\\frac{1}{2})+x_iln(p)+(1-x_i)ln(1-p)p(y_i=1|x_i,\\theta^{(t)} )}{\\partial p}$$ $$=\\sum_{i=1}^{n}(\\frac{xi}{p}-\\frac{(1-x_i)}{1- p})p(y_i=1|x_i,\\theta^{(t)} )=0$$ $$\\Rightarrow p^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=1|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}p(y_i=1|x_i,\\theta^{(t)})}$$ $$\\frac{\\partial Q}{\\partial q}=0$$ åŒç†å¯å¾—,$$q^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=2|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}p(y_i=2|x_i,\\theta^{(t)})}$$ $3^{\\circ}$ ç¶œè§€ä»¥ä¸Šçµæœï¼Œå¯ä»¥ç™¼ç¾å¯¦éš›ä¸Šæˆ‘å€‘åªéœ€è¦è¨ˆç®—å‡º$p(y_i=j|x_i,\\theta^{(t)} )$ï¼Œå°±å¯ä»¥æ‹¿ä¾†é€²è¡ŒEMè¿­ä»£ã€‚ $\\Rightarrow$ çµ¦å®šåˆå§‹å€¼$(p^{(0)},q^{(0)})$ï¼Œè¨ˆç®—å‡º$p(y_i=j|x_i,\\theta^{(0)} )$ï¼Œä»£å…¥æ›´æ–°åƒæ•¸$p^{(t+1)},q^{(t+1)}$ï¼Œé‡è¤‡è¿­ä»£ï¼Œç›´åˆ°æ”¶æ–‚æˆ–è€…é”åˆ°è‡ªè¡Œçµ¦å®štoleranceå…§. $4^{\\circ}$ è‹¥æ”¹æˆä¾æ“šçµæœï¼Œé€£çºŒæŠ•æ“² A æˆ– B ç¡¬å¹£ 10æ¬¡ï¼Œè¨˜éŒ„å…¶çµæœ åè¦†é€²è¡Œnæ¬¡ï¼Œæœ€çµ‚å¯å¾—åˆ°é¡ä¼¼å¦‚ä¸‹çµæœ: 1è¡¨ç¤ºæ­£é¢ï¼Œ0è¡¨ç¤ºåé¢ æœ€çµ‚Q-step çš„åƒæ•¸å…¬å¼ :$$p^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=1|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}10p(y_i=1|x_i,\\theta^{(t)}}$$ $$q^{(t+1)}=\\frac{\\sum_{i=1}^{n}x_ip(y_i=2|x_i,\\theta^{(t)})}{\\sum_{i=1}^{n}10p(y_i=2|x_i,\\theta^{(t)})}$$ Python å¯¦ä½œ12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import numpy as npimport pandas as pdfrom scipy import stats# 5çµ„ç¡¬å¹£æŠ•æ“²çµæœ(n=5,k=10)ï¼Œ1ä»£è¡¨æ­£é¢ï¼Œ0ä»£è¡¨åé¢observations = np.array([[1, 0, 0, 0, 1, 1, 0, 1, 0, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 1, 1], [1, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1]])da=pd.DataFrame(observations, index=[\"ç¬¬ä¸€æ¬¡\",\"ç¬¬äºŒæ¬¡\",\"ç¬¬ä¸‰æ¬¡\",\"ç¬¬å››æ¬¡\",\"ç¬¬äº”æ¬¡\"])da.columns = [1,2,3,4,5,6,7,8,9,10];da###def em_single(priors,observations): \"\"\" EMç®—æ³•-å–®æ¬¡ç–Šä»£ ------------ priors:[theta_A,theta_B] observation:[m X n matrix] Returns --------------- new_priors:[new_theta_A,new_theta_B] :param priors: :param observations: :return: \"\"\" counts = &#123;'A': &#123;'H': 0, 'T': 0&#125;, 'B': &#123;'H': 0, 'T': 0&#125;&#125; theta_A = priors[0] theta_B = priors[1] #E step for observation in observations: len_observation = len(observation) #è¨ˆç®—æŠ•æ“²æ¬¡æ•¸ num_heads = observation.sum() #æ­£é¢æ¬¡æ•¸ num_tails = len_observation-num_heads #åé¢æ¬¡æ•¸ #äºŒé …åˆ†é…å…¬å¼æ±‚è§£ contribution_A = scipy.stats.binom.pmf(num_heads,len_observation,theta_A) #Bin(x,n,p) contribution_B = scipy.stats.binom.pmf(num_heads,len_observation,theta_B) #è¨ˆç®—åœ¨çµ¦å®šè³‡æ–™ã€ç•¶å‰åƒæ•¸ä¸‹ï¼Œè³‡æ–™ä¾†è‡ªå“ªå€‹ç¡¬å¹£çš„æ©Ÿç‡ weight_A = contribution_A / (contribution_A + contribution_B) # p(y=1|x,theta) weight_B = contribution_B / (contribution_A + contribution_B) # p(y=0|x,theta) #æ›´æ–°åœ¨ç•¶å‰åƒæ•¸ä¸‹Aï¼ŒBç¡¬å¹£ç”¢ç”Ÿçš„æ­£åé¢æ¬¡æ•¸ counts['A']['H'] += weight_A * num_heads # num += 1 =&gt; num = num+1ï¼Œ sum p(y_i=1|x,theta)*x_i counts['A']['T'] += weight_A * num_tails counts['B']['H'] += weight_B * num_heads counts['B']['T'] += weight_B * num_tails # M step new_theta_A = counts['A']['H'] / (counts['A']['H'] + counts['A']['T']) #sum p(y_i=1|x,theta)*x_i / sum 10*p(y_i=1|x,theta) new_theta_B = counts['B']['H'] / (counts['B']['H'] + counts['B']['T']) #sum p(y_i=0|x,theta)*x_i / sum 10*p(y_i=1|x,theta) return [new_theta_A,new_theta_B] ###def em(observations,prior,tol = 1e-6,iterations=10000): \"\"\" EMç®—æ³• ï¼šparam observations :è§€æ¸¬æ•¸æ“š ï¼šparam priorï¼šæ¨¡å‹åˆå§‹å€¼ ï¼šparam tolï¼šè¿­ä»£ç»“æŸé˜ˆå€¼ ï¼šparam iterationsï¼šæœ€å¤§è¿­ä»£æ¬¡æ•¸ ï¼šreturnï¼šå±€éƒ¨æœ€ä½³çš„æ¨¡å‹åƒæ•¸ \"\"\" iteration = 0; while iteration &lt; iterations: new_prior = em_single(prior,observations) delta_change = np.abs(prior[0]-new_prior[0]) if delta_change &lt; tol: break else: prior = new_prior iteration +=1 return new_prior,iterationprint (\"(p,q,iteration)=\",em(observations,[0.7,0.5])) Ans : (p,q,iteration)= ([0.79678865844706648, 0.51958340803243785], 12)","tags":[{"name":"EM","slug":"EM","permalink":"https://caocharles.github.io/tags/EM/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://caocharles.github.io/tags/Algorithm/"}],"categories":[{"name":"æ©Ÿå™¨å­¸ç¿’","slug":"machine-learning","permalink":"https://caocharles.github.io/categories/machine-learning/"}]},{"title":"æˆ‘çš„æ¸¬è©¦æ–‡ç« ","date":"2018-08-27T08:23:30.000Z","path":"æˆ‘çš„æ¸¬è©¦æ–‡ç« /","text":"é€£çµæ¸¬è©¦ æ‰“æ‰“çœ‹æ–‡å­—ã€‚ 12345 1x+2y =5 $$\\sigma$$ $$\\frac{1}{2}$$ TENSORFLOWâ€™S HELLO WORLD In this notebook we will overview the basics of TensorFlow, learn itâ€™s structures and see what is the motivation to use it - How does TensorFlow work? - Building a Graph - Defining multidimensional arrays using TensorFlow - Why Tensors? - Variables - Placeholders - Operationsâ€”â€”â€”â€”â€”-# How does TensorFlow work?TensorFlowâ€™s capability to execute the code on different devices such as CPUs and GPUs is a consequence of itâ€™s specific structure:TensorFlow defines computations as Graphs, and these are made with operations (also know as â€œopsâ€). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.To execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU.For example, the image below represents a graph in TensorFlow. _W_, _x_ and b are tensors over the edges of this graph. _MatMul_ is an operation over the tensors _W_ and _x_, after that _Add_ is called and add the result of the previous operator with _b_. The resultant tensors of each operation cross the next one until the end where itâ€™s possible to get the wanted result.### Importing TensorFlowTo use TensorFlow, we need to import the library. We imported it and optionally gave it the name â€œtfâ€, so the modules can be accessed by tf.module-name:1import tensorflow as tfâ€”â€”â€”â€”â€”â€“# Building a GraphAs we said before, TensorFlow works as a graph computational model. Letâ€™s create our first graph.To create our first graph we will utilize source operations, which do not need any information input. These source operations or source ops will pass their information to other operations which will execute computations.To create two source operations which will output numbers we will define two constants:12a = tf.constant([2])b = tf.constant([3])After that, letâ€™s make an operation over these variables. The function tf.add() adds two elements (you could also use c = a + b).12c = tf.add(a,b)#c = a + b is also a way to define the sum of the termsThen TensorFlow needs to initialize a session to run our code. Sessions are, in a way, a context for creating a graph inside TensorFlow. Letâ€™s define our session:1session = tf.Session()Letâ€™s run the session to get the result from the previous defined â€˜câ€™ operation:12result = session.run(c)print(result)Close the session to release resources:1session.close()To avoid having to close sessions every time, we can define them in a with block, so after running the with block the session will close automatically:123with tf.Session() as session: result = session.run(c) print(result)Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your edge (In this case our constants), include nodes (operations, like _tf.add_), and start a session to build a graph.### What is the meaning of Tensor?In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.The word tensor from new latin means â€œthat which stretchesâ€. It is a mathematical object that is named tensor because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays. Thatâ€™s great, butâ€¦ what are these multidimensional arrays? Going back a little bit to physics to understand the concept of dimensions: [Image Source] The zero dimension can be seen as a point, a single object or a single item. The first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements. The second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. The third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line. The Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth onâ€¦ As mathematical objects: [Image Source] Summarizing: Dimension Physical Representation Mathematical Object In Code Zero Point Scalar (Single Number) [ 1 ] One Line Vector (Series of Numbers) [ 1,2,3,4,â€¦ ] Two Surface Matrix (Table of Numbers) [ [1,2,3,4,â€¦], [1,2,3,4,â€¦], [1,2,3,4,â€¦],â€¦ ] Three Volume Tensor (Cube of Numbers) [ [[1,2,â€¦], [1,2,â€¦], [1,2,â€¦],â€¦], [[1,2,â€¦], [1,2,â€¦], [1,2,â€¦],â€¦], [[1,2,â€¦], [1,2,â€¦], [1,2,â€¦] ,â€¦]â€¦ ] Defining multidimensional arrays using TensorFlowNow we will try to define such arrays using TensorFlow: 12345678910111213Scalar = tf.constant([2])Vector = tf.constant([5,6,2])Matrix = tf.constant([[1,2,3],[2,3,4],[3,4,5]])Tensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )with tf.Session() as session: result = session.run(Scalar) print (\"Scalar (1 entry):\\n %s \\n\" % result) result = session.run(Vector) print (\"Vector (3 entries) :\\n %s \\n\" % result) result = session.run(Matrix) print (\"Matrix (3x3 entries):\\n %s \\n\" % result) result = session.run(Tensor) print (\"Tensor (3x3x3 entries) :\\n %s \\n\" % result) Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types: 12345678910111213Matrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])Matrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])first_operation = tf.add(Matrix_one, Matrix_two)second_operation = Matrix_one + Matrix_twowith tf.Session() as session: result = session.run(first_operation) print (\"Defined using tensorflow function :\") print(result) result = session.run(second_operation) print (\"Defined using normal expressions :\") print(result) With the regular symbol definition and also the TensorFlow function we were able to get an element-wise multiplication, also known as Hadamard product. But what if we want the regular matrix product? We then need to use another TensorFlow function called tf.matmul(): 123456789Matrix_one = tf.constant([[2,3],[3,4]])Matrix_two = tf.constant([[2,3],[3,4]])first_operation = tf.matmul(Matrix_one, Matrix_two)with tf.Session() as session: result = session.run(first_operation) print (\"Defined using tensorflow function :\") print(result) We could also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel! Why Tensors?The Tensor structure helps us by giving the freedom to shape the dataset the way we want. And it is particularly helpful when dealing with images, due to the nature of how information in images are encoded, Thinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional strucutre (a matrix)â€¦ until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particulary helpful. Images are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this: [Image Source] So the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. VariablesNow that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables. To define variables we use the command tf.variable().To be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. This is done by running tf.global_variables_initializer(). To update the value of a variable, we simply run an assign operation that assigns a value to the variable: 1state = tf.Variable(0) Letâ€™s first create a simple counter, a variable that increases one unit at a time: 123one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value) Variables must be initialized by running an initialization operation after having launched the graph. We first have to add the initialization operation to the graph: 1init_op = tf.global_variables_initializer() We then start a session to run the graph, first initialize the variables, then print the initial value of the state variable, and then run the operation of updating the state variable and printing the result after each update: 123456with tf.Session() as session: session.run(init_op) print(session.run(state)) for _ in range(3): session.run(update) print(session.run(state)) PlaceholdersNow we know how to manipulate variables inside TensorFlow, but what about feeding data outside of a TensorFlow model? If you want to feed data to a TensorFlow model from outside a model, you will need to use placeholders. So what are these placeholders and what do they do? Placeholders can be seen as â€œholesâ€ in your model, â€œholesâ€ which you will pass the data to, you can create them using tf.placeholder(_datatype_), where _datatype_ specifies the type of data (integers, floating points, strings, booleans) along with its precision (8, 16, 32, 64) bits. The definition of each data type with the respective python sintax is defined as: Data type Python type Description DT_FLOAT tf.float32 32 bits floating point. DT_DOUBLE tf.float64 64 bits floating point. DT_INT8 tf.int8 8 bits signed integer. DT_INT16 tf.int16 16 bits signed integer. DT_INT32 tf.int32 32 bits signed integer. DT_INT64 tf.int64 64 bits signed integer. DT_UINT8 tf.uint8 8 bits unsigned integer. DT_STRING tf.string Variable length byte arrays. Each element of a Tensor is a byte array. DT_BOOL tf.bool Boolean. DT_COMPLEX64 tf.complex64 Complex number made of two 32 bits floating points: real and imaginary parts. DT_COMPLEX128 tf.complex128 Complex number made of two 64 bits floating points: real and imaginary parts. DT_QINT8 tf.qint8 8 bits signed integer used in quantized Ops. DT_QINT32 tf.qint32 32 bits signed integer used in quantized Ops. DT_QUINT8 tf.quint8 8 bits unsigned integer used in quantized Ops. [Table Source] So we create a placeholder: 1a = tf.placeholder(tf.float32) And define a simple multiplication operation: 1b = a*2 Now we need to define and run the session, but since we created a â€œholeâ€ in the model to pass the data, when we initialize the session we are obligated to pass an argument with the data, otherwise we would get an error. To pass the data to the model we call the session with an extra argument feed_dict in which we should pass a dictionary with each placeholder name folowed by its respective data, just like this: 123with tf.Session() as sess: result = sess.run(b,feed_dict=&#123;a:3.5&#125;) print (result) Since data in TensorFlow is passed in form of multidimensional arrays we can pass any kind of tensor through the placeholders to get the answer to the simple multiplication operation: 12345dictionary=&#123;a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] &#125;with tf.Session() as sess: result = sess.run(b,feed_dict=dictionary) print (result) OperationsOperations are nodes that represent the mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensor or maybe an activation function. _tf.matmul_, _tf.add_, _tf.nn.sigmoid_ are some of the operations in TensorFlow. These are like functions in python but operate directly over tensors and each one does a specific thing. Other operations can be easily found in: https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html 12345678910a = tf.constant([5])b = tf.constant([2])c = tf.add(a,b)d = tf.subtract(a,b)with tf.Session() as session: result = session.run(c) print ('c =: %s' % result) result = session.run(d) print ('d =: %s' % result) _tf.nn.sigmoid_ is an activiation function, itâ€™s a little more complicated, but this function helps learning models to evaluate what kind of information is good or not. Want to learn more?Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBMâ€™s Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a free version of PowerAI. Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBMâ€™s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at Data Science ExperienceThis is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies. Thanks for completing this lesson!Notebook created by: Rafael Belo Da Silva References:https://www.tensorflow.org/versions/r0.9/get_started/index.htmlhttp://jrmeyer.github.io/tutorial/2016/02/01/TensorFlow-Tutorial.htmlhttps://www.tensorflow.org/versions/r0.9/api_docs/python/index.htmlhttps://www.tensorflow.org/versions/r0.9/resources/dims_types.htmlhttps://en.wikipedia.org/wiki/Dimensionhttps://book.mql4.com/variables/arrayshttps://msdn.microsoft.com/en-us/library/windows/desktop/dn424131(v=vs.85).aspx Copyright &copy; 2016 Big Data University. This notebook and its source code are released under the terms of the MIT License.","tags":[{"name":"é€™æ˜¯æ¨™ç±¤","slug":"é€™æ˜¯æ¨™ç±¤","permalink":"https://caocharles.github.io/tags/é€™æ˜¯æ¨™ç±¤/"},{"name":"é€™æ˜¯æ¨™ç±¤2","slug":"é€™æ˜¯æ¨™ç±¤2","permalink":"https://caocharles.github.io/tags/é€™æ˜¯æ¨™ç±¤2/"}],"categories":[{"name":"é€™æ˜¯åˆ†é¡","slug":"é€™æ˜¯åˆ†é¡","permalink":"https://caocharles.github.io/categories/é€™æ˜¯åˆ†é¡/"},{"name":"é€™æ˜¯å­åˆ†é¡","slug":"é€™æ˜¯åˆ†é¡/é€™æ˜¯å­åˆ†é¡","permalink":"https://caocharles.github.io/categories/é€™æ˜¯åˆ†é¡/é€™æ˜¯å­åˆ†é¡/"}]},{"title":"Hello World","date":"2018-08-27T07:58:15.056Z","path":"hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]}]}